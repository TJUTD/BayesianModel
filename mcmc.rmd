---
title: Monte Carlo Strategies
output: html_document
---

```{R setup, include = FALSE}
knitr::opts_chunk$set(comment = NA, prompt = TRUE)
```

\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\bbR}{\mathbb{R}}

\newcommand{\hmu}{\hat{\mu}}
\newcommand{\tmu}{\tilde{\mu}}

\newcommand{\cS}{\mathcal{S}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\be}{\mathbf{e}}
\newcommand{\bm}{\mathbf{m}}
\newcommand{\bp}{\mathbf{p}}
\newcommand{\bv}{\mathbf{v}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}

Boltzmann distribution (Gibbs distribution) $\pi(\bx) = \frac{1}{Z}e^{-\frac{U(\bx)}{kT}}$

- configuration of a physical system $\bx$ 
- potential energy $U(\bx)$
- temperature $T$
- Boltzmann constant $k$
- partition function $Z=Z(T)$
- internal energy $\langle U\rangle = \bbE_{\pi}\{U(\bx)\}$
- set $\beta = \frac{1}{kT}$, $\frac{\partial \log(Z)}{\partial \beta}= - \langle U\rangle$
- free energy $F=-kT\log(Z)$
- specific heat of the system $C = \frac{\partial \langle U\rangle}{\partial T}       =\frac{1}{kT^2}\Var_{\pi}\{U(\bx)\}$
- the system's entropy $S = (\langle U\rangle - F)/T$

$2$-$D$ Ising model on a $N\times N$ lattice space $\mathcal{L}=\{(i,j),i,j=1,\dots,N \}$
$$U(\bx) = -J\sum_{\sigma\sim\sigma'} x_{\sigma}x_{\sigma'} + \sum_{\sigma}h_{\sigma}(x_{\sigma})$$

- configuration of the whole system $\mathbf{x}=$ $\{$ $x_{\sigma}:$ $\sigma \in \mathcal{L}$ $\}$
- a particle at site $\sigma$ has either a positive or a negative spin  $x_{\sigma}\in\{+1,-1\}$  
- sites $\sigma,\sigma'\in\mathcal{L}$ are a neighboring pair $\sigma\sim\sigma'$
- interaction strength $J$
- external magnetic field $h_{\sigma}$
- mean magnetization per spin $\langle m\rangle = \bbE_{\pi}\{\frac{1}{N^2}\left|\sum_{\sigma\in S}x_{\sigma}\right|\}$

Potts model $x_l\in\{1,2,\dots,q\}$, $H(\bx)=-J\sum_{i\sim j}\delta_{x_ix_j}-\sum_jh_j(x_j)$

- $q=2$, $H(\bx)=-\frac{1}{2}J\sum_{i\sim j}2\left(\delta_{x_ix_j}-\frac{1}{2}\right) -\sum_jh_j(x_j)$

simple liquid model $\mathbf{x} = \{x_i\in \bbR^{3}: i=1,\dots,k\}\in \bbR^{3k}$ 

- energy $U(\mathbf{x}) = \sum_{i,j}\Phi(|x_i-x_j|) = \sum_{i,j}\Phi(r_{ij})$
- Lennard-Jobes pair potential $\Phi(r) = 4\epsilon\left[\left(\frac{\sigma}{r}\right)^{12} -\left(\frac{\sigma}{r}\right)^{6} \right]$

macromolecules model

- energy $U(\mathbf{x}) = \sum_{\mbox{bonds}}\{\mbox{bond terms}\} + \sum_{i,j}\left[\Phi(r_{ij}) + \frac{q_iq_j}{4\pi\epsilon_0r_{ij}}\right]$
- bond terms $=\sum_{\mbox{bonds}} \frac{k_i}{2}(l_i-l_{i,0})^2 + \sum_{\mbox{angles}} \frac{k_i}{2}(\theta_i-\theta_{i,0})^2 + \sum_{\mbox{torsions}} v(\omega_i)$
- bond length $l_i$, bond angle $\theta_i$, torsion angle $\omega_i$
- torsion term $v(\omega_i) = \frac{V_n}{2}(1+\cos(n\omega -\gamma))$

### variance reduction methods

stratified sampling

- estimand $\int_{\cX}f(x)dx$
- partition $D_1,\dots,D_k$ of $\cX$
- $\hmu=\hmu_1+\dots+\hmu_k$, $\hmu_i= m_i^{-1}[f(X_{i,1})+\dots+f(X_{i,m_i})]$
- $\Var(\hat{\mu})=\frac{\sigma_1^2}{m_1} + \dots + \frac{\sigma_k^2}{m_k}$

control variates method

- estimand $\mu = \bbE\{X\}$
- control variate $C$ with known $\mu_C = \bbE\{C\}$
    + sample $X(b) = X + b(C-\mu_C)$
    + $\Var(X(b)) = \Var(X) + 2b\Cov(X,C) + b^2\Var(C)$
    + set $b=\Cov(X,C)/\Var(C)$, $\Var(X(b)) = (1-\rho_{XC}^2)\Var(X)$
- control variate $C$ with unknown $\bbE\{C\} = \mu$
    + sample $X(b) = bX + (1-b)C$
    + $\Var(X(b)) = b^2\Var(X) + 2b(1-b)\Cov(X,C) + (1-b)^2\Var(C)$
    + $b = [\Var(C) - \Cov(X,C)] / [\Var(X) - 2\Cov(X,C) + \Var(C)]$
		
antithetic variates method (Hammersley and Morton, 1956)

- sample pair $X=F^{-1}(U)$, $X'=F^{-1}(1-U)$ instead of two independent Monte Carlo draws for estimating $\bbE\{X\}$
- cdf $F$ is monotone, $\Cov(X,X')=\bbE\{[F^{-1}(U)-F^{-1}(U')][F^{-1}(1-U)-F^{-1}(1-U')]\}\le 0$

Rao-Blackwellization (Bickel and Doksum, 2000)

- estimand $I = \bbE\{h(X)\}$
- $X = (x_1,X_2)$. analytic $\bbE\{h(X)|X_2\}$ 
- histogram estimator $\hat{I} = \frac{1}{m}\sum_{l=1}^m h(X^{(l)})
- mixture estimator $\tilde{I} = \frac{1}{m}\sum_{l=1}^m \bbE\{h(X)|X_2^{(l)}\}$

chain-structure model

- $\pi(\bx) \propto \exp\left( -\sum_{i=1}^d h_i(x_{i-1},x_i) \right)$
- Markovian $\pi(x_i|\bx_{-i}) \propto \exp\left( -h_i(x_{i-1},x_i) -h_{i+1}(x_{i},x_{i+1}) \right)$
- hidden Markov model (HMM) when $x_i \in \mathcal{S} = \{s_1,\dots,s_k\}$
- optimization by dynamic programming $O(dk^2)$
$$m_1(x)=\min\limits_{s_i\in\cS}h_1(s_i,x),\quad m_t(x)=\min\limits_{s_i\in\cS}\{m_{t-1}(s_i)+h_t(s_i,x)\},\quad x=s_1,\dots,s_k$$
$$\hat x_1=\arg\min\limits_{s_i\in\cS} m_d(s_i),\quad \hat x_t=\arg\min\limits_{s_i\in\cS}\{m_{t}(s_i)+h_{t+1}(s_i,\hat x_{t+1})\},\quad t=d-1,\dots,1
$$
- exact simulation 
    - partition function $Z=\sum_{\bx}\exp(-H(\bx))=\sum_{x\in\cS} V_d(x)$
$$V_1(x) = \sum_{x_0\in\cS}e^{-h_1(x_0,x)},\quad V_t(x) = \sum_{y\in\cS}V_{t-1}(y)e^{-h_t(y,x)},\quad t=2,\dots,d$$
$$x_d \sim V_d(x)/Z,\quad x_t \sim \frac{V_{t}(x)e^{-h_{t+1}(x,x_{t+1})}}{\sum_{y\in\cS}V_{t}(y)e^{-h_{t+1}(y,x_{t+1})}},\quad t=d-1,\dots,1$$ 
    - Ising model $\pi(\bx) = Z^{-1}\exp(\beta(x_0x_1 + \dots + x_{d-1}x_d)), \quad x_i\in \{-1,+1\}$
$$V_1(x) = e^{\beta x} + e^{-\beta x} = e^{\beta} +e^{-\beta },\quad V_t(x) = (e^{\beta} + e^{-\beta})^t,\quad Z = 2(e^{\beta} + e^{-\beta})^d$$
    - graphical model, peeling algorithm / forward-summation-backward-sampling method
        - $\bx_C = \{x_i,i\in C\}$, clique $C\in \mathcal{C} \subset 2^{\{1,\dots,d\}}$ (Lauritzen and Spiegelhalter, 1998), connected  $C_i\cap C_j \ne \emptyset$

## importance sampling 
$$\mu=\bbE_{\pi}\{h(\bx)\}=\bbE_{g}\{w(\bx)h(\bx)\},\quad w=\pi(\bx)/g(\bx)$$

- normalized weight $\hmu = \frac{w^{(1)}h(\bx^{(1)})+\dots+w^{(n)}h(\bx^{(n)})}{w^{(1)}+\dots+w^{(n)}}=\frac{\overline{Z}}{\overline{W}}$
    - know $\pi(\bx)/g(\bx)$ up to a multiplicative constant
    - often a smaller MSE than $\tmu$
- unbiased estimate $\tmu = n^{-1}[w^{(1)}h(\bx^{(1)})+\dots+w^{(n)}h(\bx^{(n)})]$
- Effective Sample Size $=\frac{\Var_{\pi}(\bar{\mu})}{\Var_{g}(\hmu)}=\frac{n}{1+\Var_g(w(\bx))}$, direct sample $\bar{\mu}=n^{-1}[h(y^{(1)})+\dots+h(y^{(n)})]=\overline{H}$,  $y^{(i)}\sim\pi$
$$\begin{aligned}
  \Var_g(\hmu) &=\Var_g(\overline{Z}/\overline{W}) \approx \begin{bmatrix} \frac{1}{\bbE_g\{W\}} & -\frac{\bbE_g\{Z\}}{\bbE_g^2\{W\}} \end{bmatrix} \frac{1}{n}\begin{bmatrix} \Var_g(Z) & \Cov_g(Z,W)\\ \Cov_g(W,Z)   & \Var_g(W)  \end{bmatrix} \begin{bmatrix} \frac{1}{\bbE_g\{W\}} \\ -\frac{\bbE_g\{Z\}}{\bbE_g^2\{W\}} \end{bmatrix}\\
	&= \frac{1}{n}\left[ \frac{\Var_g(Z)}{\bbE_g^2\{W\}} - 2 \frac{\bbE_g\{Z\}\Cov_g(Z,W)}{\bbE_g^3\{W\}}  +\frac{\bbE_g^2\{Z\}\Var_g(W)}{\bbE_g^4\{W\}}\right]\\
	&= n^{-1}[ \Var_g(Z) - 2 \mu\Cov_g(Z,W)  + \mu^2\Var_g(W)]\\
	&= n^{-1}[ (\bbE_{\pi}\{WH^2\}-\mu) - 2 \mu(\bbE_{\pi}\{WH\} -\mu ) + \mu^2\Var_g(W)]\\	
	\bbE_{\pi}\{WH^2\}&\approx \bbE_{\pi}\{W\}\bbE_{\pi}^2\{H\} +\frac{1}{2} \mathrm{tr}\left(\begin{bmatrix} 0 & 2\bbE_{\pi}\{H\}\\
	2\bbE_{\pi}\{H\} & 2\bbE_{\pi}\{W\}\end{bmatrix} \begin{bmatrix} \Var_{\pi}(W) & \Cov_{\pi}(W,H)\\ \Cov_{\pi}(H,W) & \Var_{\pi}(H)  \end{bmatrix}\right) \\
	&= n^{-1}[ (\mu^2\bbE_{\pi}\{W\} + 2\mu\Cov_{\pi}(H,W) + \bbE_{\pi}\{W\}\Var_{\pi}(H)-\mu^2 )- 2 \mu(\Cov_{\pi}(WH) + \mu\bbE_{\pi}\{W\} -\mu ) + \mu^2\Var_{g}(W)]\\
	&= n^{-1}[ \bbE_{\pi}\{W\}\Var_{\pi}(H) +\mu^2(1-\bbE_{\pi}\{W\}+\Var_{g}(W))]\\
	&= n^{-1}[ \bbE_{\pi}\{W\}\Var_{\pi}(H) +\mu^2(1-\bbE_{g}\{W^2\}+\Var_{g}(W))]\\
	&= n^{-1} \bbE_{\pi}\{W\}\Var_{\pi}(H)\\
	&=\frac{1+\Var_g(W)}{n}\Var_{\pi}(H)
\end{aligned}
$$
- proper w.r.t $\pi$: $\bbE_g\{h(\bx^{(i)})w^{(i)}\} = c\bbE_{\pi}\{h(\bx)\}$, for all square integrable $h(\cdot)$ $\Longleftrightarrow$ $\frac{\bbE_{g}\{w|\bx\}}{\bbE_{g}\{w\}}g(\bx) = \pi(\bx)$

<!-- - missing data from a bivariate normal distribution -->
<!-- $$\begin{aligned} -->
<!-- y &\sim \mbox{N}(0,\Sigma),\quad \Sigma = \begin{bmatrix} \sigma_1^2, \rho\sigma_1\sigma_2\\ \rho\sigma_1\sigma_2 & \sigma_2^2\end{bmatrix}\\ -->
<!-- \pi(\Sigma) &\propto |\Sigma|^{-(d+1)/2}, \quad d=2\\ -->
<!-- \pi(\Sigma|y_1,\dots,y_n) &\propto |\Sigma|^{-(n+d+1)/2}\exp(-\mbox{tr}(\Sigma^{-1} S)/2)\sim\mbox{inverse Wishart}(n,S), \quad S = y^Ty\\ -->
<!-- \Sigma|\mathbf{y}_{\mbox{mis}},\mathbf{y}_{\mbox{obs}} &\sim \mbox{inverse Wishart}(n,S(\mathbf{y}_{\mbox{mis}})) \\ -->
<!-- y_{\mbox{mis}}|\Sigma,\mathbf{y}_{\mbox{obs}} &= y_{\mbox{mis}}|\Sigma,y_{\mbox{obs}}\sim \mbox{N}(\mu_*,\sigma_*),\quad \mu_*=\rho y_{\mbox{obs}} \sqrt{\sigma_{\mbox{obs}}/\sigma_{\mbox{mis}}},\sigma_*^2=(1-\rho^2)\sigma_{\mbox{obs}} \\ -->
<!-- \mbox{proposal }g:\ \Sigma &\sim \mbox{inverse Wishart}(n_{\mbox{obs}},S(\mathbf{y}_{\mbox{obs}})) -->
<!-- \end{aligned}$$ -->

<!-- ```{r misingdata, echo = FALSE} -->
<!-- library(knitr) -->
<!-- dat_mis = matrix(c(1,1,-1,-1,2,2,-2,-2,NA,NA,NA,NA,1,-1,1,-1,NA,NA,NA,NA,2,2,-2,-2),nrow = 2, byrow = T) -->
<!-- kable(as.data.frame(dat_mis), col.names = 1:dim(dat_mis)[2]) -->
<!-- ``` -->
<!-- ```{r importance sampling} -->
<!-- set.seed(123) -->
<!-- nsamp <- 5000 -->
<!-- SS_obs <- dat_mis[,1:4] %*% t(dat_mis[,1:4]) -->
<!-- dat_imp <- dat_mis -->
<!-- rho <- numeric(nsamp) -->

<!-- # weights -->
<!-- for (i in 1:nsamp) { -->
<!--   S_inv <- rWishart(1, 4, SS_obs)[,,1] -->
<!--   det_S_inv <- S_inv[1,1]*S_inv[2,2] - S_inv[1,2]*S_inv[2,1] -->
<!--   r <- -S_inv[1,2]/sqrt(S_inv[1,1]*S_inv[2,2]) -->
<!--   mu1 <- r * dat_mis[1,5:8] * sqrt(S_inv[2,2]/S_inv[1,1]) -->
<!--   mu2 <- r * dat_mis[2,9:12] * sqrt(S_inv[1,1]/S_inv[2,2]) -->
<!--   s1 <- sqrt((1-r^2) * S_inv[2,2]/det_S_inv) -->
<!--   s2 <- sqrt((1-r^2) * S_inv[1,1]/det_S_inv) -->
<!--   dat_imp[1,9] <- rnorm(1, mean = mu2[1], sd = s2) -->
<!--   dat_imp[1,10] <- rnorm(1, mean = mu2[2], sd = s2) -->
<!--   dat_imp[1,11] <- rnorm(1, mean = mu2[3], sd = s2) -->
<!--   dat_imp[1,12] <- rnorm(1, mean = mu2[4], sd = s2) -->
<!--   dat_imp[2,5] <- rnorm(1, mean = mu1[1], sd = s1) -->
<!--   dat_imp[2,6] <- rnorm(1, mean = mu1[2], sd = s1) -->
<!--   dat_imp[2,7] <- rnorm(1, mean = mu1[3], sd = s1) -->
<!--   dat_imp[2,8] <- rnorm(1, mean = mu1[4], sd = s1) -->
<!--   SS_imp <- dat_imp %*% t(dat_imp) -->
<!--   S_postr <- rWishart(1000, 12, SS_imp) -->
<!--   rho[i] <- mean(apply(S_postr, 3, function(mat){-mat[1,2]/sqrt(mat[1,1]*mat[2,2])})) -->
<!-- } -->

<!-- hist(rho, breaks = seq(-1,1,length.out = 41), freq = F, main = expression(rho)) -->
<!-- rx <- seq(-1,1,length.out = 201) -->
<!-- ry <- (1-rx^2)^(4.5)/(1.25-rx^2)^8 / integrate(function(x){(1-x^2)^(4.5)/(1.25-x^2)^8},-1,1)$value -->
<!-- lines(rx,ry) -->
<!-- ``` -->

<!-- ```{r importance sampling2} -->
<!-- set.seed(123) -->
<!-- nsamp <- 5000 -->
<!-- SS_obs <- dat_mis[,1:4] %*% t(dat_mis[,1:4]) -->
<!-- dat_imp <- dat_mis -->
<!-- rho <- numeric(nsamp) -->

<!-- # weights -->
<!-- for (i in 1:nsamp) { -->
<!--   S_inv <- rWishart(1, 4, SS_obs)[,,1] -->
<!--   det_S_inv <- S_inv[1,1]*S_inv[2,2] - S_inv[1,2]*S_inv[2,1] -->
<!--   r <- -S_inv[1,2]/sqrt(S_inv[1,1]*S_inv[2,2]) -->
<!--   mu1 <- r * dat_mis[1,5:8] * sqrt(S_inv[2,2]/S_inv[1,1]) -->
<!--   mu2 <- r * dat_mis[2,9:12] * sqrt(S_inv[1,1]/S_inv[2,2]) -->
<!--   s1 <- sqrt((1-r^2) * S_inv[2,2]/det_S_inv) -->
<!--   s2 <- sqrt((1-r^2) * S_inv[1,1]/det_S_inv) -->
<!--   dat_imp[1,9] <- rnorm(1, mean = mu2[1], sd = s2) -->
<!--   dat_imp[1,10] <- rnorm(1, mean = mu2[2], sd = s2) -->
<!--   dat_imp[1,11] <- rnorm(1, mean = mu2[3], sd = s2) -->
<!--   dat_imp[1,12] <- rnorm(1, mean = mu2[4], sd = s2) -->
<!--   dat_imp[2,5] <- rnorm(1, mean = mu1[1], sd = s1) -->
<!--   dat_imp[2,6] <- rnorm(1, mean = mu1[2], sd = s1) -->
<!--   dat_imp[2,7] <- rnorm(1, mean = mu1[3], sd = s1) -->
<!--   dat_imp[2,8] <- rnorm(1, mean = mu1[4], sd = s1) -->
<!--   SS_imp <- dat_imp %*% t(dat_imp) -->
<!--   rho[i] <- - SS_imp[1,2]/sqrt(SS_imp[1,1]*SS_imp[2,2]) -->
<!-- } -->

<!-- hist(rho, breaks = seq(-1,1,length.out = 41), freq = F, main = expression(rho)) -->
<!-- rx <- seq(-1,1,length.out = 201) -->
<!-- ry <- (1-rx^2)^(4.5)/(1.25-rx^2)^8 / integrate(function(x){(1-x^2)^(4.5)/(1.25-x^2)^8},-1,1)$value -->
<!-- lines(rx,ry) -->
<!-- ``` -->


<!-- ```{r importance sampling3} -->
<!-- set.seed(123) -->
<!-- nsamp <- 5000 -->
<!-- SS_obs <- dat_mis[,1:4] %*% t(dat_mis[,1:4]) -->
<!-- rho <- numeric(nsamp) -->
<!-- w <- numeric(nsamp) -->
<!-- SS_imp <- matrix(0, nrow = 2, ncol = 2) -->

<!-- # weights -->
<!-- for (i in 1:nsamp) { -->
<!--   S_inv <- rWishart(1, 4, SS_obs)[,,1] -->
<!--   det_S_inv <- S_inv[1,1]*S_inv[2,2] - S_inv[1,2]*S_inv[2,1] -->
<!--   r <- -S_inv[1,2]/sqrt(S_inv[1,1]*S_inv[2,2]) -->
<!--   mu1 <- r * dat_mis[1,5:8] * sqrt(S_inv[2,2]/S_inv[1,1]) -->
<!--   mu2 <- r * dat_mis[2,9:12] * sqrt(S_inv[1,1]/S_inv[2,2]) -->
<!--   s1 <- sqrt((1-r^2) * S_inv[2,2]/det_S_inv) -->
<!--   s2 <- sqrt((1-r^2) * S_inv[1,1]/det_S_inv) -->
<!--   x_1_mis <- rnorm(4, mean = mu2, sd = s2) -->
<!--   x_2_mis <- rnorm(4, mean = mu1, sd = s1) -->
<!--   SS_imp[1,1] <- sum(dat_mis[1,5:8]^2) + sum(x_1_mis^2)  -->
<!--   SS_imp[2,2] <- sum(x_2_mis^2) + sum(dat_mis[2,9:12]^2) -->
<!--   SS_imp[1,2] <- sum(dat_mis[1,5:8] * x_2_mis) + sum(x_1_mis * dat_mis[2,9:12])  -->
<!--   SS_imp[2,1] <- SS_imp[1,2] -->
<!--   SS <- SS_obs + SS_imp  -->
<!--   rho[i] <- - SS[1,2]/sqrt(SS[1,1]*SS[2,2]) -->
<!--   mu1_n <- rho[i] * dat_mis[1,5:8] * sqrt(S_inv[2,2]/S_inv[1,1]) -->
<!--   mu2_n <- rho[i] * dat_mis[2,9:12] * sqrt(S_inv[1,1]/S_inv[2,2]) -->
<!--   s1_n <- sqrt((1-rho[i]^2) * S_inv[2,2]/det_S_inv) -->
<!--   s2_n <- sqrt((1-rho[i]^2) * S_inv[1,1]/det_S_inv) -->
<!--   logw <- sum(dnorm(x_1_mis, mean = mu2_n, sd = s2_n, log = T)) + sum(dnorm(x_2_mis, mean = mu1_n, sd = s1_n, log = T)) - sum(dnorm(x_1_mis, mean = mu2, sd = s2, log = T)) - sum(dnorm(x_2_mis, mean = mu1, sd = s1, log = T)) -->
<!--   w[i] <- exp(logw) -->
<!-- } -->

<!-- var(w)/(mean(w)) -->

<!-- summary(rho) -->
<!-- hist(rho, breaks = seq(-1,1,length.out = 41), freq = F, main = expression(rho)) -->

<!-- rx <- seq(-1,1,length.out = 201) -->
<!-- ry <- (1-rx^2)^(4.5)/(1.25-rx^2)^8 / integrate(function(x){(1-x^2)^(4.5)/(1.25-x^2)^8},-1,1)$value -->
<!-- lines(rx,ry) -->
<!-- ``` -->


- adaptive importance sampling
    + $g_0(\bx) = t_{\alpha}(\bx;\mu_0,\Sigma_0)$ $\longrightarrow$ $g_1(\bx) = t_{\alpha}(\bx;\mu_1,\Sigma_1)$
    + select $\lambda=(\epsilon,\mu,\Sigma)$, $g(\bx; \lambda) = \epsilon g_0(\bx) + (1-\epsilon)t_{\alpha}(\bx;\mu,\Sigma)$ 
- rejection control (RC)
    + accept $\bx^{(j)}$ with probability $r^{(j)} = \min\{1,w^{(j)}/c\}$  
    + if accepted, update $w^{(*j)} = q_cw^{(j)}/r^{(j)}$, $q_c=\int\min\{1,w(\bx)/c\}g(\bx)d\bx$
- sequential importance sampling
    + target density $\pi(\bx) = \pi(x_1)\pi(x_2|x_1)\dots \pi(x_d|x_1,\dots,x_{d-1})$
    + trial density $g(\bx) = g_1(x_1)g_2(x_2|x_1)\dots g_d(x_d|x_1,\dots,x_{d-1})$
    + auxiliary distributions $\pi_t(\bx)$ approximate $\pi(\bx_t)$
    + $w_t(\bx_t)=w_{t-1}(\bx_{t-1})\frac{\pi(x_t|\bx_{t-1})}{g_t(x_t|\bx_{t-1})}$, $w_d(\bx) = w(\bx)$
    + draw $X_t=x_t$ from $g_t(x_t|\bx_{t-1})$, $w_t=w_{t-1}u_t$, $u_t=\frac{\pi_t(\bx_t)}{\pi_{t-1}(bx_{t-1})g_{t}(x_t|bx_{t-1})}$  
    + RC in SIS, RC$(t_k)$ at check point $0<t_1<t_2<\dots<t_k\le d$
    + growth method (inversely restricted sampling, Hammersley and Morton, 1954; biased sampling, Rosenbluth and Rosenbluth, 1955) \
    Self-Avoiding random Walk model $\pi(\bx) = Z_n^{-1}$\
    $\bbP(x_{t+1}|x_1,\dots,x_t) = n_t^{-1}$, $w(\bx) = n_1\times \dots \times n_{N-1}$
    SIS. sampling $g_t(x_t|\bx_{t-1})=n_{t-1}^{-1}$, auxiliary $\pi_t(\bx_t) = Z_t^{-1}$, marginal $\pi_t(\bx_{t-1}) = \sum_{x_t}\pi(\bx_{t-1},x_t) = \frac{n_{t-1}}{Z_t}$, conditional $\pi_t(x_t|\bx_{t-1})= n_{t-1}^{-1}$\
    $w_{t+1}=w_tn_t$, $w_N = \prod_{t\ge 2} \frac{1}{g_t(x_t|x_1,\dots,x_{t-1})}$, $\hat{Z}_N = \bar{w}^{-1}$
    + sequential imputation (Kong, liu and Wong, 1994)\
    draw $\by_{\mbox{mis}}$ from $g(\by_{\mbox{mis}}) = f(y_{\mbox{mis},1}|y_{\mbox{obs},1},\theta)\dots    f(y_{\mbox{mis},n}|\by_{\mbox{obs},n},\by_{\mbox{mis},{n-1}},\theta)$\
    calculate weight $w(\by_{\mbox{mis}}) = f(y_{\mbox{obs},1}|\theta)\dots f(y_{\mbox{obs},n}|\by_{n-1},\theta)$\
    $\bar{w} = \frac{1}{m}\sum_{i=1}^m w(\by_{\mbox{mis}}^{(i)}) \longrightarrow L(\theta|\by_{\mbox{obs}})$
    + nonlinear filtering
    state-space model (hidden Markov model) $\begin{cases}\mbox{(state equation):}& x_t\sim q_t(\cdot|x_{t-1},\theta) \\ \mbox{(observation equation):}& y_t\sim f_t(\cdot|x_{t},\phi) \end{cases}$\
    Gaussian $f_t, q_t$, linear state-space model, Kalman filter\
    bootstrap filter (particle filter) (Gordon, Salmond and Smith, 1993)\
    draw $x_{t+1}^{(*j)}$ from $q_t(x_{t+1}|x_t^{(j)})$, $w^{(j)}\propto f_t(y_{t+1}|x_{t+1}^{(*j)})$, resample $\{x_{t+1}^{(1)},\dots,x_{t+1}^{(m)}\}$ from $\{x_{t+1}^{(*1)},\dots,x_{t+1}^{(*m)}\}$ with probability proportional to $w^{(j)}$
    + probability dynamic system $\pi_t(\bx_t)$, either $\bx_{t+1}=(\bx_t,x_{t+1})$ or $\bx_{t+1}=\bx_t$\
    set $g_t(x_t|\bx_{t-1}) = \pi_t(x_{t}|\bx_{t-1})$, $w_t = \frac{\pi_t(\bx_{t-1})}{\pi_{t-1}(\bx_{t-1})}\frac{\pi_t(x_{t}|\bx_{t-1})}{g_t(x_t|\bx_{t-1})}$\
    unnormalized $q_t(\bx_{t}) =Z_t\pi_t(\bx_{t})$, $w_N = \prod_{t=1}^Nw_t = \frac{Z_N}{Z_1}\frac{\pi_N(\bx_N)}{g_1(x_1)\dots g_N(x_N|\bx_{N-1})}$, $\bbE(w_N) = \frac{Z_N}{Z_1}$\
    Prune-Enriched Rosenbluth Method (PERM) (Grassberger, 1997) $w_t>C_t$ split into $r$ copies with probability $w_t/r$, $w_t\le c_t$ keep or discard with probability 0.5    

# Metropolis algorithm
target distribution $\pi(\bx)=Z^{-1}\exp(-h(\bx))$

proposal function, trial proposal $T(\bx^{(t)},\bx')=T(\bx',\bx^{(t)})$, change $\Delta h = h(\bx') - h(\bx^{(t)})$

acceptance rejection ratio $r = \pi(\bx')/\pi(\bx^{(t)}) = \exp(\Delta h)$


1-D Ising model $U(\bx) = - J \sum_{s=1}^{d-1}x_sx_{s+1}$, $h(\bx)=JU(\bx)/(\beta T) =: -\mu \sum_{s=1}^{d-1}x_sx_{s+1}$ 

$$r = \pi(\bx')/\pi(\bx^{(t)}) = \begin{cases}\exp(-2\mu x_j^{(t)}(x_{j-1}^{(t)}+x_{j+1}^{(t)})) & j\ne 1,d\\
\exp(-2\mu x_j^{(t)}x_{j+1}^{(t)}) & j = 1\\
\exp(-2\mu x_j^{(t)}x_{j-1}^{(t)}) & j = d\end{cases}$$

```{r Ising}
mu <-  1
d <-  50
x <- rep(1,d)
nsamp <- 50000
s <- rep(0,nsamp+1)
s[1] <- sum(x)
for (i in 1:nsamp) {
  j <- sample(1:d)[1]
  if (j == 1) {
    r <- -2 * mu * x[j] * x[j+1]  
  } else if (j == d) {
    r <- -2 * mu * x[j] * x[j-1]
  } else {
    r <- -2 * mu * x[j] * (x[j-1] + x[j+1])
  }
  
  if (log(runif(1)) <= r) {
    x[j] <- -x[j]
  }
  s[i+1] <- sum(x)
}

plot(s[1:2000],type ='l', xlab = 'Iteration')
by10 <- seq(50, nsamp, by = 50)
acf(s[by10])

mu <-  2
s <- rep(0,nsamp+1)
s[1] <- sum(x)
for (i in 1:nsamp) {
  j <- sample(1:d)[1]
  if (j == 1) {
    r <- -2 * mu * x[j] * x[j+1]  
  } else if (j == d) {
    r <- -2 * mu * x[j] * x[j-1]
  } else {
    r <- -2 * mu * x[j] * (x[j-1] + x[j+1])
  }
  
  if (log(runif(1)) <= r) {
    x[j] <- -x[j]
  }
  s[i+1] <- sum(x)
}

plot(s[1:2000],type ='l', xlab = 'Iteration')
by10 <- seq(50, nsamp, by = 50)
acf(s[by10])
```

actual transition function $A(\bx,\by)$, $\int\pi(\bx)A(\bx,\by)d\bx = \pi(\by)$

detailed balance $\pi(\bx)A(\bx,\by) = \pi(\by)A(\by,\bx)$

Metropolis-Hastings $r(\bx,\by)= \min\left\{1, \frac{\pi(\by)T(\by,\bx)}{\pi(\bx)T(\bx,\by)} \right\}$, $A(\bx,\by) = \pi(\by)\min\left\{\frac{T(\bx,\by)}{\pi(\by)}, \frac{T(\by,\bx)}{\pi(\bx)} \right\}$,
$\pi(\bx)A(\bx,\by) = \min\left\{\pi(\bx)T(\bx,\by), \pi(\by)T(\by,\bx) \right\}$

Baker (1965) $r(\bx,\by)=  \frac{\pi(\by)T(\by,\bx)}{\pi(\by)T(\by,\bx)+\pi(\bx)T(\bx,\by)}$,
$A(\bx,\by) = \pi(\by)\frac{T(\bx,\by)T(\by,\bx)}{\pi(\by)T(\by,\bx)+\pi(\bx)T(\bx,\by)}$,
$\pi(\bx)A(\bx,\by) = \pi(\bx)\pi(\by)\frac{T(\bx,\by)T(\by,\bx)}{\pi(\by)T(\by,\bx)+\pi(\bx)T(\bx,\by)}$

Stein $r(\bx,\by)=  \frac{\delta(\bx,\by)}{\pi(\bx)T(\bx,\by)}$, symmetric $\delta(\bx,\by)$, $A(\bx,\by) = \pi(\by)\delta(\bx,\by)$,
$\pi(\bx)A(\bx,\by) = \pi(\bx)\pi(\by)\delta(\bx,\by)$

Random-walk Metropolis $\bx' =\bx^{(t)} + \epsilon_t$, $\epsilon_t\in g_{\sigma}(\cdot)$ a spherically symmetric distribution

Metropolized independence sampler $\by \sim g(\by)$, $r=\min\left\{1,\frac{w(\by)}{w(\bx^{(t)})}\right\}$, $w(\bx) = \pi(\bx)/g(\bx)$ 

Configurational bias Monte Carlo (CBMC)

- auxiliary distribution $\pi_1(x_1),\pi_2(x_1,x_2),\dots,\pi_{d-1}(\bx_{d-1}),\pi(\bx)$
- trial sampling distribution $g(\bx)=g_1(x_1)g_2(x_2|x_1)\dots g_d(x_d|\bx_{d-1})$
- importance weight $w(\by) = \frac{\pi(\by)}{g(\by)} = \frac{\pi_1(y_1)}{g_1(y_1)}\frac{\pi_2(y_1,y_2)}{g_2(y_2|y_1)\pi_1(y_1)}\dots\frac{\pi_d(y_1,\dots,y_d}{g_d(y_d|\by_{d-1})\pi_{d-1}(\by_{d-1})}$


multiple-try Metropolis (MTM) 

- draw $\by_1\dots,\by_k\sim T(\bx,\cdot)$ and compute $w(\bx,\by)=\pi(\bx)T(\bx,\by)\lambda(\bx,\by)$, ssymmetric $\lambda(\bx,\by)>0$ 
- draw $\by$ from $\{\by_1\dots,\by_k\}$ with probability $w(\by_j,\bx)$ and draw reference set  $\bx_1^*\dots,x_k^*\sim T(\by,\cdot)$, $\bx_k^*=\bx$
- accept $\by$ with generalized M-H ratio $r_g = \min\left\{1, \frac{w(\by_1,\bx)+\cdots+w(\by_k,\bx)}{w(\bx_1^*,\by)+\cdots+w(\bx_k^*,\by)}\right\}$

orientational bias Monte Carlo (OBMC) $\lambda(\bx,\by)=T^{-1}(\bx,\by)$

multiple-trial Metropolis independence sampler (MTMIS)

- draw $\by_j\sim p(\by)$ and compute $w(\by_j)=\pi(\by_j)/p(\by_j)$, $W=\sum w(\by_j)$
- draw $\by$ from $\{\by_1\dots,\by_k\}$ with probability $w(\by_j)$
- $\bx^{(t+1)}=\by$ with probability $\min\left\{1, \frac{W}{W-w(\by)+w(\bx)}\right\}$ and $\bx^{(t+1)}=\bx$ otherwise

multipoint method 

- draw $\by_j\sim P_j(\cdot|\bx,\by_1,\dots,\by_{j-1})=P_1(\by_1|\bx)$ and compute $w(\bx,\by_{[1:j]})=\pi(\bx)P_j(y_{[1:j]}|\bx)\lambda_j(\bx,y_{[1:j]})$, $\lambda_j(a,b,\dots,z)=\lambda_j(z,\dots,b,a)$ sequentially symmetric
- draw $\by$ from $\{\by_1\dots,\by_k\}$ with probability $w(\by_{[t:1]},\bx)$
- draw reference set $\bx_m^*\sim P_m(\cdot|\by,\bx_{[1:m-1]}^*)$, $m=j+1,\dots,k$, $\bx_l^*=\by_{j-l}$, $l=1,2,\dots,j-1$
- $\bx^{(t+1)}=\by$ with probability $r_{mp}=\min\left\{1, \frac{\sum_{l=1}^k w(\by_{[l:1]},\bx)}{\sum_{l=1}^kw(\bx_{[l:1]}^*,\by)}\right\}$ and $\bx^{(t+1)}=\bx$ otherwise

random-grid method

- generate direction $\be$ and a grid size $r$
- candidate set $\by_l = \bx + l \cdot r\cdot \be$
- draw $\by$ from $\{\by_1\dots,\by_k\}$ with probability $u_j\pi(\by_{j})$
- reference set $\bx_l^* = \by - l \cdot r\cdot \be$
- accept $\by$ with probability $\min\left\{1, \frac{\sum_{l=1}^k \pi(\by_l)}{\sum_{l=1}^k\pi(\bx_l^*)}\right\}$ 

MCMC estimation of $\bbE_{\pi}h(\bx)$, $m\Var\left(\sum h(\bx^{(l)}\right)=\sigma^2\left[1+ 2 \sum_{j=1}^{m-1}\left(1-\frac{j}{m}\right)\rho_j\right] \approx \sigma^2\left[1+ 2 \sum_{j=1}^{m-1}\rho_j\right]$, $\sigma^2=\Var(h(\bx))$, $\rho_j=\mbox{Corr}(h(\bx^{(1)}),h(\bx^{(j+1)}))$

integrated autocorrelation time $\tau_{\mbox{int}}(h) = \frac{1}{2} + \sum_{j=1}^{\infty}\rho_j$, effective sample size $m/[2\tau_{\mbox{int}}(h)]$

exponential autocorrelation time $\tau_{\mbox{exp}}(h) = \lim\sup_{j\to\infty}\frac{j}{-\log|\rho_j|} \approx \tau_{\mbox{int}}(h)$

relaxation time $\tau_{\mbox{exp}} =\sup_{h\in L^2(\pi)}\tau_{\mbox{exp}}(h)$

$\rho_j(h)=\lambda^j$, $\tau_{\mbox{int}}(h) = \frac{1+\lambda}{2(1-\lambda)}$, $\tau_{\mbox{exp}}(h) = -\frac{1}{\log(|\lambda|)}$, $\tau_{\mbox{exp}} = -\frac{1}{\log(|\lambda_2(T)|)}$, 

### Gibbs sampler

random-scan / systematic-scan

slice sampler $S = \{\by\in\bbR^{d+1}:y_{d+1}\le\pi(y_1,\dots,y_d) \}$

- draw $y^{t+1}\sim U(0,\pi(x^{(t)}))$ 
- draw $\bx^{t+1}\sim U(S)$ 

Metropolized Gibbs sampler $\min\left\{1, \frac{1-\pi(x_i|\bx_{[-i]})}{1-\pi(y_i|\bx_{[-i]})}\right\}$ 

random-ray Monte Carlo (hit-and-run algorithm)

- draw $\by_i\sim T_e(\bx^*,\cdot)$ along direction $e$, $\by_i=\bx+r_j\be$, $r_j\sim U(-\sigma,\sigma)$ 
- MTM draw $\by^*$ from $\{\by_1\dots,\by_k\}$ with probability $\pi(\by_j)$, draw reference $\bx'_1,\dots,\bx'_{k-1}\sim T_e(\by^*,\cdot)$, $\bx^*=\bx'_k$,
$r = \min\left\{1, \frac{\sum_{l=1}^k \pi(\by_j)T_e(\by_j,\bx^*)}{\sum_{l=1}^k\pi(\bx'_j)T_e(\bx'_j,\by^*)}\right\}$

data augmentation $p(\theta|\by_{\mbox{obs}}) = \int p(\theta|\by_{\mbox{mis}},\by_{\mbox{obs}})p(\by_{\mbox{mis}}|\by_{\mbox{obs}})d\by_{\mbox{mis}}$

$g(\theta)$ approximates $p(\theta|\by_{\mbox{obs}})$. draw $p(\by_{\mbox{mis}})=\int p(\by_{\mbox{mis}}|\theta,\by_{\mbox{obs}})g(\theta)d\theta$

algorithm: $\by_{\mbox{mis}}^{(t-1,l)}\Longrightarrow \theta^*\sim p(\theta|\by_{\mbox{obs}},\by_{\mbox{mis}}^{(t-1,l)})\Longrightarrow \by_{\mbox{mis}}^{(t,j)}\sim p(\by_{\mbox{mis}}|\by_{\mbox{obs}},\theta^*)$

equivalent to Gibbs sampler $\by_{\mbox{mis}}^{(t-1)}\Longrightarrow \theta^{(t)}\sim p(\theta|\by_{\mbox{obs}},\by_{\mbox{mis}}^{(t-1)})\Longrightarrow \by_{\mbox{mis}}^{(t)}\sim p(\by_{\mbox{mis}}|\by_{\mbox{obs}},\theta^{(t)})$

data augmentation draw $x_1^{(t+1)}\sim  \pi(\cdot|x_2^{(t)}$, $x_2^{(t+1)}\sim  \pi(\cdot|x_1^{(t+1)})$. $L^2_0(\pi)=\{h(\bx)\in L^2(\pi): \bbE\{h(\bx)=0\} \}$
$$\begin{aligned}
\Cov (h(x_1^{(0)}),h(x_1^{(1)})) &= \Var_{\pi}(\bbE_{\pi}\{h(x_1)|x_2\}) \\
\Cov (h(x_1^{(0)}),h(x_1^{(n)})) &= \Var_{\pi}(\bbE_{\pi}\{\dots\bbE_{\pi}\{\bbE_{\pi}\{\bbE_{\pi}\{h(x_1)|x_2\}|x_1\}|x_2\}\dots\}) 
\end{aligned}$$

random-scan
$$\begin{aligned}
\Cov (h(\bx^{(0)}),h(\bx^{(1)})) &= \bbE_{\pi}\{\bbE^2_{\pi}\{h(\bx)|i,\bx_{-i}\}\} \\
\Cov (h(\bx^{(0)}),h(\bx^{(n)})) &= \Var_{\pi}(\bbE_{\pi}\{\dots\bbE_{\pi}\{\bbE_{\pi}\{\bbE_{\pi}\{h(\bx)|i,\bx_{-i}\}|\bx\}|i,x_{-i}\}\dots\})
\end{aligned}$$

histogram estimator $\hat{I} = m^{-1}\sum_{k=1}^mh(x_1^{(k)})$, $m^2\Var(\hat I) = m^2\sigma^2_0 + 2(m-1)\sigma^2_1 + 2\sigma^2_{m-1}$, $\sigma_k^2=\Cov(h(x_1^{(0)}),h(x_1^{(k)}))$

mixture estimator $\tilde{I} = m^{-1}\sum_{k=1}^m\bbE\{h(x_1)|x_2^{(k)}\}$ Rao-Blackwellization, $m^2\Var(\tilde I) = m^2\sigma^2_1 + 2(m-1)\sigma^2_2 + 2\sigma^2_{m}\le m^2\Var(\hat I)$ due to monotonicity of the autocovariance

forward operator on $L^2(\pi)$ $Fh(\bx) = \int K(\bx,\by) h(\by)d\by = \bbE\{h(\bx^{(1)})|\bx^{(0)}=\bx\}$, $\|F\|=\sup_{h:\bbE\{h^2\}=1}\|Fh(\bx)\|=1$

forward operator on $L^2_0(\pi)$, $\lambda_1(F_0)=\lambda_2(F)$, spectral radius $\lim_{n\to\infty}\|F_0^n\|^{1/n}=r$ characterizes the rate of convergence of the Markov chain in both reversible and nonreversible cases

standard Gibbs sampler $F_s: x_1\rightarrow x_2\rightarrow\dots\rightarrow x_d$

grouping Gibbs sampler $F_s: x_1\rightarrow x_2\rightarrow\dots\rightarrow (x_{d-1},x_d)$

collapsed Gibbs sampler $F_s: x_1\rightarrow x_2\rightarrow\dots\rightarrow x_{d-1}$

do not introduce unnecessary components into a Gibbs sampler $\|F_c\| \le \|F_g\| \le \|F_s\|$ 


Swendsen-Wang algorithm, data augmentation, Ising model
$$\pi(\bx) \propto \exp(\beta J\sum_{l\sim l'}x_lx_{l'}) \propto \prod_{l\sim l'}\exp(\beta J(1+x_lx_{l'}))$$
$$\pi(\bx,\mathbf{u}) \propto \prod_{l\sim l'}I(0\le u_{l,l'} \le\exp(\beta J(1+x_lx_{l'})))$$
$$\pi(\bx,\mathbf{b}) \propto \prod_{l\sim l'}(1+b_{l,l'}(e^{2\beta J}-1))$$
bonding variable $b_{l,l'}=I(u_{l,l'}>1)$, $\bbP(b_{l,l'}=1)=e^{-2\beta J}$

partial resample on fiber $\mathcal{X}_{\alpha}$, $\mathcal{X} = \cup_{\alpha\in A}\mathcal{X}_{\alpha}$, $\mathcal{X}_{\alpha}\cap\mathcal{X}_{\beta} = \emptyset$,  $\pi(\bx) = \int \nu_{\alpha}(\bx)d\rho(\alpha)$

- Gibbs (axis) move $\alpha\in\bbR$, $\mathcal{X}_{\alpha}=\{\bx:x_1=\alpha\}$, $\nu_{\alpha}=\pi(x_2|x_1)I(x_1=\alpha)$

- $\alpha\in\bbR$, $\mathcal{X}_{\alpha}=\{\bx:x_2=x_1+\alpha\}$, $\nu_{\alpha}=\pi(x_1,x_2)I(x_2=x_1+\alpha)$

- $\alpha\in\bbR\backslash\{0\}$, $\mathcal{X}_{\alpha}=\{\bx:x_1=\alpha x_1\}$, $\nu_{\alpha}=|x_1|\pi(x_1,\alpha x_1)I(x_2=\alpha x_1)$

Gausian random field model $\pi(\bx) \propto \exp\left\{-\frac{1}{2}\sum_{s\sim s'}\beta_{ss'}(x_s-x_{s'})^2-\frac{1}{2}\sum_{s\in\Lambda}\gamma_{s}(x_s-\mu_{s})^2 \right\}$, $s,s'\in\Lambda\subset N\times N$ Markov random field (MRF)

- Gibbs $\pi(x_s|\bx_{[-s]}) \propto \exp\left\{-\frac{1}{2}\left(\gamma_s+\sum_{s'\sim s}\beta_{ss'}\right)\left(x_s-\frac{\gamma_s\mu_s+\sum_{s'\sim s}\beta_{ss'}x_{s'}}{\gamma_s+\sum_{s\sim s'}\beta_{ss'}}\right)^2\right\}$

- coarsing move $\bx \rightarrow (\bx_S+\delta,\bx_{[-S]})$, $\mathcal{X}_{\alpha}=\{\bx:x_{[-S]}=\alpha\}$, draw $p(\delta)\propto\pi(\bx_S+\delta,\bx_{[-S]})$, $\delta\sim N(\mu_*,\sigma_*^2)$, $\mu_*= \frac{\sum_{s'\sim s\in\partial S}\beta_{ss'}(x_s-x_{s'}) + \sum_{s\in S}\gamma_s\mu_s}{\sum_{s'\sim s\in\partial S}\beta_{ss'} + \sum_{s\in S}\gamma_s}$, $\sigma_*^2=\left[\sum_{s'\sim s\in\partial S}\beta_{ss'} + \sum_{s\in S}\gamma_s\right]^{-1}$

generalized Gibbs draw $\gamma$, set $\bx'=\gamma(\bx)$

locally compact group $\Gamma=\{\gamma\}$: locally compact space, group operation $\gamma_1\gamma_2(\bx)=\gamma_1(\gamma_2(\bx))$, continuous $(\gamma_1,\gamma_2)\rightarrow \gamma_1\gamma_2$ and $\gamma\rightarrow\gamma^{-1}$.

left Haar measure $L(B) = \int_BL(d\gamma)=\int_{\gamma_0B}L(d\gamma)=L(\gamma_0B)$, $\gamma_0\in\Gamma$, $B\subset \Gamma$

$\bx\sim\pi(\bx)$, $\gamma\sim p_{\bx}(\gamma)\propto\pi(\gamma(\bx))|J_{\gamma}(\bx)|L(d\gamma)\Longrightarrow\bx'=\gamma(\bx)\sim\pi$

partial resampling $\mathcal{X}_{\alpha}=\{\bx\in\mathcal{X}:\bx=\gamma(\alpha) \}$, $\nu_{\alpha}(\bx)\propto \pi(\gamma(\alpha))|J_{\alpha}(\gamma)|L(d\gamma)I(\bx=\gamma(\alpha))$

Metrpolis $A_{\bx}(\gamma,\gamma')L(d\gamma)$ such that $p_{\bx}(\gamma)d\gamma\propto\pi(\gamma(\bx))|J_{\gamma}(\bx)|L(d\gamma)$ invariant and $A_{\bx}(\gamma,\gamma') = A_{\gamma_0^{-1}\bx}(\gamma\gamma_0,\gamma'\gamma_0)$. If $\bx\sim\pi$ and $\gamma\sim A_{\bx}(\gamma_{\mbox{id}},\gamma)$, then $w=\gamma(\bx)\sim\pi$

parameter expanded data augmentation (PX-DA) draw $\by_{\mbox{mis}}\sim f(\by_{\mbox{mis}}|\theta,\by_{\mbox{obs}})$, draw $\alpha\sim p(\alpha|\by_{\mbox{obs}},\by_{\mbox{mis}})\propto f(\by_{\mbox{obs}},\gamma_{\alpha}(\by_{\mbox{mis}}))|J_{\alpha}(\by_{\mbox{mis}})|dH(\alpha)$, compute $\by'_{\mbox{mis}}=\gamma_{\alpha}(\by_{\mbox{mis}})$, draw $\theta\sim f(\theta|\by_{\mbox{obs}},\by_{\mbox{mis}})$

### hybrid Monte Carlo (HMC) with $L$ (40-70) steps of deterministic Hamiltonian moves

molecular dynamics - Monte Carlo

ergodicity theorem $\lim\limits_{t\to\infty}\frac{1}{t}\int_0^th(\bx_s)ds = Z^{-1}\int h(\bx)\exp(-U(\bx)/\beta T)d\bx$

Newtonian mechanism $d$-dimensional position $\bx(t)$, $d=3N$, $\mathbf{v}(t)=\dot{\bx}(t)$, Newton's law of motion $\mathbf{F}=\mathbf{m}\mathbf{\dot{v}}(t)$, momentum $\mathbf{p}=\mathbf{m}\mathbf{v}$, kinetic energy $k(\mathbf{p})=\frac{1}{2}\left\|\frac{\mathbf{p}}{\sqrt{\mathbf{m}}}\right\|^2$, total energy $H(\bx,\bp)=U(\bx)+k(\bp)$, Hamilton equation $\begin{cases} \dot{\bx}(t)=\frac{\partial H(\bx,\bp)}{\partial \bp}\\\dot{\bp}(t)=-\frac{\partial H(\bx,\bp)}{\partial \bx} \end{cases}$

Verlet algorithm $\bx(t+\Delta t) = 2\bx(t) - \bx(t-\Delta t) - \frac{1}{\bm}\frac{\partial H}{\partial \bx}\Big|_t(\Delta t)^2$, $\bp(t+\Delta t) = \bm \frac{\bx(t+\Delta t) -\bx(t-\Delta t)}{2 \Delta t}$

leap-frog method (Hockney 1970) $\bx(t+\Delta t) = \bx(t)  + \Delta t \frac{\bp(t+\frac{1}{2}\Delta t)}{\bm}$, $\bp(t+\frac{1}{2}\Delta t) = \bp(t-\frac{1}{2}\Delta t) + \frac{\partial H}{\partial \bx}\Big|_t \Delta t$

volume preservation $|V(t)|=\int_{V(t)}d\bx d\bp = \int_{V(0)}d\bx d\bp = |V(0)|$, $V(t) = \{(\bx(t),\bp(t)): (\bx(0),\bp(0))\in V(0)\}$

HMC draw $\bp\sim\phi(\bp)\propto\exp\{-k(\bp)\}$, leap-frog $L$ steps $(\bx,\bp)\to(\bx',\bp')$, acceptance ratio $\min\{1,\exp(-H(\bx',\bp')+H(\bx,\bp))\}$

Langevin-Euler move $d\bx_t = -\frac{1}{2}\frac{\partial U(\bx_t)}{\partial \bx}dt + dW_t \Longrightarrow X_t\sim\pi$, discretization $\bx_{t+1}=\bx_{t} -\frac{1}{2}\frac{\partial U(\bx_t)}{\partial \bx}h +\sqrt{h}Z_t$

generalized HMC $\phi=(\bx,\bp)$, $\pi^*(\phi)T(\phi,\phi')=\pi^*(\phi')T(\phi',\phi)$, acceptance ratio $\min\left\{1,\frac{\pi(\phi^{(1)})/\pi^*(\phi^{(k)})}{\pi(\phi^{(0)})/\pi^*(\phi^{(0)})}\right\}$

surrogate transimition method, a reversible Markov transition $S(\bx,\by)$ leaving $\pi^*(\bx)\propto\exp\{-h^*(\bx)\}$ invariant, $\pi^*(\bx)S(\bx,\by)=\pi^*(\by)S(\by,\bx)$, draw $\by_i\sim S(\by_{i-1},\cdot)$ with acceptance ratio $\min\left\{1,\frac{\pi(\by_k)/\pi^*(\by_k)}{\pi(\bx^{(t)})/\pi^*(\bx^{(t)})}\right\}$, actual transition $A(\bx,\by) = S^{(k)}(\bx,\by)\min\left\{1,\frac{\pi(\by)/\pi^*(\by)}{\pi(\bx)/\pi^*(\bx)}\right\}$

Neal (1994) window method: choose $W<L$, draw $K$ from $\{0,\dots,W-1\}$, obtain trjectory $\{\phi(l):l=-K,\dots,L-K\}$ from $\phi(0)=(\bx^{(t)},\bp^{(t)})$, compute free energy $F(\mathcal{W})=-\log\left\{\sum_{\phi(j)\in\mathcal{W}}\exp(-H(\phi(j)))\right\}$ for acceptance window $\mathcal{A}=\{\phi(l):l=L-K-W+1,\dots,L-K\}$ and rejection window $\mathcal{R}=\{\phi(l):l=-K,\dots,-K+W-1\}$, go to acceptance window with probability $\min\{1,\exp(F(\mathcal{A})-F(\mathcal{R}))\}$ and rejection window otherwise, select state $\phi$ within chosen window with probability $\exp(-H(\phi(j))+F(\mathcal{W}))\}$

multipoint method: obtain trjectory $\{\phi(l):l=1,\dots,L\}$ from $\phi(0)=(\bx^{(t)},\bp^{(t)})$, select $\phi'=\phi(L-K)$ from $\{\phi(l):l=L-M+1,\dots,L\}$ with Boltzmann probabilities $\bbP(\phi'=\phi(L-M+k))\sim w_k\exp(-H(\phi(L-M+k)))$, obtain trajectory $\{\phi(l):l=-1,\dots,-K\}$, accept $\phi'$ with probability $p=\min\left\{1,\frac{\sum_{j=1}^Mw_j\exp(-H(\phi(L-M+j)))}{\sum_{j=1}^Mw_j\exp(-H(\phi(M-K-j)))}\right\}$ and $\phi(0)$ otherwise, renew $\bp^{(t+1)}\sim N(0,\Sigma)$ with $\Sigma=\mbox{diag}(m_1^{-1},\dots,m_d^{-1})$ 

umbrella sampling (Torrie and Valleau 1977) 

- estimand $A=\frac{\int q_1(\bx)d\bx}{\int q_0(\bx)d\bx} = \frac{Z_1}{Z_0}=\bbE_0\left\{\frac{q_1(\bx)}{q_0(\bx)}\right\} = \frac{\bbE_u\{q_1(\bx)/q_u(\bx)\}}{\bbE_u\{q_0(\bx)/q_u(\bx)\}}$
- umbrella distribution $\pi_u(\bx)\propto w(\Delta h(\bx))\pi_0(\bx)$, $\pi_i(\bx)\propto\exp\{-h_i(\bx)/kT_i\}$, $\Delta h(\bx) = h_1(\bx)/kT_1 - h_0(\bx)/kT_0$
- when $h_0(\bx)=h_1(\bx)$ but $T_0\ne T_1$, $\pi_{\alpha_i}(\bx)\propto \exp\{-h_0(\bx)/T_{\alpha_i}\}$, $T_0>T_{\alpha_1}>\dots>T_{1}$, $0<\alpha_1<\dots<\alpha_{k-1}=1$, $\hat{A}=\frac{\hat{c}_{\alpha_1}}{\hat{c}_{0}}\times\dots\times\frac{\hat{c}_{1}}{\hat{c}_{\alpha_{k-1}}}$
- bridge sampling $A=\frac{c_1}{c_0}=\frac{\bbE_0\{q_1(\bx)\alpha(\bx)\}}{\bbE_1\{q_0(\bx)\alpha(\bx)\}}$,  $\hat{A}_{BS}=\frac{\sum_{l=1}^{m_0}q_1(\bx_0^{(l)})\alpha(\bx_0^{(l)})/n_0}{\sum_{l=1}^{m_1}q_1(\bx_1^{(l)})\alpha(\bx_1^{(l)})/n_1}$, $\alpha(\bx)\propto\{m_0\pi_0(\bx)+m_1\pi_1(\bx)\}^{-1}$

simulated annealing (SA) (Kirkpatrick et al. 1983)

- minimum of $h(\bx)$
- initialize $\bx^{(0)}$ and $T_1$, $N_k$ MCMC iterations $\pi_k(\bx)$ and pass final configuration to next interation
- global minimum of $h(\bx)$ with probability 1 if $T_k=O(\log(L_k^{-1}))$ with $L_k=N_1+\dots+N_k$

simulated tempering (ST) (Parisi 1992, Geyer and Thompson 1995)

- $\Pi=\{\pi_i(\bx)\propto \exp(-h(\bx)/T_i),i\in I\}$, target distribution when temperature is lowest 
- $\pi_{st}(\bx,i)\propto c_i\exp(-h(\bx)/T_i)$, $c_i\propto Z_i^{-1}$
- $(\bx^{(t)},i^{(t)})=(\bx,i)$, draw $u\in U(0,1)$, if $u \le \alpha_0$, $i^{(t+1)}=i$ and draw $\bx^{(t+1)}\sim T_i(\bx,\bx^{(t+1)})$ leaving $\pi_i$ invariant; if $u > \alpha_0$, $\bx^{(t+1)}=\bx$ and draw $i'\sim \alpha(i,i')$ and accept $i^{(t+1)}=i'$ with $\min\left\{1,\frac{c_{i'}\pi_{i'}(\bx)\alpha(i',i)}{c_{i}\pi_{i}(\bx)\alpha(i,i')}\right\}$

parallel tempering (PT) (Greyer 1991)

- $\pi_{pt}(\bx_1,\dots,\bx_I)=\prod_{i\in I}\pi_i(\bx_i)$
- $(\bx_1^{(t)},\dots,\bx_I^{(t)})$, draw $u\sim U(0,1)$,  if $u \le \alpha_0$, parallel step,  draw each $\bx^{(t+1)}$ via MCMC; if $u > \alpha_0$, swapping step, swap neighhoring pair $\bx_i^{(t)}$ and $\bx_{i+1}^{(t)}$ with  $\min\left\{1,\frac{\pi_{i}(\bx_{i+1}^{(t)})\pi_{i+1}(\bx_i^{(t)})}{\pi_{i}(\bx_{i}^{(t)})\pi_{i+1}(\bx_{i+1}^{(t)})}\right\}$

canonical ensemble simulation - Boltzmann $\pi(\bx)=Z^{-1}\exp(-\beta H(\bx))$, $\beta = 1/kT$, canonical ensemble assumption constant-Number Volume Temperature

- multicanonical sampling (Beg and Neuhaus 1991) $U=H(\bx)\sim Z^{-1}\Omega(u)\exp(-\beta u)$, density of states (spectral density) $\Omega(u)$. draw $\bx'\sim\exp(-S(h(\bx)))$, $S(u)=\log\Omega(u)$, then $\pi'(u)\propto c$
- $1/k$-ensemble sampling (Hesselbo and Stinchcombe 1995) $\pi^*(\bx)$ such that entropy $S=S(H(\bx))=\log(\Omega(H(\bx)))\propto c$, then $\pi_S^*(u)\propto \frac{d \log\Omega(u)}{du}$. $\pi^*(\bx)\propto 1/k(H(\bx))$, $k(H)=\int_{-\infty}^H\Omega(H')dH'$ number of configurations with smaller or equal energy, $P_{1/k}(u)\propto \frac{\Omega(u)}{k(u)}=\frac{d\log k(u)}{du}$

adaptive direction sampling (ADS) (Gilks et al. 1994)

- snooker algorithm $\mathcal{S}^{(t)}=\{\bx^{(t,1)},\dots,\bx^{(t,m)}\}$, draw a stream $\bx^{(t,c)}$ and anchor $\bx^{(t,a)}$, $\be=(\bx^{(t,c)}-\bx^{(t,c)})/\|\bx^{(t,c)}-\bx^{(t,a)}\|$, draw $r\sim f(r)$, update $\bx^{(t+1,c)}=\bx^{(t,a)}+r\be$ 

conjugate gradient Monte Carlo (CGMC) draw $\bx^{(t,a)}\in\mathcal{S}^{(t)}$, find local mode or high density value point $\by$ of $\pi$ by gradient or conjugate gradient, choose $\bx^{(t,c)}\in\mathcal{S}^{(t)}\backslash\{\bx^{(t,a)}\}$, $\be=(\by-\bx^{(t,c)})/\|\by-\bx^{(t,c)}\|$, sample along $\bx^{(t+1,c)}=\by+r\be$ by MTM with $f(r)\propto|r|^{d-1}\pi(\by+r\be)$, update other member of $\mathcal{S}^{(t)}$

evolutionary Monte Carlo (EMC)

- target $\pi(\bx)\propto\exp(-H(\bx))$
- Gibbs distribution $\pi_i(\bx_i)=Z_i(t_i)^{-1}\exp(-H(\bx_i)/t_i)$
- target of augmented system $\pi(\bx)=Z(t)^{-1}\exp\left(-\sum_{l=1}^mH(\bx_i)/t_i\right)$
- mutation. select $\bx_k$ and flip some random position to $\by_k$ with MH ratio $r_m=\exp(-(H(\by_k)-H(\bx_k))/t_k)$
- crossover $(\bx_i,\bx_j)$ with MH ratio $r_c=\exp\left(-\frac{H(\by_i)-H(\bx_i)}{t_i}-\frac{H(\by_j)-H(\bx_j)}{t_j}\right)\frac{T(\mathbf{Y},\mathbf{X})}{T(\mathbf{X},\mathbf{Y})}$
  - snooker crossover $f(r)\propto|r|^{d-1}\pi(\bx_j+r\be)$
- exchange $(\bx_i,\bx_j)$ with MH ratio


variation distance ($L^1$-distance) $\|P-Q\|_{\mbox{var}}=\sup\limits_{S\in\mathcal{X}}|P(S)-Q(S)|=\frac{1}{2}\sum\limits_{\bx\in\mathcal{X}}|P(\bx)-Q(\bx)|=\frac{1}{2}\|P-Q\|_{L^1}=\frac{1}{2}\int|p(\bx)-q(\bx)|d\bx$

$\chi^2$-distance $\|P-Q\|^2_{\chi^2}=\Var_{P}\{Q(\bx)/P(\bx)\}=\sum\limits_{\bx\in\mathcal{X}}|Q(\bx)-P(\bx)|^2/P(\bx)$

$Q(\bx,\by)=\pi(\bx)A(\bx,\by)=\pi(\by)A(\by,\bx)$, $1=\beta_1>\beta_2\ge \dots\ge\beta_{m-1}\ge -1$, $m=|\mathcal{X}|$. 

Laplacian $L=I-A$ with $\lambda_i=1-\beta_i$, $\lambda_1=\inf\limits_{\Var(\phi)>0}\frac{\mathcal{E}(\phi,\phi)}{\Var(\phi)}$, quadratic form of Laplacian $\mathcal{E}(\phi,\phi)=\frac{1}{2}\sum\limits_{\bx,\by}[\phi(\bx)-\phi(\by)]^2Q(\bx,\by)$. 

length of path $\gamma_{xy}$ $\|\gamma_{xy}\|=\sum\limits_{e\in\gamma_{xy}}Q(e)^{-1}$, $\kappa=\kappa(\Gamma)=\max\limits_e\sum\limits_{\gamma_{xy}\ni e}\|\gamma_{xy}\|\pi(\bx)\pi(\by)$. $K=\max\limits_eQ(e)^{-1}\sum\limits_{\gamma_{xy}\ni e}|\gamma_{xy}|\pi(\bx)\pi(\by)$.
Poincar\'e inequality $\beta_1\le 1- \kappa^{-1}$ and $\beta_1\le 1- K^{-1}$ 

$S\in\mathcal{X}$, $Q(S,S^c)=\sum\limits_{\bx\in S}\sum\limits_{\by\in S^c}Q(\bx,\by)$. the conductance of the chain $h=\min\limits_{S:\pi(S)\le 1/2} \frac{Q(S,S^c)}{\pi(S)}$. Cheeger's inequality $1-2h\le\beta_1\le 1-\frac{h^2}{2}$

$\eta=\max\limits_eQ(e)^{-1}\sum\limits_{\gamma_{xy}\ni e}\pi(\bx)\pi(\by)$. $\beta_1\le 1- 1/8\eta^2$

$h(\bx)\in L_0^2(\pi)$, forward operator $Fh(\bx)=\int h(\by)A(\bx,\by)d\by=\bbE\{h(\bx^{(1)})|\bx^{(0)}=\bx\}$, backward operator $Bh(\by)=\int h(\bx)\frac{A(\bx,\by)\pi(\bx)}{\pi(\by)}d\bx=\bbE\{h(\bx^{(0)})|\bx^{(1)}=\by\}$.

$\|F\|=\max\limits_{\|h\|=1}\|Fh\|$, spectral radius $r_F=\lim\limits_{n\to\infty}\|F^n\|^{1/n}$. $\langle Fh,g\rangle =\langle h,Bg\rangle$. 

$\bx^{(0)}\sim\pi$. $h,g\in L_0^2(\pi)$. $\Cov(h(\bx^{(n)}),h(\bx^{(0)}))=\Cov_{\pi}(F^kh(\bx),B^{n-k}h(\bx))$, $0\le k\le n$.

reversible Markov chain, $F=B$. $\bx^{(0)}\sim\pi$. $h\in L_0^2(\pi)$. $\Cov(h(\bx^{(0)}),h(\bx^{(2m)}))=\bbE_{\pi}\{[F^mh(\bx)]^2\}=\bbE_{\pi}\{[B^mh(\bx)]^2\}= \Var(\bbE\{\dots\bbE\{\bbE\{h(\bx^{(0)})|\bx^{(1)}\}|\bx^{(2)}|\dots|\bx^{(m)}\})$. $|\Cov(h(\bx^{(0)}),h(\bx^{(2m+1)}))|\le \Cov(h(\bx^{(0)}),h(\bx^{(2m)}))$

maximal correlation $\gamma=\sup\limits_{f\in L_x^2(\pi),g\in L_y^2(\pi)}\mbox{Corr}(f(\bx),g(\by))=\sup\limits_{\bbE\{h\}=0,\Var(h)=1}\Var(\bbE\{h(\bx)|\by\})$. $\|F^n\|=\|B^n\|=\gamma_n=\sup\limits_{f,g\in L^2(\pi)}\mbox{Corr}(f(\bx^{(0)}),g(\bx^{(n)}))$

reversible Markov chain $\rho_n(h)=\mbox{Corr}(F^nh(\bx),h(\bx))$, $\lim\limits_{n\to\infty}|\rho_n(h)|^{1/n}=|\lambda_1|$

irreducible aperiodic finite-state Markov chain $\bar{h}_m\to\bbE_{\pi}\{h\}$ a.s. $\sqrt{m} [\bar{h}_m-\bbE_{\pi}\{h\}]\to N(0,\sigma^2(h))$, $\sigma(h)^2=\sigma^2\left[1+2\sum\limits_{j=1}^{\infty}\rho_j\right]=2\tau_{\mbox{int}}(h)\sigma^2$, $\rho_j=\mbox{Corr}(h(\bx^{(1)}),h(\bx^{(j+1)}))$

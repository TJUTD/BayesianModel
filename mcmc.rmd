---
title: Monte Carlo Strategies
output: html_document
---

```{R setup, include = FALSE}
knitr::opts_chunk$set(comment = NA, prompt = TRUE)
```

\newcommand{\bbP}{\mathbb{P}}
\newcommand{\bbE}{\mathbb{E}}
\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\bbR}{\mathbb{R}}

\newcommand{\hmu}{\hat{\mu}}
\newcommand{\tmu}{\tilde{\mu}}

\newcommand{\cS}{\mathcal{S}}
\newcommand{\cX}{\mathcal{X}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}

Boltzmann distribution (Gibbs distribution) $\pi(\bx) = \frac{1}{Z}e^{-\frac{U(\bx)}{kT}}$

- configuration of a physical system $\bx$ 
- potential energy $U(\bx)$
- temperature $T$
- Boltzmann constant $k$
- partition function $Z=Z(T)$
- internal energy $\langle U\rangle = \bbE_{\pi}\{U(\bx)\}$
- set $\beta = \frac{1}{kT}$, $\frac{\partial \log(Z)}{\partial \beta}= - \langle U\rangle$
- free energy $F=-kT\log(Z)$
- specific heat of the system $C = \frac{\partial \langle U\rangle}{\partial T}       =\frac{1}{kT^2}\Var_{\pi}\{U(\bx)\}$
- the system's entropy $S = (\langle U\rangle - F)/T$

$2$-$D$ Ising model on a $N\times N$ lattice space $\mathcal{L}=\{(i,j),i,j=1,\dots,N \}$
$$U(\bx) = -J\sum_{\sigma\sim\sigma'} x_{\sigma}x_{\sigma'} + \sum_{\sigma}h_{\sigma}x_{\sigma}$$

- configuration of the whole system $\mathbf{x}=$ $\{$ $x_{\sigma}:$ $\sigma \in \mathcal{L}$ $\}$
- a particle at site $\sigma$ has either a positive or a negative spin  $x_{\sigma}\in\{+1,-1\}$  
- sites $\sigma,\sigma'\in\mathcal{L}$ are a neighboring pair $\sigma\sim\sigma'$
- interaction strength $J$
- external magnetic field $h_{\sigma}$
- mean magnetization per spin $\langle m\rangle = \bbE_{\pi}\{\frac{1}{N^2}\left|\sum_{\sigma\in S}x_{\sigma}\right|\}$

simple liquid model $\mathbf{x} = \{x_i\in \bbR^{3}: i=1,\dots,k\}\in \bbR^{3k}$ 

- energy $U(\mathbf{x}) = \sum_{i,j}\Phi(|x_i-x_j|) = \sum_{i,j}\Phi(r_{ij})$
- Lennard-Jobes pair potential $\Phi(r) = 4\epsilon\left[\left(\frac{\sigma}{r}\right)^{12} -\left(\frac{\sigma}{r}\right)^{6} \right]$

macromolecules model

- energy $U(\mathbf{x}) = \sum_{\mbox{bonds}}\{\mbox{bond terms}\} + \sum_{i,j}\left[\Phi(r_{ij}) + \frac{q_iq_j}{4\pi\epsilon_0r_{ij}}\right]$
- bond terms $=\sum_{\mbox{bonds}} \frac{k_i}{2}(l_i-l_{i,0})^2 + \sum_{\mbox{angles}} \frac{k_i}{2}(\theta_i-\theta_{i,0})^2 + \sum_{\mbox{torsions}} v(\omega_i)$
- bond length $l_i$, bond angle $\theta_i$, torsion angle $\omega_i$
- torsion term $v(\omega_i) = \frac{V_n}{2}(1+\cos(n\omega -\gamma))$

### variance reduction methods

stratified sampling

- estimand $\int_{\cX}f(x)dx$
- partition $D_1,\dots,D_k$ of $\cX$
- $\hmu=\hmu_1+\dots+\hmu_k$, $\hmu_i= m_i^{-1}[f(X_{i,1})+\dots+f(X_{i,m_i})]$
- $\Var(\hat{\mu})=\frac{\sigma_1^2}{m_1} + \dots + \frac{\sigma_k^2}{m_k}$

control variates method

- estimand $\mu = \bbE\{X\}$
- control variate $C$ with known $\mu_C = \bbE\{C\}$
    + sample $X(b) = X + b(C-\mu_C)$
    + $\Var(X(b)) = \Var(X) + 2b\Cov(X,C) + b^2\Var(C)$
    + set $b=\Cov(X,C)/\Var(C)$, $\Var(X(b)) = (1-\rho_{XC}^2)\Var(X)$
- control variate $C$ with unknown $\bbE\{C\} = \mu$
    + sample $X(b) = bX + (1-b)C$
    + $\Var(X(b)) = b^2\Var(X) + 2b(1-b)\Cov(X,C) + (1-b)^2\Var(C)$
    + $b = [\Var(C) - \Cov(X,C)] / [\Var(X) - 2\Cov(X,C) + \Var(C)]$
		
antithetic variates method (Hammersley and Morton, 1956)

- sample pair $X=F^{-1}(U)$, $X'=F^{-1}(1-U)$ instead of two independent Monte Carlo draws for estimating $\bbE\{X\}$
- cdf $F$ is monotone, $\Cov(X,X')=\bbE\{[F^{-1}(U)-F^{-1}(U')][F^{-1}(1-U)-F^{-1}(1-U')]\}\le 0$

Rao-Blackwellization (Bickel and Doksum, 2000)

- estimand $I = \bbE\{h(X)\}$
- $X = (x_1,X_2)$. analytic $\bbE\{h(X)|X_2\}$ 
- histogram estimator $\hat{I} = \frac{1}{m}\sum_{l=1}^m h(X^{(l)})
- mixture estimator $\tilde{I} = \frac{1}{m}\sum_{l=1}^m \bbE\{h(X)|X_2^{(l)}\}$

chain-structure model

- $\pi(\bx) \propto \exp\left( -\sum_{i=1}^d h_i(x_{i-1},x_i) \right)$
- Markovian $\pi(x_i|\bx_{-i}) \propto \exp\left( -h_i(x_{i-1},x_i) -h_{i+1}(x_{i},x_{i+1}) \right)$
- hidden Markov model (HMM) when $x_i \in \mathcal{S} = \{s_1,\dots,s_k\}$
- optimization by dynamic programming $O(dk^2)$
$$m_1(x)=\min\limits_{s_i\in\cS}h_1(s_i,x),\quad m_t(x)=\min\limits_{s_i\in\cS}\{m_{t-1}(s_i)+h_t(s_i,x)\},\quad x=s_1,\dots,s_k$$
$$\hat x_1=\arg\min\limits_{s_i\in\cS} m_d(s_i),\quad \hat x_t=\arg\min\limits_{s_i\in\cS}\{m_{t}(s_i)+h_{t+1}(s_i,\hat x_{t+1})\},\quad t=d-1,\dots,1
$$
- exact simulation 
    - partition function $Z=\sum_{\bx}\exp(-H(\bx))=\sum_{x\in\cS} V_d(x)$
$$V_1(x) = \sum_{x_0\in\cS}e^{-h_1(x_0,x)},\quad V_t(x) = \sum_{y\in\cS}V_{t-1}(y)e^{-h_t(y,x)},\quad t=2,\dots,d$$
$$x_d \sim V_d(x)/Z,\quad x_t \sim \frac{V_{t}(x)e^{-h_{t+1}(x,x_{t+1})}}{\sum_{y\in\cS}V_{t}(y)e^{-h_{t+1}(y,x_{t+1})}},\quad t=d-1,\dots,1$$ 
    - Ising model $\pi(\bx) = Z^{-1}\exp(\beta(x_0x_1 + \dots + x_{d-1}x_d)), \quad x_i\in \{-1,+1\}$
$$V_1(x) = e^{\beta x} + e^{-\beta x} = e^{\beta} +e^{-\beta },\quad V_t(x) = (e^{\beta} + e^{-\beta})^t,\quad Z = 2(e^{\beta} + e^{-\beta})^d$$
    - graphical model, peeling algorithm / forward-summation-backward-sampling method
        - $\bx_C = \{x_i,i\in C\}$, clique $C\in \mathcal{C} \subset 2^{\{1,\dots,d\}}$ (Lauritzen and Spiegelhalter, 1998), connected  $C_i\cap C_j \ne \emptyset$

## importance sampling 
$$\mu=\bbE_{\pi}\{h(\bx)\}=\bbE_{g}\{w(\bx)h(\bx)\},\quad w=\pi(\bx)/g(\bx)$$

- normalized weight $\hmu = \frac{w^{(1)}h(\bx^{(1)})+\dots+w^{(n)}h(\bx^{(n)})}{w^{(1)}+\dots+w^{(n)}}=\frac{\overline{Z}}{\overline{W}}$
    - know $\pi(\bx)/g(\bx)$ up to a multiplicative constant
    - often a smaller MSE than $\tmu$
- unbiased estimate $\tmu = n^{-1}[w^{(1)}h(\bx^{(1)})+\dots+w^{(n)}h(\bx^{(n)})]$
- Effective Sample Size $=\frac{\Var_{\pi}(\bar{\mu})}{\Var_{g}(\hmu)}=\frac{n}{1+\Var_g(w(\bx))}$, direct sample $\bar{\mu}=n^{-1}[h(y^{(1)})+\dots+h(y^{(n)})]=\overline{H}$,  $y^{(i)}\sim\pi$
$$\begin{aligned}
  \Var_g(\hmu) &=\Var_g(\overline{Z}/\overline{W}) \approx \begin{bmatrix} \frac{1}{\bbE_g\{W\}} & -\frac{\bbE_g\{Z\}}{\bbE_g^2\{W\}} \end{bmatrix} \frac{1}{n}\begin{bmatrix} \Var_g(Z) & \Cov_g(Z,W)\\ \Cov_g(W,Z)   & \Var_g(W)  \end{bmatrix} \begin{bmatrix} \frac{1}{\bbE_g\{W\}} \\ -\frac{\bbE_g\{Z\}}{\bbE_g^2\{W\}} \end{bmatrix}\\
	&= \frac{1}{n}\left[ \frac{\Var_g(Z)}{\bbE_g^2\{W\}} - 2 \frac{\bbE_g\{Z\}\Cov_g(Z,W)}{\bbE_g^3\{W\}}  +\frac{\bbE_g^2\{Z\}\Var_g(W)}{\bbE_g^4\{W\}}\right]\\
	&= n^{-1}[ \Var_g(Z) - 2 \mu\Cov_g(Z,W)  + \mu^2\Var_g(W)]\\
	&= n^{-1}[ (\bbE_{\pi}\{WH^2\}-\mu) - 2 \mu(\bbE_{\pi}\{WH\} -\mu ) + \mu^2\Var_g(W)]\\	
	\bbE_{\pi}\{WH^2\}&\approx \bbE_{\pi}\{W\}\bbE_{\pi}^2\{H\} +\frac{1}{2} \mathrm{tr}\left(\begin{bmatrix} 0 & 2\bbE_{\pi}\{H\}\\
	2\bbE_{\pi}\{H\} & 2\bbE_{\pi}\{W\}\end{bmatrix} \begin{bmatrix} \Var_{\pi}(W) & \Cov_{\pi}(W,H)\\ \Cov_{\pi}(H,W) & \Var_{\pi}(H)  \end{bmatrix}\right) \\
	&= n^{-1}[ (\mu^2\bbE_{\pi}\{W\} + 2\mu\Cov_{\pi}(H,W) + \bbE_{\pi}\{W\}\Var_{\pi}(H)-\mu^2 )- 2 \mu(\Cov_{\pi}(WH) + \mu\bbE_{\pi}\{W\} -\mu ) + \mu^2\Var_{g}(W)]\\
	&= n^{-1}[ \bbE_{\pi}\{W\}\Var_{\pi}(H) +\mu^2(1-\bbE_{\pi}\{W\}+\Var_{g}(W))]\\
	&= n^{-1}[ \bbE_{\pi}\{W\}\Var_{\pi}(H) +\mu^2(1-\bbE_{g}\{W^2\}+\Var_{g}(W))]\\
	&= n^{-1} \bbE_{\pi}\{W\}\Var_{\pi}(H)\\
	&=\frac{1+\Var_g(W)}{n}\Var_{\pi}(H)
\end{aligned}
$$
- proper w.r.t $\pi$: $\bbE_g\{h(\bx^{(i)})w^{(i)}\} = c\bbE_{\pi}\{h(\bx)\}$, for all square integrable $h(\cdot)$ $\Longleftrightarrow$ $\frac{\bbE_{g}\{w|\bx\}}{\bbE_{g}\{w\}}g(\bx) = \pi(\bx)$

<!-- - missing data from a bivariate normal distribution -->
<!-- $$\begin{aligned} -->
<!-- y &\sim \mbox{N}(0,\Sigma),\quad \Sigma = \begin{bmatrix} \sigma_1^2, \rho\sigma_1\sigma_2\\ \rho\sigma_1\sigma_2 & \sigma_2^2\end{bmatrix}\\ -->
<!-- \pi(\Sigma) &\propto |\Sigma|^{-(d+1)/2}, \quad d=2\\ -->
<!-- \pi(\Sigma|y_1,\dots,y_n) &\propto |\Sigma|^{-(n+d+1)/2}\exp(-\mbox{tr}(\Sigma^{-1} S)/2)\sim\mbox{inverse Wishart}(n,S), \quad S = y^Ty\\ -->
<!-- \Sigma|\mathbf{y}_{\mbox{mis}},\mathbf{y}_{\mbox{obs}} &\sim \mbox{inverse Wishart}(n,S(\mathbf{y}_{\mbox{mis}})) \\ -->
<!-- y_{\mbox{mis}}|\Sigma,\mathbf{y}_{\mbox{obs}} &= y_{\mbox{mis}}|\Sigma,y_{\mbox{obs}}\sim \mbox{N}(\mu_*,\sigma_*),\quad \mu_*=\rho y_{\mbox{obs}} \sqrt{\sigma_{\mbox{obs}}/\sigma_{\mbox{mis}}},\sigma_*^2=(1-\rho^2)\sigma_{\mbox{obs}} \\ -->
<!-- \mbox{proposal }g:\ \Sigma &\sim \mbox{inverse Wishart}(n_{\mbox{obs}},S(\mathbf{y}_{\mbox{obs}})) -->
<!-- \end{aligned}$$ -->

<!-- ```{r misingdata, echo = FALSE} -->
<!-- library(knitr) -->
<!-- dat_mis = matrix(c(1,1,-1,-1,2,2,-2,-2,NA,NA,NA,NA,1,-1,1,-1,NA,NA,NA,NA,2,2,-2,-2),nrow = 2, byrow = T) -->
<!-- kable(as.data.frame(dat_mis), col.names = 1:dim(dat_mis)[2]) -->
<!-- ``` -->
<!-- ```{r importance sampling} -->
<!-- set.seed(123) -->
<!-- nsamp <- 5000 -->
<!-- SS_obs <- dat_mis[,1:4] %*% t(dat_mis[,1:4]) -->
<!-- dat_imp <- dat_mis -->
<!-- rho <- numeric(nsamp) -->

<!-- # weights -->
<!-- for (i in 1:nsamp) { -->
<!--   S_inv <- rWishart(1, 4, SS_obs)[,,1] -->
<!--   det_S_inv <- S_inv[1,1]*S_inv[2,2] - S_inv[1,2]*S_inv[2,1] -->
<!--   r <- -S_inv[1,2]/sqrt(S_inv[1,1]*S_inv[2,2]) -->
<!--   mu1 <- r * dat_mis[1,5:8] * sqrt(S_inv[2,2]/S_inv[1,1]) -->
<!--   mu2 <- r * dat_mis[2,9:12] * sqrt(S_inv[1,1]/S_inv[2,2]) -->
<!--   s1 <- sqrt((1-r^2) * S_inv[2,2]/det_S_inv) -->
<!--   s2 <- sqrt((1-r^2) * S_inv[1,1]/det_S_inv) -->
<!--   dat_imp[1,9] <- rnorm(1, mean = mu2[1], sd = s2) -->
<!--   dat_imp[1,10] <- rnorm(1, mean = mu2[2], sd = s2) -->
<!--   dat_imp[1,11] <- rnorm(1, mean = mu2[3], sd = s2) -->
<!--   dat_imp[1,12] <- rnorm(1, mean = mu2[4], sd = s2) -->
<!--   dat_imp[2,5] <- rnorm(1, mean = mu1[1], sd = s1) -->
<!--   dat_imp[2,6] <- rnorm(1, mean = mu1[2], sd = s1) -->
<!--   dat_imp[2,7] <- rnorm(1, mean = mu1[3], sd = s1) -->
<!--   dat_imp[2,8] <- rnorm(1, mean = mu1[4], sd = s1) -->
<!--   SS_imp <- dat_imp %*% t(dat_imp) -->
<!--   S_postr <- rWishart(1000, 12, SS_imp) -->
<!--   rho[i] <- mean(apply(S_postr, 3, function(mat){-mat[1,2]/sqrt(mat[1,1]*mat[2,2])})) -->
<!-- } -->

<!-- hist(rho, breaks = seq(-1,1,length.out = 41), freq = F, main = expression(rho)) -->
<!-- rx <- seq(-1,1,length.out = 201) -->
<!-- ry <- (1-rx^2)^(4.5)/(1.25-rx^2)^8 / integrate(function(x){(1-x^2)^(4.5)/(1.25-x^2)^8},-1,1)$value -->
<!-- lines(rx,ry) -->
<!-- ``` -->

<!-- ```{r importance sampling2} -->
<!-- set.seed(123) -->
<!-- nsamp <- 5000 -->
<!-- SS_obs <- dat_mis[,1:4] %*% t(dat_mis[,1:4]) -->
<!-- dat_imp <- dat_mis -->
<!-- rho <- numeric(nsamp) -->

<!-- # weights -->
<!-- for (i in 1:nsamp) { -->
<!--   S_inv <- rWishart(1, 4, SS_obs)[,,1] -->
<!--   det_S_inv <- S_inv[1,1]*S_inv[2,2] - S_inv[1,2]*S_inv[2,1] -->
<!--   r <- -S_inv[1,2]/sqrt(S_inv[1,1]*S_inv[2,2]) -->
<!--   mu1 <- r * dat_mis[1,5:8] * sqrt(S_inv[2,2]/S_inv[1,1]) -->
<!--   mu2 <- r * dat_mis[2,9:12] * sqrt(S_inv[1,1]/S_inv[2,2]) -->
<!--   s1 <- sqrt((1-r^2) * S_inv[2,2]/det_S_inv) -->
<!--   s2 <- sqrt((1-r^2) * S_inv[1,1]/det_S_inv) -->
<!--   dat_imp[1,9] <- rnorm(1, mean = mu2[1], sd = s2) -->
<!--   dat_imp[1,10] <- rnorm(1, mean = mu2[2], sd = s2) -->
<!--   dat_imp[1,11] <- rnorm(1, mean = mu2[3], sd = s2) -->
<!--   dat_imp[1,12] <- rnorm(1, mean = mu2[4], sd = s2) -->
<!--   dat_imp[2,5] <- rnorm(1, mean = mu1[1], sd = s1) -->
<!--   dat_imp[2,6] <- rnorm(1, mean = mu1[2], sd = s1) -->
<!--   dat_imp[2,7] <- rnorm(1, mean = mu1[3], sd = s1) -->
<!--   dat_imp[2,8] <- rnorm(1, mean = mu1[4], sd = s1) -->
<!--   SS_imp <- dat_imp %*% t(dat_imp) -->
<!--   rho[i] <- - SS_imp[1,2]/sqrt(SS_imp[1,1]*SS_imp[2,2]) -->
<!-- } -->

<!-- hist(rho, breaks = seq(-1,1,length.out = 41), freq = F, main = expression(rho)) -->
<!-- rx <- seq(-1,1,length.out = 201) -->
<!-- ry <- (1-rx^2)^(4.5)/(1.25-rx^2)^8 / integrate(function(x){(1-x^2)^(4.5)/(1.25-x^2)^8},-1,1)$value -->
<!-- lines(rx,ry) -->
<!-- ``` -->


<!-- ```{r importance sampling3} -->
<!-- set.seed(123) -->
<!-- nsamp <- 5000 -->
<!-- SS_obs <- dat_mis[,1:4] %*% t(dat_mis[,1:4]) -->
<!-- rho <- numeric(nsamp) -->
<!-- w <- numeric(nsamp) -->
<!-- SS_imp <- matrix(0, nrow = 2, ncol = 2) -->

<!-- # weights -->
<!-- for (i in 1:nsamp) { -->
<!--   S_inv <- rWishart(1, 4, SS_obs)[,,1] -->
<!--   det_S_inv <- S_inv[1,1]*S_inv[2,2] - S_inv[1,2]*S_inv[2,1] -->
<!--   r <- -S_inv[1,2]/sqrt(S_inv[1,1]*S_inv[2,2]) -->
<!--   mu1 <- r * dat_mis[1,5:8] * sqrt(S_inv[2,2]/S_inv[1,1]) -->
<!--   mu2 <- r * dat_mis[2,9:12] * sqrt(S_inv[1,1]/S_inv[2,2]) -->
<!--   s1 <- sqrt((1-r^2) * S_inv[2,2]/det_S_inv) -->
<!--   s2 <- sqrt((1-r^2) * S_inv[1,1]/det_S_inv) -->
<!--   x_1_mis <- rnorm(4, mean = mu2, sd = s2) -->
<!--   x_2_mis <- rnorm(4, mean = mu1, sd = s1) -->
<!--   SS_imp[1,1] <- sum(dat_mis[1,5:8]^2) + sum(x_1_mis^2)  -->
<!--   SS_imp[2,2] <- sum(x_2_mis^2) + sum(dat_mis[2,9:12]^2) -->
<!--   SS_imp[1,2] <- sum(dat_mis[1,5:8] * x_2_mis) + sum(x_1_mis * dat_mis[2,9:12])  -->
<!--   SS_imp[2,1] <- SS_imp[1,2] -->
<!--   SS <- SS_obs + SS_imp  -->
<!--   rho[i] <- - SS[1,2]/sqrt(SS[1,1]*SS[2,2]) -->
<!--   mu1_n <- rho[i] * dat_mis[1,5:8] * sqrt(S_inv[2,2]/S_inv[1,1]) -->
<!--   mu2_n <- rho[i] * dat_mis[2,9:12] * sqrt(S_inv[1,1]/S_inv[2,2]) -->
<!--   s1_n <- sqrt((1-rho[i]^2) * S_inv[2,2]/det_S_inv) -->
<!--   s2_n <- sqrt((1-rho[i]^2) * S_inv[1,1]/det_S_inv) -->
<!--   logw <- sum(dnorm(x_1_mis, mean = mu2_n, sd = s2_n, log = T)) + sum(dnorm(x_2_mis, mean = mu1_n, sd = s1_n, log = T)) - sum(dnorm(x_1_mis, mean = mu2, sd = s2, log = T)) - sum(dnorm(x_2_mis, mean = mu1, sd = s1, log = T)) -->
<!--   w[i] <- exp(logw) -->
<!-- } -->

<!-- var(w)/(mean(w)) -->

<!-- summary(rho) -->
<!-- hist(rho, breaks = seq(-1,1,length.out = 41), freq = F, main = expression(rho)) -->

<!-- rx <- seq(-1,1,length.out = 201) -->
<!-- ry <- (1-rx^2)^(4.5)/(1.25-rx^2)^8 / integrate(function(x){(1-x^2)^(4.5)/(1.25-x^2)^8},-1,1)$value -->
<!-- lines(rx,ry) -->
<!-- ``` -->


- adaptive importance sampling
    + $g_0(\bx) = t_{\alpha}(\bx;\mu_0,\Sigma_0)$ $\longrightarrow$ $g_1(\bx) = t_{\alpha}(\bx;\mu_1,\Sigma_1)$
    + select $\lambda=(\epsilon,\mu,\Sigma)$, $g(\bx; \lambda) = \epsilon g_0(\bx) + (1-\epsilon)t_{\alpha}(\bx;\mu,\Sigma)$ 
- rejection control (RC)
    + accept $\bx^{(j)}$ with probability $r^{(j)} = \min\{1,w^{(j)}/c\}$  
    + if accepted, update $w^{(*j)} = q_cw^{(j)}/r^{(j)}$, $q_c=\int\min\{1,w(\bx)/c\}g(\bx)d\bx$
- sequential importance sampling
    + target density $\pi(\bx) = \pi(x_1)\pi(x_2|x_1)\dots \pi(x_d|x_1,\dots,x_{d-1})$
    + trial density $g(\bx) = g_1(x_1)g_2(x_2|x_1)\dots g_d(x_d|x_1,\dots,x_{d-1})$
    + auxiliary distributions $\pi_t(\bx)$ approximate $\pi(\bx_t)$
    + $w_t(\bx_t)=w_{t-1}(\bx_{t-1})\frac{\pi(x_t|\bx_{t-1})}{g_t(x_t|\bx_{t-1})}$, $w_d(\bx) = w(\bx)$
    + draw $X_t=x_t$ from $g_t(x_t|\bx_{t-1})$, $w_t=w_{t-1}u_t$, $u_t=\frac{\pi_t(\bx_t)}{\pi_{t-1}(bx_{t-1})g_{t}(x_t|bx_{t-1})}$  
    + RC in SIS, RC$(t_k)$ at check point $0<t_1<t_2<\dots<t_k\le d$
    + growth method (inversely restricted sampling, Hammersley and Morton, 1954; biased sampling, Rosenbluth and Rosenbluth, 1955) \
    Self-Avoiding random Walk model $\pi(\bx) = Z_n^{-1}$\
    $\bbP(x_{t+1}|x_1,\dots,x_t) = n_t^{-1}$, $w(\bx) = n_1\times \dots \times n_{N-1}$
    SIS. sampling $g_t(x_t|\bx_{t-1})=n_{t-1}^{-1}$, auxiliary $\pi_t(\bx_t) = Z_t^{-1}$, marginal $\pi_t(\bx_{t-1}) = \sum_{x_t}\pi(\bx_{t-1},x_t) = \frac{n_{t-1}}{Z_t}$, conditional $\pi_t(x_t|\bx_{t-1})= n_{t-1}^{-1}$\
    $w_{t+1}=w_tn_t$, $w_N = \prod_{t\ge 2} \frac{1}{g_t(x_t|x_1,\dots,x_{t-1})}$, $\hat{Z}_N = \bar{w}^{-1}$
    + sequential imputation (Kong, liu and Wong, 1994)\
    draw $\by_{\mbox{mis}}$ from $g(\by_{\mbox{mis}}) = f(y_{\mbox{mis},1}|y_{\mbox{obs},1},\theta)\dots    f(y_{\mbox{mis},n}|\by_{\mbox{obs},n},\by_{\mbox{mis},{n-1}},\theta)$\
    calculate weight $w(\by_{\mbox{mis}}) = f(y_{\mbox{obs},1}|\theta)\dots f(y_{\mbox{obs},n}|\by_{n-1},\theta)$\
    $\bar{w} = \frac{1}{m}\sum_{i=1}^m w(\by_{\mbox{mis}}^{(i)}) \longrightarrow L(\theta|\by_{\mbox{obs}})$
    + nonlinear filtering
    state-space model (hidden Markov model) $\begin{cases}\mbox{(state equation):}& x_t\sim q_t(\cdot|x_{t-1},\theta) \\ \mbox{(observation equation):}& y_t\sim f_t(\cdot|x_{t},\phi) \end{cases}$\
    Gaussian $f_t, q_t$, linear state-space model, Kalman filter\
    bootstrap filter (particle filter) (Gordon, Salmond and Smith, 1993)\
    draw $x_{t+1}^{(*j)}$ from $q_t(x_{t+1}|x_t^{(j)})$, $w^{(j)}\propto f_t(y_{t+1}|x_{t+1}^{(*j)})$, resample $\{x_{t+1}^{(1)},\dots,x_{t+1}^{(m)}\}$ from $\{x_{t+1}^{(*1)},\dots,x_{t+1}^{(*m)}\}$ with probability proportional to $w^{(j)}$
    + probability dynamic system $\pi_t(\bx_t)$, either $\bx_{t+1}=(\bx_t,x_{t+1})$ or $\bx_{t+1}=\bx_t$\
    set $g_t(x_t|\bx_{t-1}) = \pi_t(x_{t}|\bx_{t-1})$, $w_t = \frac{\pi_t(\bx_{t-1})}{\pi_{t-1}(\bx_{t-1})}\frac{\pi_t(x_{t}|\bx_{t-1})}{g_t(x_t|\bx_{t-1})}$\
    unnormalized $q_t(\bx_{t}) =Z_t\pi_t(\bx_{t})$, $w_N = \prod_{t=1}^Nw_t = \frac{Z_N}{Z_1}\frac{\pi_N(\bx_N)}{g_1(x_1)\dots g_N(x_N|\bx_{N-1})}$, $\bbE(w_N) = \frac{Z_N}{Z_1}$\
    Prune-Enriched Rosenbluth Method (PERM) (Grassberger, 1997) $w_t>C_t$ split into $r$ copies with probability $w_t/r$, $w_t\le c_t$ keep or discard with probability 0.5    

# Metropolis algorithm
target distribution $\pi(\bx)=Z^{-1}\exp(-h(\bx))$

proposal function, trial proposal $T(\bx^{(t)},\bx')=T(\bx',\bx^{(t)})$, change $\Delta h = h(\bx') - h(\bx^{(t)})$

acceptance rejection ratio $r = \pi(\bx')/\pi(\bx^{(t)}) = \exp(\Delta h)$


1-D Ising model $U(\bx) = - J \sum_{s=1}^{d-1}x_sx_{s+1}$, $h(\bx)=JU(\bx)/(\beta T) =: -\mu \sum_{s=1}^{d-1}x_sx_{s+1}$ 

$r = \pi(\bx')/\pi(\bx^{(t)}) = \begin{cases}\exp(-2\mu x_j^{(t)}(x_{j-1}^{(t)}+x_{j+1}^{(t)})) & j\ne 1,d\\
\exp(-2\mu x_j^{(t)}x_{j+1}^{(t)}) & j = 1\\
\exp(-2\mu x_j^{(t)}x_{j-1}^{(t)}) & j = d\end{case}$

```{r Ising}
mu <-  1
d <-  50
x <- rep(1,d)
nsamp <- 50000
s <- rep(0,nsamp+1)
s[1] <- sum(x)
for (i in 1:nsamp) {
  j <- sample(1:d)[1]
  if (j == 1) {
    r <- -2 * mu * x[j] * x[j+1]  
  } else if (j == d) {
    r <- -2 * mu * x[j] * x[j-1]
  } else {
    r <- -2 * mu * x[j] * (x[j-1] + x[j+1])
  }
  
  if (log(runif(1)) <= r) {
    x[j] <- -x[j]
  }
  s[i+1] <- sum(x)
}

plot(s[1:2000],type ='l', xlab = 'Iteration')
by10 <- seq(50, nsamp, by = 50)
acf(s[by10])

mu <-  2
s <- rep(0,nsamp+1)
s[1] <- sum(x)
for (i in 1:nsamp) {
  j <- sample(1:d)[1]
  if (j == 1) {
    r <- -2 * mu * x[j] * x[j+1]  
  } else if (j == d) {
    r <- -2 * mu * x[j] * x[j-1]
  } else {
    r <- -2 * mu * x[j] * (x[j-1] + x[j+1])
  }
  
  if (log(runif(1)) <= r) {
    x[j] <- -x[j]
  }
  s[i+1] <- sum(x)
}

plot(s[1:2000],type ='l', xlab = 'Iteration')
by10 <- seq(50, nsamp, by = 50)
acf(s[by10])
```

actual transition function $A(\bx,\by)$, $\int\pi(\bx)A(\bx,\by)d\bx = \pi(\by)$

detailed balance $\pi(\bx)A(\bx,\by) = \pi(\by)A(\by,\bx)$

Metropolis-Hastings $r(\bx,\by)= \min\left\{1, \frac{\pi(\by)T(\by,\bx)}{\pi(\bx)T(\bx,\by)} \right\}$, $A(\bx,\by) = \pi(\by)\min\left\{\frac{T(\bx,\by)}{\pi(\by)}, \frac{T(\by,\bx)}{\pi(\bx)} \right\}$,
$\pi(\bx)A(\bx,\by) = \min\left\{\pi(\bx)T(\bx,\by), \pi(\by)T(\by,\bx) \right\}$

Baker (1965) $r(\bx,\by)=  \frac{\pi(\by)T(\by,\bx)}{\pi(\by)T(\by,\bx)+\pi(\bx)T(\bx,\by)}$,
$A(\bx,\by) = \pi(\by)\frac{T(\bx,\by)T(\by,\bx)}{\pi(\by)T(\by,\bx)+\pi(\bx)T(\bx,\by)}$,
$\pi(\bx)A(\bx,\by) = \pi(\bx)\pi(\by)\frac{T(\bx,\by)T(\by,\bx)}{\pi(\by)T(\by,\bx)+\pi(\bx)T(\bx,\by)}$

Stein $r(\bx,\by)=  \frac{\delta(\bx,\by)}{\pi(\bx)T(\bx,\by)}$, symmetric $\delta(\bx,\by)$, $A(\bx,\by) = \pi(\by)\delta(\bx,\by)$,
$\pi(\bx)A(\bx,\by) = \pi(\bx)\pi(\by)\delta(\bx,\by)$

Random-walk Metropolis $\bx' =\bx^{(t)} + \epsilon_t$, $\epsilon_t\in g_{\sigma}(\cdot)$ a spherically symmetric distribution

Metropolized independence sampler $\by \sim g(\by)$, $r=\min\left\{1,\frac{w(\by)}{w(\bx^{(t)})}\right\}$, $w(\bx) = \pi(\bx)/g(\bx)$ 

Configurational bias Monte Carlo (CBMC)

- auxiliary distribution $\pi_1(x_1),\pi_2(x_1,x_2),\dots,\pi_{d-1}(\bx_{d-1}),\pi(\bx)$
- trial sampling distribution $g(\bx)=g_1(x_1)g_2(x_2|x_1)\dots g_d(x_d|\bx_{d-1})$
- importance weight $w(\by) = \frac{\pi(\by)}{g(\by)} = \frac{\pi_1(y_1)}{g_1(y_1)}\frac{\pi_2(y_1,y_2)}{g_2(y_2|y_1)\pi_1(y_1)}\dots\frac{\pi_d(y_1,\dots,y_d}{g_d(y_d|\by_{d-1})\pi_{d-1}(\by_{d-1})}


multiple-try Metropolis (MTM) 

- draw $\by_1\dots,\by_k\sim T(\bx,\cdot)$ and compute $w(\bx,\by)=\pi(\bx)T(\bx,\by)\lambda(\bx,\by)$, ssymmetric $\lambda(\bx,\by)>0$ 
- draw $\by$ from $\{\by_1\dots,\by_k\}$ with probability $w(\by_j,\bx)$ and draw reference set  $\bx_1^*\dots,x_k^*\sim T(\by,\cdot)$, $\bx_k^*=\bx$
- accept $\by$ with generalized M-H ratio $r_g = \min\left\{1, \frac{w(\by_1,\bx)+\cdots+w(\by_k,\bx)}{w(\bx_1^*,\by)+\cdots+w(\bx_k^*,\by)}\right\}$

orientational bias Monte Carlo (OBMC) $\lambda(\bx,\by)=T^{-1}(\bx,\by)$

multiple-trial Metropolis independence sampler (MTMIS)

- draw $\by_j\sim p(\by)$ and compute $w(\by_j)=\pi(\by_j)/p(\by_j)$, $W=\sum w(\by_j)$
- draw $\by$ from $\{\by_1\dots,\by_k\}$ with probability $w(\by_j)$
- $\bx^{(t+1)}=\by$ with probability $\min\left\{1, \frac{W}{W-w(\by)+w(\bx)}\right\}$ and $\bx^{(t+1)}=\bx$ otherwise

multipoint method 

- draw $\by_j\sim P_j(\cdot|\bx,\by_1,\dots,\by_{j-1})=P_1(\by_1|\bx)$ and compute $w(\bx,\by_{[1:j]})=\pi(\bx)P_j(y_{[1:j]}|\bx)\lambda_j(\bx,y_{[1:j]})$, $\lambda_j(a,b,\dots,z)=\lambda_j(z,\dots,b,a)$ sequentially symmetric
- draw $\by$ from $\{\by_1\dots,\by_k\}$ with probability $w(\by_{[t:1]},\bx)$
- draw reference set $\bx_m^*\sim P_m(\cdot|\by,\bx_{[1:m-1]}^*)$, $m=j+1,\dots,k$, $\bx_l^*=\by_{j-l}$, $l=1,2,\dots,j-1$
- $\bx^{(t+1)}=\by$ with probability $r_{mp}=\min\left\{1, \frac{\sum_{l=1}^k w(\by_{[l:1]},\bx)}{\sum_{l=1}^kw(\bx_{[l:1]}^*,\by)}\right\}$ and $\bx^{(t+1)}=\bx$ otherwise

random-grid method

- generate direction $e$ and a grid size $r$
- candidate set $\by_l = \bx + l \cdot r\cdot e$
- draw $\by$ from $\{\by_1\dots,\by_k\}$ with probability $u_j\pi(\by_{j})$
- reference set $\bx_l^* = \by - l \cdot r\cdot e$
- accept $\by$ with probability $\min\left\{1, \frac{\sum_{l=1}^k \pi(\by_l)}{\sum_{l=1}^k\pi(\bx_l^*)}\right\}$ 

MCMC estimation of $\bbE_{\pi}h(\bx)$, $m\Var\left(\sum h(\bx^{(l)}\right)=\sigma^2\left[1+ 2 \sum_{j=1}^{m-1}\left(1-\frac{j}{m}\right)\rho_j\right] \approx \sigma^2\left[1+ 2 \sum_{j=1}^{m-1}\rho_j\right]$, $\sigma^2=\Var(h(\bx))$, $\rho_j=\mbox{Corr}(h(\bx^{(1)}),h(\bx^{(j+1)}))$

integrated autocorrelation time $\tau_{\mbox{int}}(h) = \frac{1}{2} + \sum_{j=1}^{\infty}\rho_j$, effective sample size $m/[2\tau_{\mbox{int}}(h)]$

exponential autocorrelation time $\tau_{\mbox{exp}}(h) = \lim\sup_{j\to\infty}\frac{j}{-\log|\rho_j|} \approx \tau_{\mbox{int}}(h)$

relaxation time $\tau_{\mbox{exp}} =\sup_{h\in L^2(\pi)}\tau_{\mbox{exp}}(h)$

$\rho_j(h)=\lambda^j$, $\tau_{\mbox{int}}(h) = \frac{1+\lambda}{2(1-\lambda)}$, $\tau_{\mbox{exp}}(h) = -\frac{1}{\log(|\lambda|)}$, $\tau_{\mbox{exp}} = -\frac{1}{\log(|\lambda_2(T)|)}$, 

Gibbs sampler

random-scan / systematic-scan

slice sampler $S = \{\by\in\bbR^{d+1}:y_{d+1}\le\pi(y_1,\dots,y_d) \}$

- draw $y^{t+1}\sim U(0,\pi(x^{(t)}))$ 
- draw $\bx^{t+1}\sim U(S)$ 

Metropolized Gibbs sampler $\min\left\{1, \frac{1-\pi(x_i|\bx_{[-i]})}{1-\pi(y_i|\bx_{[-i]})}\right\}$ 

random-ray Monte Carlo (hit-and-run algorithm)

- draw $\by_i\sim T_e(\bx^*,\cdot)$ along direction $e$, $\by_i=\bx+r_je$, $r_j\sim U(-\sigma,\sigma)$ 
- MTM draw $\by^*$ from $\{\by_1\dots,\by_k\}$ with probability $\pi(\by_j)$, draw reference $\bx'_1,\dots,\bx'_{k-1}\sim T_e(\by^*,\cdot)$, $\bx^*=\bx'_k$,
$r = =\min\left\{1, \frac{\sum_{l=1}^k \pi(\by_j)T_e(\by_j,\bx^*)}{\sum_{l=1}^k\pi(\bx'_j)T_e(\bx'_j,\by^*)}\right\}$

---
title: Bayesian Model
output: html_document
---

```{R setup, include = FALSE}
knitr::opts_chunk$set(comment = NA, prompt = TRUE)
```

# conjugate
$$p(\theta|y) \in \mathcal{P} \mbox{ for all } p(\cdot|\theta) \in \mathcal{F} \mbox{ and } p(\cdot)\in \mathcal{P}$$
exponential family sampling distribution $\mathcal{F}$
$$p(y_l|\theta) = f(y_l) g(\theta) e^{\phi(\theta)^Tu(y_l)} \qquad p(y|\theta) \propto [g(\theta)]^n e^{\phi(\theta)^Tt(y)},\mbox{ where } t(y) = \sum\limits_{l=1}^n u(y_l)$$
conjugacy 
$$p(\theta) \propto [g(\theta)]^{\eta}  e^{\phi(\theta)^T\nu} \qquad p(\theta|y) \propto [g(\theta)]^{\eta+n}  e^{\phi(\theta)^T[\nu+t(y)]}$$
$$\begin{aligned}
y_l|\theta &\sim \mathrm{Binomial}(n,\theta)\\
\theta&\sim\mathrm{Beta}(\alpha,\beta)\\
\theta|y&\sim\mathrm{Beta}(\alpha+y,\beta+n-y)
\end{aligned}$$

$$\begin{aligned}
y_l|\theta &\sim \mathrm{Poisson}(\theta)\\
\theta&\sim\mathrm{Gamma}(\alpha,\beta)\\
\theta|y&\sim\mathrm{Gamma}(\alpha+n\bar{y},\beta+n)\\
y&\sim\mathrm{NegBin}(\alpha,\beta)\\
\mathrm{NegBin}(y|\alpha,\beta)&=\int \mathrm{Poisson}(y|\theta)\mathrm{Gamma}(\theta|\alpha,\beta)d\theta
\end{aligned}$$

exposure $x_i$
$$\begin{aligned}
y_l|\theta &\sim \mathrm{Poisson}(x_l\theta)\\
\theta&\sim\mathrm{Gamma}(\alpha,\beta)\\
\theta|y&\sim\mathrm{Gamma}(\alpha+n\bar{y},\beta+n\bar{x})
\end{aligned}$$

$$\begin{aligned}
y_l|\theta &\sim \mathrm{Exp}(\theta)\\
\theta&\sim\mathrm{Gamma}(\alpha,\beta)\\
\theta|y&\sim\mathrm{Gamma}(\alpha+1,\beta+n\bar{y})
\end{aligned}$$

# noninformative
Jeffreys' invariance principle $p(\theta)\propto [J(\theta)]^{1/2} = [J(\phi)]^{1/2} \left|\frac{d\phi}{d\theta}\right|$

pivotal quantities 

- location $p(y-\theta|\theta) = f(u)$, $u=y-\theta$, $f(u)=p(y-\theta|y)\propto p(\theta)p(y-\theta|\theta)=p(\theta)f(u) \Longrightarrow p(\theta) \propto$ constant 

- scale $p\left(\frac{y}{\theta}\Big|\theta\right) = g(u)$, $u=\frac{y}{\theta}$, $\frac{1}{\theta}g(u)=p\left(\frac{y}{\theta}\Big|y\right)\propto p(\theta)p\left(\frac{y}{\theta}\Big|\theta\right)=p(\theta)\frac{y}{\theta^2}g(u) \Longrightarrow p(\theta) \propto \frac{1}{\theta}$ 

# asymptotics at posterior mode

$$\begin{aligned}
y|\mu,\sigma^2 &\sim \mathrm{Normal}(\mu,\sigma^2) \\
p(\mu,\log(\sigma))&\propto \mbox{ constant}\\
(\mu,\log(\sigma))&\overset{\cdot}{\sim}\mathrm{normal}\left(\begin{bmatrix}\bar{y}\\\log(s\sqrt{(n-1)/n})\end{bmatrix}, \begin{bmatrix}(n-1)s^2/n^2 & 0\\0& 1/(2n)\end{bmatrix} \right)
\end{aligned}
$$
counterexamples

- underidentified models and nonidentified parameters

- number of parameters incresasing with sample size

- aliasing

- unbounded likelihoods

- improper posterior distributions

- prior distributions that excludes the point of convergence

- convergence to the edge of parameter space

- tails of the distribution


# hierarchical model

$$\begin{aligned}
y_{ij}|\theta_j &\sim \mathrm{normal}(\theta_j,\sigma^2) \quad i=1,2,\dots,n_j;j=1,2,\dots,J\\
\bar{y}_{.j}|\theta_j &\sim \mathrm{normal}(\theta_j,\sigma_j^2) \quad  \bar{y}_{.j} = \frac{1}{n_j}\sum_{i=1}^{n_j}y_{ij}, \sigma_j^2=\sigma^2/n_j, \bar{y}_{.j} = \frac{\sum_{j=1}^{J}\bar{y}_{.j}/\sigma_j^2}{\sum_{j=1}^{J}1/\sigma_j^2}\\
p(\theta|\mu,\tau) &= \prod_{j=1}^J \mathrm{dnorm}(\theta_j|\mu,\tau^2)\\
p(\mu,\tau) &\propto p(\tau)\\
p(\theta,\mu,\tau|y) &\propto p(\mu,\tau)p(\theta|\mu,\tau)p(y|\theta)\\
&\propto p(\mu,\tau)\prod_{j=1}^J \mathrm{dnorm}(\theta_j|\mu,\tau^2)\prod_{j=1}^J \mathrm{dnorm}(\bar{y}_{.j}|\theta_j,\sigma_j^2)\\
\theta_j|\mu,\tau,y &\sim \mathrm{normal}(\hat{\theta}_j,V_j) \quad \hat{\theta}_j = \frac{\bar{y}_{.j}/\sigma_j^2+\mu/\tau^2}{1/\sigma_j^2+1/\tau^2}, V_j=\frac{1}{1/\sigma_j^2+1/\tau^2}\\
\bar{y}_{.j}|\mu,\tau &\sim \mathrm{normal}(\mu,\sigma_j^2+\tau^2)\\
p(\mu,\tau|y) &\propto p(\mu,\tau)\prod_{j=1}^J \mathrm{dnorm}(\bar{y}_{.j}|\mu,\sigma_j^2+\tau^2)\\
\mu|\tau,y &\sim \mathrm{normal}(\hat{\mu}_j,V_{\mu}) \quad \hat{\mu} = \frac{\sum_{j=1}^J\bar{y}_{.j}/(\sigma_j^2+\tau^2)}{\sum_{j=1}^J1/(\sigma_j^2+\tau^2)}, V_{\mu}^{-1}=\frac{1}{\sum_{j=1}^J1/(\sigma_j^2+\tau^2)}\\
p(\tau|y) &= \frac{p(\mu,\tau|y)}{p(\mu|\tau,y)} \\
&\propto \frac{p(\tau)\prod_{j=1}^J \mathrm{dnorm}(\bar{y}_{.j}|\mu,\sigma_j^2+\tau^2)}{p(\mu|\mu,V_{\mu})} \\
&\propto \frac{p(\tau)\prod_{j=1}^J \mathrm{dnorm}(\bar{y}_{.j}|\hat{\mu},\sigma_j^2+\tau^2)}{p(\hat{\mu}|\hat{\mu},V_{\mu})} \\
&\propto p(\tau)V_{\mu}^{1/2}\prod_{j=1}^J (\sigma_j^2+\tau^2)^{-1/2}\exp\left(-\frac{(\bar{y}_{.j}-\hat{\mu})^2}{2(\sigma_j^2+\tau^2)}\right) \\
\end{aligned}$$
		
# mixture (identifiability issue)
$$X\sim \mathrm{normal}(\theta_1+\theta_2,1),\quad \theta_1\sim \mathrm{normal}(\mu_1,\tau_1^2)\ \perp  \theta_2\sim \mathrm{normal}(\mu_2,\tau_2^2)$$
$$\begin{aligned}
\theta_1|\theta_2,x & \sim  \mathrm{normal}\left(\frac{\mu_1/\tau_1^2 + x-\theta_2}{1/\tau_1^2+1},\frac{1}{1/\tau_1^2+1}\right) \\
\theta_2|\theta_1,x & \sim  \mathrm{normal}\left(\frac{\mu_2/\tau_2^2 + x-\theta_1}{1/\tau_2^2+1},\frac{1}{1/\tau_2^2+1}\right)
\end{aligned}$$

$$\begin{aligned}
p(\theta_1|x) &\propto \int p(x|\theta_1,\theta_2) p(\theta_1)p(\theta_2) d\theta_2\\
&\propto \exp\Bigg(-\frac{\theta_1^2 - 2\mu_1\theta_1}{2\tau_1^2}\Bigg)
\exp\Bigg(-\frac{\theta_1^2-2x\theta_1}{2}\Bigg) \int \exp\Bigg(-\frac{\theta_2^2-2(x-\theta_1)\theta_2}{2}\Bigg)
\exp\Bigg(-\frac{\theta_2^2 - 2\mu_2\theta_2}{2\tau_2^2}\Bigg)d\theta_2\\
&\propto \exp\Bigg(-\frac{\theta_1^2 - 2\mu_1\theta_1}{2\tau_1^2}\Bigg)
\exp\Bigg(-\frac{\theta_1^2-2x\theta_1}{2}\Bigg) \int \exp\Bigg(-\frac{1}{2}(1+1/\tau_2^2)\theta_2^2 + (x-\theta_1+\mu_2/\tau_2^2)\theta_2\Bigg)d\theta_2\\
&\propto \exp\Bigg(-\frac{\theta_1^2 - 2\mu_1\theta_1}{2\tau_1^2}\Bigg)
\exp\Bigg(-\frac{\theta_1^2-2x\theta_1}{2}\Bigg) \exp\Bigg(\frac{1}{2(1+1/\tau_2^2)}(x-\theta_1+\mu_2/\tau_2^2)^2\Bigg)\\
&\propto \exp\Bigg(-\frac{1}{2}[1+1/\tau_1^2-1/(1+1/\tau_2^2)]\theta_1^2 + [x + \mu_1/\tau_1^2 - (x+\mu_2/\tau_2^2)/(1+1/\tau_2^2)]\theta_1\Bigg)\\
\theta_1|x&\sim \mathrm{normal}\left(\frac{x + \mu_1/\tau_1^2 - (x+\mu_2/\tau_2^2)/(1+1/\tau_2^2)}{1+1/\tau_1^2-1/(1+1/\tau_2^2)},\frac{1}{1+1/\tau_1^2-1/(1+1/\tau_2^2)}\right)\\
&\sim \mathrm{normal}\left(\frac{\tau_1^2(x - \mu_2) + (1+\tau_2^2)\mu_1}{1+\tau_1^2+\tau_2^2},\frac{\tau_1^2(1+\tau_2^2)}{1+\tau_1^2+\tau_2^2}\right)\\
&\sim \mathrm{normal}\left(\frac{\mu_1/\tau_1^2 + (x - \mu_2)/(1+\tau_2^2)}{1/\tau_1^2+1/(1+\tau_2^2)},\frac{1}{1/\tau_1^2+1/(1+\tau_2^2)}\right)\\
\theta_2|x&\sim \mathrm{normal}\left(\frac{\mu_2/\tau_2^2 + (x - \mu_1)/(1+\tau_1^2)}{1/\tau_2^2+1/(1+\tau_1^2)},\frac{1}{1/\tau_2^2+1/(1+\tau_1^2)}\right)
\end{aligned}$$

```{r}
a1 <- 50; a2 <- 50
b1 <- 10; b2 <- 20
y <- 0; S <- 1000
mtheta1  <-  matrix(nrow = S, ncol = 5)
mtheta2  <-  matrix(nrow = S, ncol = 5)
mtheta1[1,1] <- 45

set.seed(1)

for (r in 1:5) {
  for(i in 1:S){
    if (i == 1) {
      theta1 <- 45
    } else {
      theta1 <- mtheta1[i-1,r]
    }
    m2 <- (b2^2*(y-theta1)+a2)/(b2^2+1)
    sigma2 <- sqrt(b2^2/(b2^2+1))
    mtheta2[i,r] <- rnorm(1,m2,sigma2)
    
    theta2 <- mtheta2[i,r]
    m1 <- (b1^2*(y-theta2)+a1)/(b1^2+1)
    sigma1 <- sqrt(b1^2/(b1^2+1))
    mtheta1[i,r] <- rnorm(1,m1,sigma1)
  }
}


mmu <- mtheta1 + mtheta2


par(mfrow = c(2,2))
matplot(mtheta1, type = "l", xlab = "Iterations", main = expression(theta[1]))
matplot(mtheta2, type = "l", xlab = "Iterations", main = expression(theta[2]))
matplot(mmu, type = "l", xlab = "Iterations", main = expression(mu))
colMeans(mmu)
```

# measures of predictive accuracy

$$\begin{aligned}
\mbox{elpd} &= \mbox{expected log predictive density for a new data point}\\
&= \mathrm{E}_f(\log p_{\mathrm{post}}(\tilde{y}_i)) = \int (\log p_{\mathrm{post}}(\tilde{y}_i))f(\tilde{y}_i)d\tilde{y}_i\\
\mbox{elppd} &= \mbox{expected log pointwise predictive density for a new dataset}\\
&= \sum_{i=1}^n \mathrm{E}_f(\log p_{\mathrm{post}}(\tilde{y}_i)) \\
\mbox{lppd} &= \mbox{log pointwise predictive density}\\
&= \log\prod_{i=1}^n p_{\mathrm{post}}(y_i) = \sum_{i=1}^n \log \int p(y_i|\theta)p_{\mathrm{post}}(\theta)d\theta \\
\mbox{computed lppd} &= \mbox{computed log pointwise predictive density}\\
&= \sum_{i=1}^n \log \left( \frac{1}{S} \sum_{s=1}^S  p(y_i|\theta^s) \right)
\end{aligned}
$$

Akaike information criterion (AIC)
$$\begin{aligned}
\widehat{\mathrm{elpd}}_{\mathrm{AIC}} &= \log p(y|\hat{\theta}_{\mathrm{mle}}) - k \\
\mathrm{AIC} &= -2\log p(y|\hat{\theta}_{\mathrm{mle}}) + 2k 
\end{aligned}$$

Deviance information criterion (DIC)
$$\begin{aligned}
\widehat{\mathrm{elpd}}_{\mathrm{DIC}} &= \log p(y|\hat{\theta}_{\mathrm{Bayes}}) - p_{\mathrm{DIC}} \\
p_{\mathrm{DIC}} &= 2 \Bigg(\log p(y|\hat{\theta}_{\mathrm{Bayes}}) - \mathrm{E}_{\mathrm{post}}(\log p(y|\theta))\Bigg) \\
\mbox{computed } p_{\mathrm{DIC}} &= 2 \Bigg(\log p(y|\hat{\theta}_{\mathrm{Bayes}}) - \frac{1}{S} \sum_{s=1}^S \log p(y|\theta^s) \Bigg) \\
p_{\mathrm{DIC}_\mathrm{alt}} &= 2 \mathrm{var}_{\mathrm{post}}(\log p(y|\theta))) \\
\mathrm{DIC} &= -2\log p(y|\hat{\theta}_{\mathrm{Bayes}}) + 2p_{\mathrm{DIC}} 
\end{aligned}$$

Watanabe-Akaike or widely applicable information criterion (WAIC)
$$\begin{aligned}
p_{\mathrm{WAIC}_1} &= 2 \sum_{i=1}^n \Bigg(\log (\mathrm{E}_{\mathrm{post}} p(y_i|\theta)) - \mathrm{E}_{\mathrm{post}}(\log p(y_i|\theta)) \Bigg) \\
\mbox{computed } p_{\mathrm{WAIC}_1} &= 2 \sum_{i=1}^n  \Bigg(\log \Bigg(\frac{1}{S} \sum_{s=1}^S \log p(y_i|\theta^s) \Bigg) - \frac{1}{S} \sum_{s=1}^S \log p(y_i|\theta^s) \Bigg) \\
p_{\mathrm{WAIC}_2} &= \sum_{i=1}^n \mathrm{var}_{\mathrm{post}}(\log p(y_i|\theta)) \\
\mbox{computed } p_{\mathrm{WAIC}_2} &= \sum_{i=1}^n \widehat{\mathrm{var}}_{\mathrm{post}}(\log p(y_i|\theta^s)) \\
\widehat{\mathrm{elppd}}_{\mathrm{WAIC}} &= \mathrm{lppd} - p_{\mathrm{WAIC}} \\
\mathrm{WAIC} &= -2\mathrm{lppd} + 2p_{\mathrm{WAIC}_2} 
\end{aligned}$$

`Bayesian' information criterion (BIC)
$$\mathrm{BIC} = -2 \log p(y|\hat{\theta}) + k\log n$$

Leave-one-out cross-validation
$$\begin{aligned}
\mbox{lppd}_{\mbox{loo-cv}} &= \sum_{i=1}^n \log p_{\mbox{post}(-i)}(y_i) \quad\mbox{Bayesian LOO-CV estimate of out of sample predictive fit}\\
b &= \mbox{lppd} - \overline{\mbox{lppd}}_{-i} \quad\mbox{first order bias correction}\\
\mbox{lppd}_{\mbox{cloo-cv}} &= \mbox{lppd}_{\mbox{loo-cv}} + b \quad\mbox{bias-corrected Bayesian LOO-CV}\\
p_{\mbox{loo-cv}} &= \mbox{lppd} - \mbox{lppd}_{\mbox{loo-cv}} \quad\mbox{effective number of parameters}\\
p_{\mbox{cloo-cv}} &= \mbox{lppd} - \mbox{lppd}_{\mbox{cloo-cv}} = \overline{\mbox{lppd}}_{-i} - \mbox{lppd}_{\mbox{loo-cv}} 
\end{aligned}$$

# missing data
sampling model and missing-data model
$$p(y_{\mbox{obs}},I|x,\theta,\phi) = \int p(y,I|x,\theta,\phi)dy_{\mbox{mis}}$$

posterior
$$p(\theta,\phi|x,y_{\mbox{obs}},I) \propto p(\theta,\phi|x)\int p(y|x,\theta)p(I|x,\theta,\phi)dy_{\mbox{mis}}$$

- superpopulation inference $p(\theta,\phi|x,y_{\mbox{obs}},I)$
- finite-population inference $p(y_{\mbox{mis}}|x,y_{\mbox{obs}},I,\theta,\phi)$
- multiple imputations $p(y_{\mbox{mis}_i}|x,y_{\mbox{obs}},I,\hat{\theta}_i,\hat{\phi}_i)$

Ignorability
$$p(\theta|x,y_{\mbox{obs}},I) = p(\theta|x,y_{\mbox{obs}}) \propto p(\theta|x)\int p(y|x,\theta)dy_{\mbox{mis}}$$
- missing at random $p(I|x,y,\phi) = p(I|x,y_{\mbox{obs}},\phi)$ free of $y_{\mbox{mis}}$
- distinct parameters $p(\phi|x,\theta) = p(\phi|x)$

ignorable and known designs with no covariates - simple random sampling and completely randomized experiments $p(I|x,y,\phi) = p(I)$

ignorable and known designs given covariates - stratified sampling and randomized
block experiments.

strongly ignorable and known designs $p(I|x,y,\phi) = p(I|x)$

ignorable and unknown designs - experiments with nonrandom treatment assignments based on fully observed covariates

nonignorable and known designs - censoring at an known point

nonignorable and unknown designs - censoring at an unknown point

propensity score  $P(I_i|X)=\pi_i$



# multiple imputation
parameter estimate $\hat{\theta}_k$ and associated variance estimate $\widehat{W}_k$ from $K$ imputed data sets
$$\begin{aligned}
&\mbox{combined estimate } \bar{\theta}_K = \frac{1}{K}\sum_{k=1}^K \hat{\theta}_k \\
&\mbox{total variance associated with } T_K = W_K + \frac{K+1}{K} B_K\\
&\mbox{within-imputation variance component } W_K = \frac{1}{K}\sum_{k=1}^K \widehat{W}_k \\
&\mbox{between-imputation variance component } B_K = \frac{1}{K-1} \sum_{k=1}^K (\hat{\theta}_k - \bar{\theta}_K)^2 \\
&t\mbox{ distribution approximation with d.f. }=(K-1)\left( 1+ \frac{K}{K+1} \frac{W_K}{B_K} \right)^2 
\end{aligned}$$


# linear regression
semiconjugate prior distribution
$$
\begin{aligned}
y|X,\beta,\sigma^2 &\sim \mbox{normal}(X\beta,\sigma^2) \propto \exp(-\mbox{SSR}(\beta)/(2\sigma^2))  \\
\beta &\sim \mbox{normal}(\beta_0, \Sigma_0)\\
\beta|y,X,\sigma^2 &\sim \mbox{normal}(\mathbf{m}, \mathbf{V})\\
\mathbf{m}=\mbox{E}[\beta|y,X,\sigma^2] &= (\Sigma_0^{-1} + X^TX/\sigma^2)^{-1} (\Sigma_0^{-1}\beta_0 + X^Ty/\sigma^2)\\
\mathbf{V} =\mbox{Var}(\beta|y,X,\sigma^2) &= (\Sigma_0^{-1} + X^TX/\sigma^2)^{-1}
\end{aligned}
$$
$$\begin{aligned}
\gamma = 1/\sigma^2 &\sim \mbox{gamma}(\nu_0/2, \nu_0\sigma_0^2/2)\\
\gamma|y,X,\beta &\sim \mbox{gamma}((\nu_0+n)/2, [\nu_0\sigma_0^2+\mbox{SSR}(\beta)]/2)\\
\sigma^2|y,X,\beta &\sim \mbox{inverse-gamma}((\nu_0+n)/2, [\nu_0\sigma_0^2+\mbox{SSR}(\beta)]/2)
\end{aligned}$$

$$\beta^{(s)}\sim\mbox{normal}(\mathbf{m}^{(s-1)},\mathbf{V}^{(s-1)}) \longrightarrow \mbox{SSR}(\beta^{(s)}) \longrightarrow \sigma^{2(s)}\sim\mbox{inverse-gamma}((\nu_0+n)/2, [\nu_0\sigma_0^2+\mbox{SSR}(\beta^{(s)})]/2) \longrightarrow (\mathbf{m}^{(s)},\mathbf{V}^{(s)})$$
- unit information prior (Kass and Wasserman, 1995) $\Sigma_0^{-1} = (X^TX)/(n\sigma^2)$, $\beta_0=\hat{\beta}_{\mbox{ols}}$

- $g$-prior (Zellner, 1986) $\Sigma_0 = k(X^TX)^{-1} = (g\sigma^2)(X^TX)^{-1}$, $\beta_0=0$
$$
\begin{aligned}
\mathbf{m} &= (X^TX/(g\sigma^2) + X^TX/\sigma^2)^{-1} (0  + X^Ty/\sigma^2)  = \frac{g}{g+1}\sigma^2(X^TX)^{-1}X^Ty\\
\mathbf{V}  &= (X^TX/(g\sigma^2) + X^TX/\sigma^2)^{-1} = \frac{g}{g+1}\sigma^2(X^TX)^{-1}\\
\sigma^2|y,X &\sim \mbox{inverse-gamma}((\nu_0+n)/2, [\nu_0\sigma_0^2+\mbox{SSR}_g]/2)\\
\mbox{SSR}_g &= y^Ty - \mathbf{m}^T\mathbf{V}^{-1}\mathbf{m} = y^T(\mathbf{I} - \frac{g}{g+1}X(X^TX)^{-1}X^T)y
\end{aligned}
$$


# Gaussian process regression
- a Gaussian process $\mu \sim\mbox{GP}(m,k)$ with a mean function $m$ and covariance function $k$
$$(\mu(x_1),\dots,\mu(x_n)) \sim N_n((m(x_1),\dots,m(x_n)), K(x_1,\dots,x_n))$$
- a common choice of covariance function $k(x,x') = \tau^2 \exp(-|x-x'|^2/(2l^2))$

- observe $(x_i,y_i)$ and predict $\mu(\tilde{x})$ 
$$\begin{aligned}
\mbox{observation model } y_i &\sim N(\mu_i, \sigma^2) \\
\mbox{prior } \mu&\sim\mbox{GP(0,k)}\\
\begin{pmatrix} y \\ \tilde{\mu} \end{pmatrix} &\sim N\left( \begin{pmatrix} 0 \\ 0 \end{pmatrix}, \begin{pmatrix} K(x,x) + \sigma^2I & K(\tilde{x},x) \\K(x,\tilde{x}) & K(\tilde{x},\tilde{x}) \end{pmatrix} \right)\\
\mbox{posterior } \tilde{\mu}|x,y,\tau,l,\sigma &\sim N(\mbox{E}[\tilde{\mu}], \mbox{Cov}(\tilde{\mu}))\\
\mbox{E}[\tilde{\mu}]&= K(\tilde{x},x) (K(x,x) + \sigma^2I)^{-1}y\\
\mbox{Cov}(\tilde{\mu}) &= K(\tilde{x},\tilde{x}) - K(\tilde{x},x) (K(x,x) + \sigma^2I)^{-1}K(x,\tilde{x})
\end{aligned}$$

- a connection to basis expansion $\mu(x) = \sum_{h=1}^H \beta_h b_h(x)$, $\beta=(\beta_1,\dots,\beta_H)\sim N_H(\beta_0,\Sigma_{\beta})$
$$\begin{aligned}
(\mu(x_1),\dots,\mu(x_n)) &\sim N_n((m(x_1),\dots,m(x_n)), K(x_1,\dots,x_n))\\
\mbox{mean function } m(x) &= b(x)^T\beta_0\\
\mbox{covariance function } k(x,x') &= b(x)^T\Sigma_{\beta}b(x')
\end{aligned}$$

# Dirichlet process
- Bayesian histogram for $y_i\overset{i.i.d.}{\sim}f$ with prespecified knots $\xi = (\xi_0,\dots,\xi_K)$, $\xi_0<\xi_1<\dots<\xi_K$, $y_i \in [\xi_0,\xi_K]$
$$\begin{aligned}
f(y) &= \sum_{k=1}^K I(\xi_{k-1}<y\le\xi_k) \frac{\pi_k}{\xi_k-\xi_{k-1}} \\
\mbox{prior } p(\pi|a) &= \frac{\Gamma(\sum_{k=1}^Ka_k)}{\prod_{k=1}^K\Gamma(a_k)}\prod_{k=1}^K \pi_k^{a_k-1}\\
\mbox{posterior } \pi|y &\sim \mbox{Dirichlet}(a_1+n_1,\dots,a_K+n_K),\quad n_k=\sum_iI(\xi_{k-1}<y_i\le \xi_k)
\end{aligned}$$

- a random probability measure $P$ over $(\Omega,\mathcal{B})$ assigned a Dirichlet process $\mbox{DP}(\alpha P_0)$.
    - (indirect representation) For finite $B_1,\dots,B_K$ partitioning  $\Omega$, 
$$(P(B_1),\dots,P(B_K)) \sim \mbox{Dirichlet}(\alpha P_0(B_1), \dots, \alpha P_0(B_K))$$
    - (direct representation) stick-breaking construction 
$$P(\cdot) = \sum_{k=1}^{\infty} \pi_k \delta_{\theta_k}(\cdot),\quad \pi_k=V_k\prod_{l<k}(1-V_l), \quad V_k\sim \mbox{Beta}(1,\alpha),\quad \theta_k\sim P_0$$
$$\begin{aligned}
\mbox{observation model } y_i &\overset{i.i.d.}{\sim} P \\
\mbox{prior } P &\sim \mbox{DP}(\alpha P_0)\\
\mbox{E}[P(B)] &= P_0(B), \mbox{ for all }B\in \mathcal{B}\\ 
\mbox{Var}[P(B)] &= \frac{P_0(B)(1-P_0(B))}{1+\alpha}, \mbox{ for all }B\in \mathcal{B}\\ 
\mbox{posterior } P|y_1,\dots,y_n &\sim \mbox{DP}(\alpha P_0 + \sum_{i=1}^n\delta_{yi})\\
(P(B_1),\dots,P(B_K))|y_1,\dots,y_n &\sim \mbox{Dirichlet}(\alpha P_0(B_1) + \sum_{i=1}^nI(y_i\in B_1), \dots, \alpha P_0(B_K) + \sum_{i=1}^nI(y_i\in B_K)) \\
\mbox{E}[P(B)|y_1,\dots,y_n] &= \frac{\alpha}{\alpha + n}P_0(B) + \frac{n}{\alpha + n}\sum_{i=1}^n\frac{1}{n}\delta_{y_i}(B)\\
\alpha\to 0,\quad P|y_1,\dots,y_n &\sim \mbox{DP}(\sum_{i=1}^n\delta_{y_i})\quad\mbox{Bayesian bootstrap}
\end{aligned}$$

- DP mixture model $f(y) = \sum_{k=1}^{\infty} \pi_k \mathcal{K}(y|\theta_k^*)$ with $\pi\sim\mbox{DP}(\alpha P_0)$
$$\begin{aligned}
y_i &\sim \mathcal{K}(\theta_i)\\
\theta_i &\sim P\\
P &\sim \mbox{DP}(\alpha P_0)\\
\mbox{Polya urn } \theta_i|\theta_{-i} &\sim \frac{\alpha}{\alpha+i-1} P_0(\theta_i) + \sum_{j=1}^{i-1} \frac{1}{\alpha + i-1}\delta_{\theta_j} \\
\mbox{posterior } \theta_i|\theta^{(-i)} &\sim \frac{\alpha}{\alpha+n-1} P_0(\theta_i) + \sum_{k=1}^{K^{(-i)}} \frac{n_k^{(-i)}}{\alpha + n-1}\delta_{\theta_k^{*(-i)}},\quad \theta_k^{*(-i)} \mbox{ are unique values of }\theta^{(-i)}, \quad n_k^{(-i)}=\sum_{j\ne i}I(\theta_j=\theta_k^*)
\end{aligned}$$




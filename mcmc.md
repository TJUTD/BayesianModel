Boltzmann distribution (Gibbs distribution)
$\pi(\mathbf{x}) = \frac{1}{Z}e^{-\frac{U(\mathbf{x})}{kT}}$

-   configuration of a physical system **x**
-   potential energy *U*(**x**)
-   temperature *T*
-   Boltzmann constant *k*
-   partition function *Z*â€„=â€„*Z*(*T*)
-   internal energy âŸ¨*U*âŸ©â€„=â€„ğ”¼<sub>*Ï€*</sub>{*U*(**x**)}
-   set $\beta = \frac{1}{kT}$,
    $\frac{\partial \log(Z)}{\partial \beta}= - \langle U\rangle$
-   free energy *F*â€„=â€„â€…âˆ’â€…*k**T*logâ€†(*Z*)
-   specific heat of the system
    $C = \frac{\partial \langle U\rangle}{\partial T}       =\frac{1}{kT^2}\mathrm{Var}\_{\pi}\\U(\mathbf{x})\\$
-   the systemâ€™s entropy *S*â€„=â€„(âŸ¨*U*âŸ©âˆ’*F*)/*T*

2-*D* Ising model on a *N*â€…Ã—â€…*N* lattice space
â„’â€„=â€„{(*i*,*j*),â€†*i*,â€†*j*â€„=â€„1,â€†â€¦,â€†*N*}

*U*(**x**)â€„=â€„â€…âˆ’â€…*J*âˆ‘<sub>*Ïƒ*â€„âˆ¼â€„*Ïƒ*â€²</sub>*x*<sub>*Ïƒ*</sub>*x*<sub>*Ïƒ*â€²</sub>â€…+â€…âˆ‘<sub>*Ïƒ*</sub>*h*<sub>*Ïƒ*</sub>(*x*<sub>*Ïƒ*</sub>)

-   configuration of the whole system **x**= { *x*<sub>*Ïƒ*</sub>:
    *Ïƒ*â€„âˆˆâ€„â„’ }
-   a particle at site *Ïƒ* has either a positive or a negative spin
    *x*<sub>*Ïƒ*</sub>â€„âˆˆâ€„{â€…+â€…1,â€†â€…âˆ’â€…1}  
-   sites *Ïƒ*,â€†*Ïƒ*â€²â€„âˆˆâ€„â„’ are a neighboring pair *Ïƒ*â€„âˆ¼â€„*Ïƒ*â€²
-   interaction strength *J*
-   external magnetic field *h*<sub>*Ïƒ*</sub>
-   mean magnetization per spin
    $\langle m\rangle = \mathbb{E}\_{\pi}\\\frac{1}{N^2}\left\|\sum\_{\sigma\in S}x\_{\sigma}\right\|\\$

Potts model *x*<sub>*l*</sub>â€„âˆˆâ€„{1,â€†2,â€†â€¦,â€†*q*},
*H*(**x**)â€„=â€„â€…âˆ’â€…*J*âˆ‘<sub>*i*â€„âˆ¼â€„*j*</sub>*Î´*<sub>*x*<sub>*i*</sub>*x*<sub>*j*</sub></sub>â€…âˆ’â€…âˆ‘<sub>*j*</sub>*h*<sub>*j*</sub>(*x*<sub>*j*</sub>)

-   *q*â€„=â€„2,
    $H(\mathbf{x})=-\frac{1}{2}J\sum\_{i\sim j}2\left(\delta\_{x_ix_j}-\frac{1}{2}\right) -\sum_jh_j(x_j)$

simple liquid model
**x**â€„=â€„{*x*<sub>*i*</sub>â€„âˆˆâ€„â„<sup>3</sup>â€„:â€„*i*â€„=â€„1,â€†â€¦,â€†*k*}â€„âˆˆâ€„â„<sup>3*k*</sup>

-   energy
    *U*(**x**)â€„=â€„âˆ‘<sub>*i*,â€†*j*</sub>*Î¦*(\|*x*<sub>*i*</sub>âˆ’*x*<sub>*j*</sub>\|)â€„=â€„âˆ‘<sub>*i*,â€†*j*</sub>*Î¦*(*r*<sub>*i**j*</sub>)
-   Lennard-Jobes pair potential
    $\Phi(r) = 4\epsilon\left\[\left(\frac{\sigma}{r}\right)^{12} -\left(\frac{\sigma}{r}\right)^{6} \right\]$

macromolecules model

-   energy
    $U(\mathbf{x}) = \sum\_{\mbox{bonds}}\\\mbox{bond terms}\\ + \sum\_{i,j}\left\[\Phi(r\_{ij}) + \frac{q_iq_j}{4\pi\epsilon_0r\_{ij}}\right\]$
-   bond terms
    $=\sum\_{\mbox{bonds}} \frac{k_i}{2}(l_i-l\_{i,0})^2 + \sum\_{\mbox{angles}} \frac{k_i}{2}(\theta_i-\theta\_{i,0})^2 + \sum\_{\mbox{torsions}} v(\omega_i)$
-   bond length *l*<sub>*i*</sub>, bond angle *Î¸*<sub>*i*</sub>, torsion
    angle *Ï‰*<sub>*i*</sub>
-   torsion term $v(\omega_i) = \frac{V_n}{2}(1+\cos(n\omega -\gamma))$

### variance reduction methods

stratified sampling

-   estimand âˆ«<sub>ğ’³</sub>*f*(*x*)*d**x*
-   partition *D*<sub>1</sub>,â€†â€¦,â€†*D*<sub>*k*</sub> of ğ’³
-   *Î¼Ì‚*â€„=â€„*Î¼Ì‚*<sub>1</sub>â€…+â€…â€¦â€…+â€…*Î¼Ì‚*<sub>*k*</sub>,
    *Î¼Ì‚*<sub>*i*</sub>â€„=â€„*m*<sub>*i*</sub><sup>âˆ’1</sup>\[*f*(*X*<sub>*i*,â€†1</sub>)+â€¦+*f*(*X*<sub>*i*,â€†*m*<sub>*i*</sub></sub>)\]
-   $\mathrm{Var}(\hat{\mu})=\frac{\sigma_1^2}{m_1} + \dots + \frac{\sigma_k^2}{m_k}$

control variates method

-   estimand *Î¼*â€„=â€„ğ”¼{*X*}
-   control variate *C* with known *Î¼*<sub>*C*</sub>â€„=â€„ğ”¼{*C*}
    -   sample *X*(*b*)â€„=â€„*X*â€…+â€…*b*(*C*âˆ’*Î¼*<sub>*C*</sub>)
    -   Var(*X*(*b*))â€„=â€„Var(*X*)â€…+â€…2*b*Cov(*X*,*C*)â€…+â€…*b*<sup>2</sup>Var(*C*)
    -   set *b*â€„=â€„Cov(*X*,*C*)/Var(*C*),
        Var(*X*(*b*))â€„=â€„(1âˆ’*Ï*<sub>*X**C*</sub><sup>2</sup>)Var(*X*)
-   control variate *C* with unknown ğ”¼{*C*}â€„=â€„*Î¼*
    -   sample *X*(*b*)â€„=â€„*b**X*â€…+â€…(1âˆ’*b*)*C*
    -   Var(*X*(*b*))â€„=â€„*b*<sup>2</sup>Var(*X*)â€…+â€…2*b*(1âˆ’*b*)Cov(*X*,*C*)â€…+â€…(1âˆ’*b*)<sup>2</sup>Var(*C*)
    -   *b*â€„=â€„\[Var(*C*)âˆ’Cov(*X*,*C*)\]/\[Var(*X*)âˆ’2Cov(*X*,*C*)+Var(*C*)\]

antithetic variates method (Hammersley and Morton, 1956)

-   sample pair *X*â€„=â€„*F*<sup>âˆ’1</sup>(*U*),
    *X*â€²â€„=â€„*F*<sup>âˆ’1</sup>(1âˆ’*U*) instead of two independent Monte
    Carlo draws for estimating ğ”¼{*X*}
-   cdf *F* is monotone,
    Cov(*X*,*X*â€²)â€„=â€„ğ”¼{\[*F*<sup>âˆ’1</sup>(*U*)âˆ’*F*<sup>âˆ’1</sup>(*U*â€²)\]\[*F*<sup>âˆ’1</sup>(1âˆ’*U*)âˆ’*F*<sup>âˆ’1</sup>(1âˆ’*U*â€²)\]}â€„â‰¤â€„0

Rao-Blackwellization (Bickel and Doksum, 2000)

-   estimand *I*â€„=â€„ğ”¼{*h*(*X*)}
-   *X*â€„=â€„(*x*<sub>1</sub>,*X*<sub>2</sub>). analytic
    ğ”¼{*h*(*X*)\|*X*<sub>2</sub>}
-   histogram estimator $ = \_{l=1}^m h(X^{(l)})
-   mixture estimator
    $\tilde{I} = \frac{1}{m}\sum\_{l=1}^m \mathbb{E}\\h(X)\|X_2^{(l)}\\$

chain-structure model

-   $\pi(\mathbf{x}) \propto \exp\left( -\sum\_{i=1}^d h_i(x\_{i-1},x_i) \right)$
-   Markovian
    *Ï€*(*x*<sub>*i*</sub>\|**x**<sub>âˆ’*i*</sub>)â€„âˆâ€„expâ€†(âˆ’*h*<sub>*i*</sub>(*x*<sub>*i*â€…âˆ’â€…1</sub>,*x*<sub>*i*</sub>)âˆ’*h*<sub>*i*â€…+â€…1</sub>(*x*<sub>*i*</sub>,*x*<sub>*i*â€…+â€…1</sub>))
-   hidden Markov model (HMM) when
    *x*<sub>*i*</sub>â€„âˆˆâ€„ğ’®â€„=â€„{*s*<sub>1</sub>,â€†â€¦,â€†*s*<sub>*k*</sub>}
-   optimization by dynamic programming *O*(*d**k*<sup>2</sup>)

$$m_1(x)=\min\limits\_{s_i\in\mathcal{S}}h_1(s_i,x),\quad m_t(x)=\min\limits\_{s_i\in\mathcal{S}}\\m\_{t-1}(s_i)+h_t(s_i,x)\\,\quad x=s_1,\dots,s_k$$

$$\hat x_1=\arg\min\limits\_{s_i\in\mathcal{S}} m_d(s_i),\quad \hat x_t=\arg\min\limits\_{s_i\in\mathcal{S}}\\m\_{t}(s_i)+h\_{t+1}(s_i,\hat x\_{t+1})\\,\quad t=d-1,\dots,1
$$

-   exact simulation
    -   partition function
        *Z*â€„=â€„âˆ‘<sub>**x**</sub>expâ€†(âˆ’*H*(**x**))â€„=â€„âˆ‘<sub>*x*â€„âˆˆâ€„ğ’®</sub>*V*<sub>*d*</sub>(*x*)

*V*<sub>1</sub>(*x*)â€„=â€„âˆ‘<sub>*x*<sub>0</sub>â€„âˆˆâ€„ğ’®</sub>*e*<sup>âˆ’*h*<sub>1</sub>(*x*<sub>0</sub>,*x*)</sup>,â€Šâ€*V*<sub>*t*</sub>(*x*)â€„=â€„âˆ‘<sub>*y*â€„âˆˆâ€„ğ’®</sub>*V*<sub>*t*â€…âˆ’â€…1</sub>(*y*)*e*<sup>âˆ’*h*<sub>*t*</sub>(*y*,*x*)</sup>,â€Šâ€*t*â€„=â€„2,â€†â€¦,â€†*d*

$$x_d \sim V_d(x)/Z,\quad x_t \sim \frac{V\_{t}(x)e^{-h\_{t+1}(x,x\_{t+1})}}{\sum\_{y\in\mathcal{S}}V\_{t}(y)e^{-h\_{t+1}(y,x\_{t+1})}},\quad t=d-1,\dots,1$$

    - Ising model $\pi(\bx) = Z^{-1}\exp(\beta(x_0x_1 + \dots + x_{d-1}x_d)), \quad x_i\in \{-1,+1\}$

*V*<sub>1</sub>(*x*)â€„=â€„*e*<sup>*Î²**x*</sup>â€…+â€…*e*<sup>âˆ’*Î²**x*</sup>â€„=â€„*e*<sup>*Î²*</sup>â€…+â€…*e*<sup>âˆ’*Î²*</sup>,â€Šâ€*V*<sub>*t*</sub>(*x*)â€„=â€„(*e*<sup>*Î²*</sup>+*e*<sup>âˆ’*Î²*</sup>)<sup>*t*</sup>,â€Šâ€*Z*â€„=â€„2(*e*<sup>*Î²*</sup>+*e*<sup>âˆ’*Î²*</sup>)<sup>*d*</sup>
 - graphical model, peeling algorithm /
forward-summation-backward-sampling method -
**x**<sub>*C*</sub>â€„=â€„{*x*<sub>*i*</sub>,â€†*i*â€„âˆˆâ€„*C*}, clique
*C*â€„âˆˆâ€„ğ’â€„âŠ‚â€„2<sup>{1,â€†â€¦,â€†*d*}</sup> (Lauritzen and Spiegelhalter, 1998),
connected *C*<sub>*i*</sub>â€…âˆ©â€…*C*<sub>*j*</sub>â€„â‰ â€„âˆ…

## importance sampling

*Î¼*â€„=â€„ğ”¼<sub>*Ï€*</sub>{*h*(**x**)}â€„=â€„ğ”¼<sub>*g*</sub>{*w*(**x**)*h*(**x**)},â€Šâ€*w*â€„=â€„*Ï€*(**x**)/*g*(**x**)

-   normalized weight
    $\hat{\mu}= \frac{w^{(1)}h(\mathbf{x}^{(1)})+\dots+w^{(n)}h(\mathbf{x}^{(n)})}{w^{(1)}+\dots+w^{(n)}}=\frac{\overline{Z}}{\overline{W}}$
    -   know *Ï€*(**x**)/*g*(**x**) up to a multiplicative constant
    -   often a smaller MSE than *Î¼Ìƒ*
-   unbiased estimate
    *Î¼Ìƒ*â€„=â€„*n*<sup>âˆ’1</sup>\[*w*<sup>(1)</sup>*h*(**x**<sup>(1)</sup>)+â€¦+*w*<sup>(*n*)</sup>*h*(**x**<sup>(*n*)</sup>)\]
-   Effective Sample Size
    $=\frac{\mathrm{Var}\_{\pi}(\bar{\mu})}{\mathrm{Var}\_{g}(\hat{\mu})}=\frac{n}{1+\mathrm{Var}\_g(w(\mathbf{x}))}$,
    direct sample
    $\bar{\mu}=n^{-1}\[h(y^{(1)})+\dots+h(y^{(n)})\]=\overline{H}$,
    *y*<sup>(*i*)</sup>â€„âˆ¼â€„*Ï€*

$$\begin{aligned}
  \mathrm{Var}\_g(\hat{\mu}) &=\mathrm{Var}\_g(\overline{Z}/\overline{W}) \approx \begin{bmatrix} \frac{1}{\mathbb{E}\_g\\W\\} & -\frac{\mathbb{E}\_g\\Z\\}{\mathbb{E}\_g^2\\W\\} \end{bmatrix} \frac{1}{n}\begin{bmatrix} \mathrm{Var}\_g(Z) & \mathrm{Cov}\_g(Z,W)\\ \mathrm{Cov}\_g(W,Z)   & \mathrm{Var}\_g(W)  \end{bmatrix} \begin{bmatrix} \frac{1}{\mathbb{E}\_g\\W\\} \\ -\frac{\mathbb{E}\_g\\Z\\}{\mathbb{E}\_g^2\\W\\} \end{bmatrix}\\
    &= \frac{1}{n}\left\[ \frac{\mathrm{Var}\_g(Z)}{\mathbb{E}\_g^2\\W\\} - 2 \frac{\mathbb{E}\_g\\Z\\\mathrm{Cov}\_g(Z,W)}{\mathbb{E}\_g^3\\W\\}  +\frac{\mathbb{E}\_g^2\\Z\\\mathrm{Var}\_g(W)}{\mathbb{E}\_g^4\\W\\}\right\]\\
    &= n^{-1}\[ \mathrm{Var}\_g(Z) - 2 \mu\mathrm{Cov}\_g(Z,W)  + \mu^2\mathrm{Var}\_g(W)\]\\
    &= n^{-1}\[ (\mathbb{E}\_{\pi}\\WH^2\\-\mu) - 2 \mu(\mathbb{E}\_{\pi}\\WH\\ -\mu ) + \mu^2\mathrm{Var}\_g(W)\]\\ 
    \mathbb{E}\_{\pi}\\WH^2\\&\approx \mathbb{E}\_{\pi}\\W\\\mathbb{E}\_{\pi}^2\\H\\ +\frac{1}{2} \mathrm{tr}\left(\begin{bmatrix} 0 & 2\mathbb{E}\_{\pi}\\H\\\\
    2\mathbb{E}\_{\pi}\\H\\ & 2\mathbb{E}\_{\pi}\\W\\\end{bmatrix} \begin{bmatrix} \mathrm{Var}\_{\pi}(W) & \mathrm{Cov}\_{\pi}(W,H)\\ \mathrm{Cov}\_{\pi}(H,W) & \mathrm{Var}\_{\pi}(H)  \end{bmatrix}\right) \\
    &= n^{-1}\[ (\mu^2\mathbb{E}\_{\pi}\\W\\ + 2\mu\mathrm{Cov}\_{\pi}(H,W) + \mathbb{E}\_{\pi}\\W\\\mathrm{Var}\_{\pi}(H)-\mu^2 )- 2 \mu(\mathrm{Cov}\_{\pi}(WH) + \mu\mathbb{E}\_{\pi}\\W\\ -\mu ) + \mu^2\mathrm{Var}\_{g}(W)\]\\
    &= n^{-1}\[ \mathbb{E}\_{\pi}\\W\\\mathrm{Var}\_{\pi}(H) +\mu^2(1-\mathbb{E}\_{\pi}\\W\\+\mathrm{Var}\_{g}(W))\]\\
    &= n^{-1}\[ \mathbb{E}\_{\pi}\\W\\\mathrm{Var}\_{\pi}(H) +\mu^2(1-\mathbb{E}\_{g}\\W^2\\+\mathrm{Var}\_{g}(W))\]\\
    &= n^{-1} \mathbb{E}\_{\pi}\\W\\\mathrm{Var}\_{\pi}(H)\\
    &=\frac{1+\mathrm{Var}\_g(W)}{n}\mathrm{Var}\_{\pi}(H)
\end{aligned}
$$

-   proper w.r.t *Ï€*:
    ğ”¼<sub>*g*</sub>{*h*(**x**<sup>(*i*)</sup>)*w*<sup>(*i*)</sup>}â€„=â€„*c*ğ”¼<sub>*Ï€*</sub>{*h*(**x**)},
    for all square integrable *h*(â‹…) â‡”
    $\frac{\mathbb{E}\_{g}\\w\|\mathbf{x}\\}{\mathbb{E}\_{g}\\w\\}g(\mathbf{x}) = \pi(\mathbf{x})$

<!-- - missing data from a bivariate normal distribution -->
<!-- $$\begin{aligned} -->
<!-- y &\sim \mbox{N}(0,\Sigma),\quad \Sigma = \begin{bmatrix} \sigma_1^2, \rho\sigma_1\sigma_2\\ \rho\sigma_1\sigma_2 & \sigma_2^2\end{bmatrix}\\ -->
<!-- \pi(\Sigma) &\propto |\Sigma|^{-(d+1)/2}, \quad d=2\\ -->
<!-- \pi(\Sigma|y_1,\dots,y_n) &\propto |\Sigma|^{-(n+d+1)/2}\exp(-\mbox{tr}(\Sigma^{-1} S)/2)\sim\mbox{inverse Wishart}(n,S), \quad S = y^Ty\\ -->
<!-- \Sigma|\mathbf{y}_{\mbox{mis}},\mathbf{y}_{\mbox{obs}} &\sim \mbox{inverse Wishart}(n,S(\mathbf{y}_{\mbox{mis}})) \\ -->
<!-- y_{\mbox{mis}}|\Sigma,\mathbf{y}_{\mbox{obs}} &= y_{\mbox{mis}}|\Sigma,y_{\mbox{obs}}\sim \mbox{N}(\mu_*,\sigma_*),\quad \mu_*=\rho y_{\mbox{obs}} \sqrt{\sigma_{\mbox{obs}}/\sigma_{\mbox{mis}}},\sigma_*^2=(1-\rho^2)\sigma_{\mbox{obs}} \\ -->
<!-- \mbox{proposal }g:\ \Sigma &\sim \mbox{inverse Wishart}(n_{\mbox{obs}},S(\mathbf{y}_{\mbox{obs}})) -->
<!-- \end{aligned}$$ -->
<!-- ```{r misingdata, echo = FALSE} -->
<!-- library(knitr) -->
<!-- dat_mis = matrix(c(1,1,-1,-1,2,2,-2,-2,NA,NA,NA,NA,1,-1,1,-1,NA,NA,NA,NA,2,2,-2,-2),nrow = 2, byrow = T) -->
<!-- kable(as.data.frame(dat_mis), col.names = 1:dim(dat_mis)[2]) -->
<!-- ``` -->
<!-- ```{r importance sampling} -->
<!-- set.seed(123) -->
<!-- nsamp <- 5000 -->
<!-- SS_obs <- dat_mis[,1:4] %*% t(dat_mis[,1:4]) -->
<!-- dat_imp <- dat_mis -->
<!-- rho <- numeric(nsamp) -->
<!-- # weights -->
<!-- for (i in 1:nsamp) { -->
<!--   S_inv <- rWishart(1, 4, SS_obs)[,,1] -->
<!--   det_S_inv <- S_inv[1,1]*S_inv[2,2] - S_inv[1,2]*S_inv[2,1] -->
<!--   r <- -S_inv[1,2]/sqrt(S_inv[1,1]*S_inv[2,2]) -->
<!--   mu1 <- r * dat_mis[1,5:8] * sqrt(S_inv[2,2]/S_inv[1,1]) -->
<!--   mu2 <- r * dat_mis[2,9:12] * sqrt(S_inv[1,1]/S_inv[2,2]) -->
<!--   s1 <- sqrt((1-r^2) * S_inv[2,2]/det_S_inv) -->
<!--   s2 <- sqrt((1-r^2) * S_inv[1,1]/det_S_inv) -->
<!--   dat_imp[1,9] <- rnorm(1, mean = mu2[1], sd = s2) -->
<!--   dat_imp[1,10] <- rnorm(1, mean = mu2[2], sd = s2) -->
<!--   dat_imp[1,11] <- rnorm(1, mean = mu2[3], sd = s2) -->
<!--   dat_imp[1,12] <- rnorm(1, mean = mu2[4], sd = s2) -->
<!--   dat_imp[2,5] <- rnorm(1, mean = mu1[1], sd = s1) -->
<!--   dat_imp[2,6] <- rnorm(1, mean = mu1[2], sd = s1) -->
<!--   dat_imp[2,7] <- rnorm(1, mean = mu1[3], sd = s1) -->
<!--   dat_imp[2,8] <- rnorm(1, mean = mu1[4], sd = s1) -->
<!--   SS_imp <- dat_imp %*% t(dat_imp) -->
<!--   S_postr <- rWishart(1000, 12, SS_imp) -->
<!--   rho[i] <- mean(apply(S_postr, 3, function(mat){-mat[1,2]/sqrt(mat[1,1]*mat[2,2])})) -->
<!-- } -->
<!-- hist(rho, breaks = seq(-1,1,length.out = 41), freq = F, main = expression(rho)) -->
<!-- rx <- seq(-1,1,length.out = 201) -->
<!-- ry <- (1-rx^2)^(4.5)/(1.25-rx^2)^8 / integrate(function(x){(1-x^2)^(4.5)/(1.25-x^2)^8},-1,1)$value -->
<!-- lines(rx,ry) -->
<!-- ``` -->
<!-- ```{r importance sampling2} -->
<!-- set.seed(123) -->
<!-- nsamp <- 5000 -->
<!-- SS_obs <- dat_mis[,1:4] %*% t(dat_mis[,1:4]) -->
<!-- dat_imp <- dat_mis -->
<!-- rho <- numeric(nsamp) -->
<!-- # weights -->
<!-- for (i in 1:nsamp) { -->
<!--   S_inv <- rWishart(1, 4, SS_obs)[,,1] -->
<!--   det_S_inv <- S_inv[1,1]*S_inv[2,2] - S_inv[1,2]*S_inv[2,1] -->
<!--   r <- -S_inv[1,2]/sqrt(S_inv[1,1]*S_inv[2,2]) -->
<!--   mu1 <- r * dat_mis[1,5:8] * sqrt(S_inv[2,2]/S_inv[1,1]) -->
<!--   mu2 <- r * dat_mis[2,9:12] * sqrt(S_inv[1,1]/S_inv[2,2]) -->
<!--   s1 <- sqrt((1-r^2) * S_inv[2,2]/det_S_inv) -->
<!--   s2 <- sqrt((1-r^2) * S_inv[1,1]/det_S_inv) -->
<!--   dat_imp[1,9] <- rnorm(1, mean = mu2[1], sd = s2) -->
<!--   dat_imp[1,10] <- rnorm(1, mean = mu2[2], sd = s2) -->
<!--   dat_imp[1,11] <- rnorm(1, mean = mu2[3], sd = s2) -->
<!--   dat_imp[1,12] <- rnorm(1, mean = mu2[4], sd = s2) -->
<!--   dat_imp[2,5] <- rnorm(1, mean = mu1[1], sd = s1) -->
<!--   dat_imp[2,6] <- rnorm(1, mean = mu1[2], sd = s1) -->
<!--   dat_imp[2,7] <- rnorm(1, mean = mu1[3], sd = s1) -->
<!--   dat_imp[2,8] <- rnorm(1, mean = mu1[4], sd = s1) -->
<!--   SS_imp <- dat_imp %*% t(dat_imp) -->
<!--   rho[i] <- - SS_imp[1,2]/sqrt(SS_imp[1,1]*SS_imp[2,2]) -->
<!-- } -->
<!-- hist(rho, breaks = seq(-1,1,length.out = 41), freq = F, main = expression(rho)) -->
<!-- rx <- seq(-1,1,length.out = 201) -->
<!-- ry <- (1-rx^2)^(4.5)/(1.25-rx^2)^8 / integrate(function(x){(1-x^2)^(4.5)/(1.25-x^2)^8},-1,1)$value -->
<!-- lines(rx,ry) -->
<!-- ``` -->
<!-- ```{r importance sampling3} -->
<!-- set.seed(123) -->
<!-- nsamp <- 5000 -->
<!-- SS_obs <- dat_mis[,1:4] %*% t(dat_mis[,1:4]) -->
<!-- rho <- numeric(nsamp) -->
<!-- w <- numeric(nsamp) -->
<!-- SS_imp <- matrix(0, nrow = 2, ncol = 2) -->
<!-- # weights -->
<!-- for (i in 1:nsamp) { -->
<!--   S_inv <- rWishart(1, 4, SS_obs)[,,1] -->
<!--   det_S_inv <- S_inv[1,1]*S_inv[2,2] - S_inv[1,2]*S_inv[2,1] -->
<!--   r <- -S_inv[1,2]/sqrt(S_inv[1,1]*S_inv[2,2]) -->
<!--   mu1 <- r * dat_mis[1,5:8] * sqrt(S_inv[2,2]/S_inv[1,1]) -->
<!--   mu2 <- r * dat_mis[2,9:12] * sqrt(S_inv[1,1]/S_inv[2,2]) -->
<!--   s1 <- sqrt((1-r^2) * S_inv[2,2]/det_S_inv) -->
<!--   s2 <- sqrt((1-r^2) * S_inv[1,1]/det_S_inv) -->
<!--   x_1_mis <- rnorm(4, mean = mu2, sd = s2) -->
<!--   x_2_mis <- rnorm(4, mean = mu1, sd = s1) -->
<!--   SS_imp[1,1] <- sum(dat_mis[1,5:8]^2) + sum(x_1_mis^2)  -->
<!--   SS_imp[2,2] <- sum(x_2_mis^2) + sum(dat_mis[2,9:12]^2) -->
<!--   SS_imp[1,2] <- sum(dat_mis[1,5:8] * x_2_mis) + sum(x_1_mis * dat_mis[2,9:12])  -->
<!--   SS_imp[2,1] <- SS_imp[1,2] -->
<!--   SS <- SS_obs + SS_imp  -->
<!--   rho[i] <- - SS[1,2]/sqrt(SS[1,1]*SS[2,2]) -->
<!--   mu1_n <- rho[i] * dat_mis[1,5:8] * sqrt(S_inv[2,2]/S_inv[1,1]) -->
<!--   mu2_n <- rho[i] * dat_mis[2,9:12] * sqrt(S_inv[1,1]/S_inv[2,2]) -->
<!--   s1_n <- sqrt((1-rho[i]^2) * S_inv[2,2]/det_S_inv) -->
<!--   s2_n <- sqrt((1-rho[i]^2) * S_inv[1,1]/det_S_inv) -->
<!--   logw <- sum(dnorm(x_1_mis, mean = mu2_n, sd = s2_n, log = T)) + sum(dnorm(x_2_mis, mean = mu1_n, sd = s1_n, log = T)) - sum(dnorm(x_1_mis, mean = mu2, sd = s2, log = T)) - sum(dnorm(x_2_mis, mean = mu1, sd = s1, log = T)) -->
<!--   w[i] <- exp(logw) -->
<!-- } -->
<!-- var(w)/(mean(w)) -->
<!-- summary(rho) -->
<!-- hist(rho, breaks = seq(-1,1,length.out = 41), freq = F, main = expression(rho)) -->
<!-- rx <- seq(-1,1,length.out = 201) -->
<!-- ry <- (1-rx^2)^(4.5)/(1.25-rx^2)^8 / integrate(function(x){(1-x^2)^(4.5)/(1.25-x^2)^8},-1,1)$value -->
<!-- lines(rx,ry) -->
<!-- ``` -->

-   adaptive importance sampling
    -   *g*<sub>0</sub>(**x**)â€„=â€„*t*<sub>*Î±*</sub>(**x**;*Î¼*<sub>0</sub>,*Î£*<sub>0</sub>)
        â†’
        *g*<sub>1</sub>(**x**)â€„=â€„*t*<sub>*Î±*</sub>(**x**;*Î¼*<sub>1</sub>,*Î£*<sub>1</sub>)
    -   select *Î»*â€„=â€„(*Ïµ*,*Î¼*,*Î£*),
        *g*(**x**;*Î»*)â€„=â€„*Ïµ**g*<sub>0</sub>(**x**)â€…+â€…(1âˆ’*Ïµ*)*t*<sub>*Î±*</sub>(**x**;*Î¼*,*Î£*)
-   rejection control (RC)
    -   accept **x**<sup>(*j*)</sup> with probability
        *r*<sup>(*j*)</sup>â€„=â€„minâ€†{1,â€†*w*<sup>(*j*)</sup>/*c*}  
    -   if accepted, update
        *w*<sup>(\**j*)</sup>â€„=â€„*q*<sub>*c*</sub>*w*<sup>(*j*)</sup>/*r*<sup>(*j*)</sup>,
        *q*<sub>*c*</sub>â€„=â€„âˆ«minâ€†{1,â€†*w*(**x**)/*c*}*g*(**x**)*d***x**
-   sequential importance sampling
    -   target density
        *Ï€*(**x**)â€„=â€„*Ï€*(*x*<sub>1</sub>)*Ï€*(*x*<sub>2</sub>\|*x*<sub>1</sub>)â€¦*Ï€*(*x*<sub>*d*</sub>\|*x*<sub>1</sub>,â€¦,*x*<sub>*d*â€…âˆ’â€…1</sub>)
    -   trial density
        *g*(**x**)â€„=â€„*g*<sub>1</sub>(*x*<sub>1</sub>)*g*<sub>2</sub>(*x*<sub>2</sub>\|*x*<sub>1</sub>)â€¦*g*<sub>*d*</sub>(*x*<sub>*d*</sub>\|*x*<sub>1</sub>,â€¦,*x*<sub>*d*â€…âˆ’â€…1</sub>)
    -   auxiliary distributions *Ï€*<sub>*t*</sub>(**x**) approximate
        *Ï€*(**x**<sub>*t*</sub>)
    -   $w_t(\mathbf{x}\_t)=w\_{t-1}(\mathbf{x}\_{t-1})\frac{\pi(x_t\|\mathbf{x}\_{t-1})}{g_t(x_t\|\mathbf{x}\_{t-1})}$,
        *w*<sub>*d*</sub>(**x**)â€„=â€„*w*(**x**)
    -   draw *X*<sub>*t*</sub>â€„=â€„*x*<sub>*t*</sub> from
        *g*<sub>*t*</sub>(*x*<sub>*t*</sub>\|**x**<sub>*t*â€…âˆ’â€…1</sub>),
        *w*<sub>*t*</sub>â€„=â€„*w*<sub>*t*â€…âˆ’â€…1</sub>*u*<sub>*t*</sub>,
        $u_t=\frac{\pi_t(\mathbf{x}\_t)}{\pi\_{t-1}(bx\_{t-1})g\_{t}(x_t\|bx\_{t-1})}$  
    -   RC in SIS, RC(*t*<sub>*k*</sub>) at check point
        0â€„\<â€„*t*<sub>1</sub>â€„\<â€„*t*<sub>2</sub>â€„\<â€„â€¦â€„\<â€„*t*<sub>*k*</sub>â€„â‰¤â€„*d*
    -   growth method (inversely restricted sampling, Hammersley and
        Morton, 1954; biased sampling, Rosenbluth and Rosenbluth,
        1955)  
        Self-Avoiding random Walk model
        *Ï€*(**x**)â€„=â€„*Z*<sub>*n*</sub><sup>âˆ’1</sup>  
        â„™(*x*<sub>*t*â€…+â€…1</sub>\|*x*<sub>1</sub>,â€¦,*x*<sub>*t*</sub>)â€„=â€„*n*<sub>*t*</sub><sup>âˆ’1</sup>,
        *w*(**x**)â€„=â€„*n*<sub>1</sub>â€…Ã—â€…â€¦â€…Ã—â€…*n*<sub>*N*â€…âˆ’â€…1</sub> SIS.
        sampling
        *g*<sub>*t*</sub>(*x*<sub>*t*</sub>\|**x**<sub>*t*â€…âˆ’â€…1</sub>)â€„=â€„*n*<sub>*t*â€…âˆ’â€…1</sub><sup>âˆ’1</sup>,
        auxiliary
        *Ï€*<sub>*t*</sub>(**x**<sub>*t*</sub>)â€„=â€„*Z*<sub>*t*</sub><sup>âˆ’1</sup>,
        marginal
        $\pi_t(\mathbf{x}\_{t-1}) = \sum\_{x_t}\pi(\mathbf{x}\_{t-1},x_t) = \frac{n\_{t-1}}{Z_t}$,
        conditional
        *Ï€*<sub>*t*</sub>(*x*<sub>*t*</sub>\|**x**<sub>*t*â€…âˆ’â€…1</sub>)â€„=â€„*n*<sub>*t*â€…âˆ’â€…1</sub><sup>âˆ’1</sup>  
        *w*<sub>*t*â€…+â€…1</sub>â€„=â€„*w*<sub>*t*</sub>*n*<sub>*t*</sub>,
        $w_N = \prod\_{t\ge 2} \frac{1}{g_t(x_t\|x_1,\dots,x\_{t-1})}$,
        *ZÌ‚*<sub>*N*</sub>â€„=â€„*wÌ„*<sup>âˆ’1</sup>
    -   sequential imputation (Kong, liu and Wong, 1994)  
        draw **y**<sub>mis</sub> from
        *g*(**y**<sub>mis</sub>)â€„=â€„*f*(*y*<sub>mis,â€†1</sub>\|*y*<sub>obs,â€†1</sub>,*Î¸*)â€¦*f*(*y*<sub>mis,â€†*n*</sub>\|**y**<sub>obs,â€†*n*</sub>,**y**<sub>mis,â€†*n*â€…âˆ’â€…1</sub>,*Î¸*)  
        calculate weight
        *w*(**y**<sub>mis</sub>)â€„=â€„*f*(*y*<sub>obs,â€†1</sub>\|*Î¸*)â€¦*f*(*y*<sub>obs,â€†*n*</sub>\|**y**<sub>*n*â€…âˆ’â€…1</sub>,*Î¸*)  
        $\bar{w} = \frac{1}{m}\sum\_{i=1}^m w(\mathbf{y}\_{\mbox{mis}}^{(i)}) \longrightarrow L(\theta\|\mathbf{y}\_{\mbox{obs}})$
    -   nonlinear filtering state-space model (hidden Markov model)
        $\begin{cases}\mbox{(state equation):}& x_t\sim q_t(\cdot\|x\_{t-1},\theta) \\ \mbox{(observation equation):}& y_t\sim f_t(\cdot\|x\_{t},\phi) \end{cases}$  
        Gaussian *f*<sub>*t*</sub>,â€†*q*<sub>*t*</sub>, linear
        state-space model, Kalman filter  
        bootstrap filter (particle filter) (Gordon, Salmond and Smith,
        1993)  
        draw *x*<sub>*t*â€…+â€…1</sub><sup>(\**j*)</sup> from
        *q*<sub>*t*</sub>(*x*<sub>*t*â€…+â€…1</sub>\|*x*<sub>*t*</sub><sup>(*j*)</sup>),
        *w*<sup>(*j*)</sup>â€„âˆâ€„*f*<sub>*t*</sub>(*y*<sub>*t*â€…+â€…1</sub>\|*x*<sub>*t*â€…+â€…1</sub><sup>(\**j*)</sup>),
        resample
        {*x*<sub>*t*â€…+â€…1</sub><sup>(1)</sup>,â€†â€¦,â€†*x*<sub>*t*â€…+â€…1</sub><sup>(*m*)</sup>}
        from
        {*x*<sub>*t*â€…+â€…1</sub><sup>(\*1)</sup>,â€†â€¦,â€†*x*<sub>*t*â€…+â€…1</sub><sup>(\**m*)</sup>}
        with probability proportional to *w*<sup>(*j*)</sup>
    -   probability dynamic system
        *Ï€*<sub>*t*</sub>(**x**<sub>*t*</sub>), either
        **x**<sub>*t*â€…+â€…1</sub>â€„=â€„(**x**<sub>*t*</sub>,*x*<sub>*t*â€…+â€…1</sub>)
        or **x**<sub>*t*â€…+â€…1</sub>â€„=â€„**x**<sub>*t*</sub>  
        set
        *g*<sub>*t*</sub>(*x*<sub>*t*</sub>\|**x**<sub>*t*â€…âˆ’â€…1</sub>)â€„=â€„*Ï€*<sub>*t*</sub>(*x*<sub>*t*</sub>\|**x**<sub>*t*â€…âˆ’â€…1</sub>),
        $w_t = \frac{\pi_t(\mathbf{x}\_{t-1})}{\pi\_{t-1}(\mathbf{x}\_{t-1})}\frac{\pi_t(x\_{t}\|\mathbf{x}\_{t-1})}{g_t(x_t\|\mathbf{x}\_{t-1})}$  
        unnormalized
        *q*<sub>*t*</sub>(**x**<sub>*t*</sub>)â€„=â€„*Z*<sub>*t*</sub>*Ï€*<sub>*t*</sub>(**x**<sub>*t*</sub>),
        $w_N = \prod\_{t=1}^Nw_t = \frac{Z_N}{Z_1}\frac{\pi_N(\mathbf{x}\_N)}{g_1(x_1)\dots g_N(x_N\|\mathbf{x}\_{N-1})}$,
        $\mathbb{E}(w_N) = \frac{Z_N}{Z_1}$  
        Prune-Enriched Rosenbluth Method (PERM) (Grassberger, 1997)
        *w*<sub>*t*</sub>â€„\>â€„*C*<sub>*t*</sub> split into *r* copies
        with probability *w*<sub>*t*</sub>/*r*,
        *w*<sub>*t*</sub>â€„â‰¤â€„*c*<sub>*t*</sub> keep or discard with
        probability 0.5

# Metropolis algorithm

target distribution *Ï€*(**x**)â€„=â€„*Z*<sup>âˆ’1</sup>expâ€†(âˆ’*h*(**x**))

proposal function, trial proposal
*T*(**x**<sup>(*t*)</sup>,**x**â€²)â€„=â€„*T*(**x**â€²,**x**<sup>(*t*)</sup>),
change *Î”**h*â€„=â€„*h*(**x**â€²)â€…âˆ’â€…*h*(**x**<sup>(*t*)</sup>)

acceptance rejection ratio
*r*â€„=â€„*Ï€*(**x**â€²)/*Ï€*(**x**<sup>(*t*)</sup>)â€„=â€„expâ€†(*Î”**h*)

1-D Ising model $U(\mathbf{x}) = - J \sum\_{s=1}^{d-1}x_sx\_{s+1}$,
$h(\mathbf{x})=JU(\mathbf{x})/(\beta T) =: -\mu \sum\_{s=1}^{d-1}x_sx\_{s+1}$

$$r = \pi(\mathbf{x}')/\pi(\mathbf{x}^{(t)}) = \begin{cases}\exp(-2\mu x_j^{(t)}(x\_{j-1}^{(t)}+x\_{j+1}^{(t)})) & j\ne 1,d\\
\exp(-2\mu x_j^{(t)}x\_{j+1}^{(t)}) & j = 1\\
\exp(-2\mu x_j^{(t)}x\_{j-1}^{(t)}) & j = d\end{cases}$$

``` r
> mu <-  1
> d <-  50
> x <- rep(1,d)
> nsamp <- 50000
> s <- rep(0,nsamp+1)
> s[1] <- sum(x)
> for (i in 1:nsamp) {
+   j <- sample(1:d)[1]
+   if (j == 1) {
+     r <- -2 * mu * x[j] * x[j+1]  
+   } else if (j == d) {
+     r <- -2 * mu * x[j] * x[j-1]
+   } else {
+     r <- -2 * mu * x[j] * (x[j-1] + x[j+1])
+   }
+   
+   if (log(runif(1)) <= r) {
+     x[j] <- -x[j]
+   }
+   s[i+1] <- sum(x)
+ }
> 
> plot(s[1:2000],type ='l', xlab = 'Iteration')
```

![](mcmc_files/figure-markdown_github/Ising-1.png)

``` r
> by10 <- seq(50, nsamp, by = 50)
> acf(s[by10])
```

![](mcmc_files/figure-markdown_github/Ising-2.png)

``` r
> mu <-  2
> s <- rep(0,nsamp+1)
> s[1] <- sum(x)
> for (i in 1:nsamp) {
+   j <- sample(1:d)[1]
+   if (j == 1) {
+     r <- -2 * mu * x[j] * x[j+1]  
+   } else if (j == d) {
+     r <- -2 * mu * x[j] * x[j-1]
+   } else {
+     r <- -2 * mu * x[j] * (x[j-1] + x[j+1])
+   }
+   
+   if (log(runif(1)) <= r) {
+     x[j] <- -x[j]
+   }
+   s[i+1] <- sum(x)
+ }
> 
> plot(s[1:2000],type ='l', xlab = 'Iteration')
```

![](mcmc_files/figure-markdown_github/Ising-3.png)

``` r
> by10 <- seq(50, nsamp, by = 50)
> acf(s[by10])
```

![](mcmc_files/figure-markdown_github/Ising-4.png)

actual transition function *A*(**x**,**y**),
âˆ«*Ï€*(**x**)*A*(**x**,**y**)*d***x**â€„=â€„*Ï€*(**y**)

detailed balance *Ï€*(**x**)*A*(**x**,**y**)â€„=â€„*Ï€*(**y**)*A*(**y**,**x**)

Metropolis-Hastings
$r(\mathbf{x},\mathbf{y})= \min\left\\1, \frac{\pi(\mathbf{y})T(\mathbf{y},\mathbf{x})}{\pi(\mathbf{x})T(\mathbf{x},\mathbf{y})} \right\\$,
$A(\mathbf{x},\mathbf{y}) = \pi(\mathbf{y})\min\left\\\frac{T(\mathbf{x},\mathbf{y})}{\pi(\mathbf{y})}, \frac{T(\mathbf{y},\mathbf{x})}{\pi(\mathbf{x})} \right\\$,
*Ï€*(**x**)*A*(**x**,**y**)â€„=â€„minâ€†{*Ï€*(**x**)*T*(**x**,**y**),*Ï€*(**y**)*T*(**y**,**x**)}

Baker (1965)
$r(\mathbf{x},\mathbf{y})=  \frac{\pi(\mathbf{y})T(\mathbf{y},\mathbf{x})}{\pi(\mathbf{y})T(\mathbf{y},\mathbf{x})+\pi(\mathbf{x})T(\mathbf{x},\mathbf{y})}$,
$A(\mathbf{x},\mathbf{y}) = \pi(\mathbf{y})\frac{T(\mathbf{x},\mathbf{y})T(\mathbf{y},\mathbf{x})}{\pi(\mathbf{y})T(\mathbf{y},\mathbf{x})+\pi(\mathbf{x})T(\mathbf{x},\mathbf{y})}$,
$\pi(\mathbf{x})A(\mathbf{x},\mathbf{y}) = \pi(\mathbf{x})\pi(\mathbf{y})\frac{T(\mathbf{x},\mathbf{y})T(\mathbf{y},\mathbf{x})}{\pi(\mathbf{y})T(\mathbf{y},\mathbf{x})+\pi(\mathbf{x})T(\mathbf{x},\mathbf{y})}$

Stein
$r(\mathbf{x},\mathbf{y})=  \frac{\delta(\mathbf{x},\mathbf{y})}{\pi(\mathbf{x})T(\mathbf{x},\mathbf{y})}$,
symmetric *Î´*(**x**,**y**),
*A*(**x**,**y**)â€„=â€„*Ï€*(**y**)*Î´*(**x**,**y**),
*Ï€*(**x**)*A*(**x**,**y**)â€„=â€„*Ï€*(**x**)*Ï€*(**y**)*Î´*(**x**,**y**)

Random-walk Metropolis
**x**â€²â€„=â€„**x**<sup>(*t*)</sup>â€…+â€…*Ïµ*<sub>*t*</sub>,
*Ïµ*<sub>*t*</sub>â€„âˆˆâ€„*g*<sub>*Ïƒ*</sub>(â‹…) a spherically symmetric
distribution

Metropolized independence sampler **y**â€„âˆ¼â€„*g*(**y**),
$r=\min\left\\1,\frac{w(\mathbf{y})}{w(\mathbf{x}^{(t)})}\right\\$,
*w*(**x**)â€„=â€„*Ï€*(**x**)/*g*(**x**)

Configurational bias Monte Carlo (CBMC)

-   auxiliary distribution
    *Ï€*<sub>1</sub>(*x*<sub>1</sub>),â€†*Ï€*<sub>2</sub>(*x*<sub>1</sub>,*x*<sub>2</sub>),â€†â€¦,â€†*Ï€*<sub>*d*â€…âˆ’â€…1</sub>(**x**<sub>*d*â€…âˆ’â€…1</sub>),â€†*Ï€*(**x**)
-   trial sampling distribution
    *g*(**x**)â€„=â€„*g*<sub>1</sub>(*x*<sub>1</sub>)*g*<sub>2</sub>(*x*<sub>2</sub>\|*x*<sub>1</sub>)â€¦*g*<sub>*d*</sub>(*x*<sub>*d*</sub>\|**x**<sub>*d*â€…âˆ’â€…1</sub>)
-   importance weight
    $w(\mathbf{y}) = \frac{\pi(\mathbf{y})}{g(\mathbf{y})} = \frac{\pi_1(y_1)}{g_1(y_1)}\frac{\pi_2(y_1,y_2)}{g_2(y_2\|y_1)\pi_1(y_1)}\dots\frac{\pi_d(y_1,\dots,y_d}{g_d(y_d\|\mathbf{y}\_{d-1})\pi\_{d-1}(\mathbf{y}\_{d-1})}$

multiple-try Metropolis (MTM)

-   draw **y**<sub>1</sub>â€¦,â€†**y**<sub>*k*</sub>â€„âˆ¼â€„*T*(**x**,â‹…) and
    compute
    *w*(**x**,**y**)â€„=â€„*Ï€*(**x**)*T*(**x**,**y**)*Î»*(**x**,**y**),
    ssymmetric *Î»*(**x**,**y**)â€„\>â€„0
-   draw **y** from {**y**<sub>1</sub>â€¦,â€†**y**<sub>*k*</sub>} with
    probability *w*(**y**<sub>*j*</sub>,**x**) and draw reference set
    **x**<sub>1</sub><sup>\*</sup>â€¦,â€†*x*<sub>*k*</sub><sup>\*</sup>â€„âˆ¼â€„*T*(**y**,â‹…),
    **x**<sub>*k*</sub><sup>\*</sup>â€„=â€„**x**
-   accept **y** with generalized M-H ratio
    $r_g = \min\left\\1, \frac{w(\mathbf{y}\_1,\mathbf{x})+\cdots+w(\mathbf{y}\_k,\mathbf{x})}{w(\mathbf{x}\_1^\*,\mathbf{y})+\cdots+w(\mathbf{x}\_k^\*,\mathbf{y})}\right\\$

orientational bias Monte Carlo (OBMC)
*Î»*(**x**,**y**)â€„=â€„*T*<sup>âˆ’1</sup>(**x**,**y**)

multiple-trial Metropolis independence sampler (MTMIS)

-   draw **y**<sub>*j*</sub>â€„âˆ¼â€„*p*(**y**) and compute
    *w*(**y**<sub>*j*</sub>)â€„=â€„*Ï€*(**y**<sub>*j*</sub>)/*p*(**y**<sub>*j*</sub>),
    *W*â€„=â€„âˆ‘*w*(**y**<sub>*j*</sub>)
-   draw **y** from {**y**<sub>1</sub>â€¦,â€†**y**<sub>*k*</sub>} with
    probability *w*(**y**<sub>*j*</sub>)
-   **x**<sup>(*t*+1)</sup>â€„=â€„**y** with probability
    $\min\left\\1, \frac{W}{W-w(\mathbf{y})+w(\mathbf{x})}\right\\$ and
    **x**<sup>(*t*+1)</sup>â€„=â€„**x** otherwise

multipoint method

-   draw
    **y**<sub>*j*</sub>â€„âˆ¼â€„*P*<sub>*j*</sub>(â‹…\|**x**,**y**<sub>1</sub>,â€¦,**y**<sub>*j*â€…âˆ’â€…1</sub>)â€„=â€„*P*<sub>1</sub>(**y**<sub>1</sub>\|**x**)
    and compute
    *w*(**x**,**y**<sub>\[1:*j*\]</sub>)â€„=â€„*Ï€*(**x**)*P*<sub>*j*</sub>(*y*<sub>\[1:*j*\]</sub>\|**x**)*Î»*<sub>*j*</sub>(**x**,*y*<sub>\[1:*j*\]</sub>),
    *Î»*<sub>*j*</sub>(*a*,*b*,â€¦,*z*)â€„=â€„*Î»*<sub>*j*</sub>(*z*,â€¦,*b*,*a*)
    sequentially symmetric
-   draw **y** from {**y**<sub>1</sub>â€¦,â€†**y**<sub>*k*</sub>} with
    probability *w*(**y**<sub>\[*t*:1\]</sub>,**x**)
-   draw reference set
    **x**<sub>*m*</sub><sup>\*</sup>â€„âˆ¼â€„*P*<sub>*m*</sub>(â‹…\|**y**,**x**<sub>\[1:*m*âˆ’1\]</sub><sup>\*</sup>),
    *m*â€„=â€„*j*â€…+â€…1,â€†â€¦,â€†*k*,
    **x**<sub>*l*</sub><sup>\*</sup>â€„=â€„**y**<sub>*j*â€…âˆ’â€…*l*</sub>,
    *l*â€„=â€„1,â€†2,â€†â€¦,â€†*j*â€…âˆ’â€…1
-   **x**<sup>(*t*+1)</sup>â€„=â€„**y** with probability
    $r\_{mp}=\min\left\\1, \frac{\sum\_{l=1}^k w(\mathbf{y}\_{\[l:1\]},\mathbf{x})}{\sum\_{l=1}^kw(\mathbf{x}\_{\[l:1\]}^\*,\mathbf{y})}\right\\$
    and **x**<sup>(*t*+1)</sup>â€„=â€„**x** otherwise

random-grid method

-   generate direction **e** and a grid size *r*
-   candidate set **y**<sub>*l*</sub>â€„=â€„**x**â€…+â€…*l*â€…â‹…â€…*r*â€…â‹…â€…**e**
-   draw **y** from {**y**<sub>1</sub>â€¦,â€†**y**<sub>*k*</sub>} with
    probability *u*<sub>*j*</sub>*Ï€*(**y**<sub>*j*</sub>)
-   reference set
    **x**<sub>*l*</sub><sup>\*</sup>â€„=â€„**y**â€…âˆ’â€…*l*â€…â‹…â€…*r*â€…â‹…â€…**e**
-   accept **y** with probability
    $\min\left\\1, \frac{\sum\_{l=1}^k \pi(\mathbf{y}\_l)}{\sum\_{l=1}^k\pi(\mathbf{x}\_l^\*)}\right\\$

MCMC estimation of ğ”¼<sub>*Ï€*</sub>*h*(**x**),
$m\mathrm{Var}\left(\sum h(\mathbf{x}^{(l)}\right)=\sigma^2\left\[1+ 2 \sum\_{j=1}^{m-1}\left(1-\frac{j}{m}\right)\rho_j\right\] \approx \sigma^2\left\[1+ 2 \sum\_{j=1}^{m-1}\rho_j\right\]$,
*Ïƒ*<sup>2</sup>â€„=â€„Var(*h*(**x**)),
*Ï*<sub>*j*</sub>â€„=â€„Corr(*h*(**x**<sup>(1)</sup>),*h*(**x**<sup>(*j*+1)</sup>))

integrated autocorrelation time
$\tau\_{\mbox{int}}(h) = \frac{1}{2} + \sum\_{j=1}^{\infty}\rho_j$,
effective sample size *m*/\[2*Ï„*<sub>int</sub>(*h*)\]

exponential autocorrelation time
$\tau\_{\mbox{exp}}(h) = \lim\sup\_{j\to\infty}\frac{j}{-\log\|\rho_j\|} \approx \tau\_{\mbox{int}}(h)$

relaxation time
*Ï„*<sub>exp</sub>â€„=â€„sup<sub>*h*â€„âˆˆâ€„*L*<sup>2</sup>(*Ï€*)</sub>*Ï„*<sub>exp</sub>(*h*)

*Ï*<sub>*j*</sub>(*h*)â€„=â€„*Î»*<sup>*j*</sup>,
$\tau\_{\mbox{int}}(h) = \frac{1+\lambda}{2(1-\lambda)}$,
$\tau\_{\mbox{exp}}(h) = -\frac{1}{\log(\|\lambda\|)}$,
$\tau\_{\mbox{exp}} = -\frac{1}{\log(\|\lambda_2(T)\|)}$,

### Gibbs sampler

random-scan / systematic-scan

slice sampler
*S*â€„=â€„{**y**â€„âˆˆâ€„â„<sup>*d*â€…+â€…1</sup>â€„:â€„*y*<sub>*d*â€…+â€…1</sub>â€„â‰¤â€„*Ï€*(*y*<sub>1</sub>,â€¦,*y*<sub>*d*</sub>)}

-   draw *y*<sup>*t*â€…+â€…1</sup>â€„âˆ¼â€„*U*(0,*Ï€*(*x*<sup>(*t*)</sup>))
-   draw **x**<sup>*t*â€…+â€…1</sup>â€„âˆ¼â€„*U*(*S*)

Metropolized Gibbs sampler
$\min\left\\1, \frac{1-\pi(x_i\|\mathbf{x}\_{\[-i\]})}{1-\pi(y_i\|\mathbf{x}\_{\[-i\]})}\right\\$

random-ray Monte Carlo (hit-and-run algorithm)

-   draw **y**<sub>*i*</sub>â€„âˆ¼â€„*T*<sub>*e*</sub>(**x**<sup>\*</sup>,â‹…)
    along direction *e*,
    **y**<sub>*i*</sub>â€„=â€„**x**â€…+â€…*r*<sub>*j*</sub>**e**,
    *r*<sub>*j*</sub>â€„âˆ¼â€„*U*(âˆ’*Ïƒ*,*Ïƒ*)
-   MTM draw **y**<sup>\*</sup> from
    {**y**<sub>1</sub>â€¦,â€†**y**<sub>*k*</sub>} with probability
    *Ï€*(**y**<sub>*j*</sub>), draw reference
    **x**â€²<sub>1</sub>,â€†â€¦,â€†**x**â€²<sub>*k*â€…âˆ’â€…1</sub>â€„âˆ¼â€„*T*<sub>*e*</sub>(**y**<sup>\*</sup>,â‹…),
    **x**<sup>\*</sup>â€„=â€„**x**â€²<sub>*k*</sub>,
    $r = \min\left\\1, \frac{\sum\_{l=1}^k \pi(\mathbf{y}\_j)T_e(\mathbf{y}\_j,\mathbf{x}^\*)}{\sum\_{l=1}^k\pi(\mathbf{x}'\_j)T_e(\mathbf{x}'\_j,\mathbf{y}^\*)}\right\\$

data augmentation
*p*(*Î¸*\|**y**<sub>obs</sub>)â€„=â€„âˆ«*p*(*Î¸*\|**y**<sub>mis</sub>,**y**<sub>obs</sub>)*p*(**y**<sub>mis</sub>\|**y**<sub>obs</sub>)*d***y**<sub>mis</sub>

*g*(*Î¸*) approximates *p*(*Î¸*\|**y**<sub>obs</sub>). draw
*p*(**y**<sub>mis</sub>)â€„=â€„âˆ«*p*(**y**<sub>mis</sub>\|*Î¸*,**y**<sub>obs</sub>)*g*(*Î¸*)*d**Î¸*

algorithm:
**y**<sub>mis</sub><sup>(*t*âˆ’1,*l*)</sup>â€„â‡’â€„*Î¸*<sup>\*</sup>â€„âˆ¼â€„*p*(*Î¸*\|**y**<sub>obs</sub>,**y**<sub>mis</sub><sup>(*t*âˆ’1,*l*)</sup>)â€„â‡’â€„**y**<sub>mis</sub><sup>(*t*,*j*)</sup>â€„âˆ¼â€„*p*(**y**<sub>mis</sub>\|**y**<sub>obs</sub>,*Î¸*<sup>\*</sup>)

equivalent to Gibbs sampler
**y**<sub>mis</sub><sup>(*t*âˆ’1)</sup>â€„â‡’â€„*Î¸*<sup>(*t*)</sup>â€„âˆ¼â€„*p*(*Î¸*\|**y**<sub>obs</sub>,**y**<sub>mis</sub><sup>(*t*âˆ’1)</sup>)â€„â‡’â€„**y**<sub>mis</sub><sup>(*t*)</sup>â€„âˆ¼â€„*p*(**y**<sub>mis</sub>\|**y**<sub>obs</sub>,*Î¸*<sup>(*t*)</sup>)

data augmentation draw
*x*<sub>1</sub><sup>(*t*+1)</sup>â€„âˆ¼â€„*Ï€*(â€…â‹…â€…\|*x*<sub>2</sub><sup>(*t*)</sup>,
*x*<sub>2</sub><sup>(*t*+1)</sup>â€„âˆ¼â€„*Ï€*(â‹…\|*x*<sub>1</sub><sup>(*t*+1)</sup>).
*L*<sub>0</sub><sup>2</sup>(*Ï€*)â€„=â€„{*h*(**x**)â€„âˆˆâ€„*L*<sup>2</sup>(*Ï€*)â€„:â€„ğ”¼{*h*(**x**)â€„=â€„0}}

$$\begin{aligned}
\mathrm{Cov}(h(x_1^{(0)}),h(x_1^{(1)})) &= \mathrm{Var}\_{\pi}(\mathbb{E}\_{\pi}\\h(x_1)\|x_2\\) \\
\mathrm{Cov}(h(x_1^{(0)}),h(x_1^{(n)})) &= \mathrm{Var}\_{\pi}(\mathbb{E}\_{\pi}\\\dots\mathbb{E}\_{\pi}\\\mathbb{E}\_{\pi}\\\mathbb{E}\_{\pi}\\h(x_1)\|x_2\\\|x_1\\\|x_2\\\dots\\) 
\end{aligned}$$

random-scan

$$\begin{aligned}
\mathrm{Cov}(h(\mathbf{x}^{(0)}),h(\mathbf{x}^{(1)})) &= \mathbb{E}\_{\pi}\\\mathbb{E}^2\_{\pi}\\h(\mathbf{x})\|i,\mathbf{x}\_{-i}\\\\ \\
\mathrm{Cov}(h(\mathbf{x}^{(0)}),h(\mathbf{x}^{(n)})) &= \mathrm{Var}\_{\pi}(\mathbb{E}\_{\pi}\\\dots\mathbb{E}\_{\pi}\\\mathbb{E}\_{\pi}\\\mathbb{E}\_{\pi}\\h(\mathbf{x})\|i,\mathbf{x}\_{-i}\\\|\mathbf{x}\\\|i,x\_{-i}\\\dots\\)
\end{aligned}$$

histogram estimator $\hat{I} = m^{-1}\sum\_{k=1}^mh(x_1^{(k)})$,
*m*<sup>2</sup>Var(*IÌ‚*)â€„=â€„*m*<sup>2</sup>*Ïƒ*<sub>0</sub><sup>2</sup>â€…+â€…2(*m*âˆ’1)*Ïƒ*<sub>1</sub><sup>2</sup>â€…+â€…2*Ïƒ*<sub>*m*â€…âˆ’â€…1</sub><sup>2</sup>,
*Ïƒ*<sub>*k*</sub><sup>2</sup>â€„=â€„Cov(*h*(*x*<sub>1</sub><sup>(0)</sup>),*h*(*x*<sub>1</sub><sup>(*k*)</sup>))

mixture estimator
$\tilde{I} = m^{-1}\sum\_{k=1}^m\mathbb{E}\\h(x_1)\|x_2^{(k)}\\$
Rao-Blackwellization,
*m*<sup>2</sup>Var(*IÌƒ*)â€„=â€„*m*<sup>2</sup>*Ïƒ*<sub>1</sub><sup>2</sup>â€…+â€…2(*m*âˆ’1)*Ïƒ*<sub>2</sub><sup>2</sup>â€…+â€…2*Ïƒ*<sub>*m*</sub><sup>2</sup>â€„â‰¤â€„*m*<sup>2</sup>Var(*IÌ‚*)
due to monotonicity of the autocovariance

forward operator on *L*<sup>2</sup>(*Ï€*)
*F**h*(**x**)â€„=â€„âˆ«*K*(**x**,**y**)*h*(**y**)*d***y**â€„=â€„ğ”¼{*h*(**x**<sup>(1)</sup>)\|**x**<sup>(0)</sup>â€„=â€„**x**},
âˆ¥*F*âˆ¥â€„=â€„sup<sub>*h*â€„:â€„ğ”¼{*h*<sup>2</sup>}â€„=â€„1</sub>âˆ¥*F**h*(**x**)âˆ¥â€„=â€„1

forward operator on *L*<sub>0</sub><sup>2</sup>(*Ï€*),
*Î»*<sub>1</sub>(*F*<sub>0</sub>)â€„=â€„*Î»*<sub>2</sub>(*F*), spectral radius
lim<sub>*n*â€„â†’â€„âˆ</sub>âˆ¥*F*<sub>0</sub><sup>*n*</sup>âˆ¥<sup>1/*n*</sup>â€„=â€„*r*
characterizes the rate of convergence of the Markov chain in both
reversible and nonreversible cases

standard Gibbs sampler
*F*<sub>*s*</sub>â€„:â€„*x*<sub>1</sub>â€„â†’â€„*x*<sub>2</sub>â€„â†’â€„â€¦â€„â†’â€„*x*<sub>*d*</sub>

grouping Gibbs sampler
*F*<sub>*s*</sub>â€„:â€„*x*<sub>1</sub>â€„â†’â€„*x*<sub>2</sub>â€„â†’â€„â€¦â€„â†’â€„(*x*<sub>*d*â€…âˆ’â€…1</sub>,*x*<sub>*d*</sub>)

collapsed Gibbs sampler
*F*<sub>*s*</sub>â€„:â€„*x*<sub>1</sub>â€„â†’â€„*x*<sub>2</sub>â€„â†’â€„â€¦â€„â†’â€„*x*<sub>*d*â€…âˆ’â€…1</sub>

do not introduce unnecessary components into a Gibbs sampler
âˆ¥*F*<sub>*c*</sub>âˆ¥â€„â‰¤â€„âˆ¥*F*<sub>*g*</sub>âˆ¥â€„â‰¤â€„âˆ¥*F*<sub>*s*</sub>âˆ¥

Swendsen-Wang algorithm, data augmentation, Ising model

*Ï€*(**x**)â€„âˆâ€„expâ€†(*Î²**J*âˆ‘<sub>*l*â€„âˆ¼â€„*l*â€²</sub>*x*<sub>*l*</sub>*x*<sub>*l*â€²</sub>)â€„âˆâ€„âˆ<sub>*l*â€„âˆ¼â€„*l*â€²</sub>expâ€†(*Î²**J*(1+*x*<sub>*l*</sub>*x*<sub>*l*â€²</sub>))

*Ï€*(**x**,**u**)â€„âˆâ€„âˆ<sub>*l*â€„âˆ¼â€„*l*â€²</sub>*I*(0â‰¤*u*<sub>*l*,â€†*l*â€²</sub>â‰¤exp(*Î²**J*(1+*x*<sub>*l*</sub>*x*<sub>*l*â€²</sub>)))

*Ï€*(**x**,**b**)â€„âˆâ€„âˆ<sub>*l*â€„âˆ¼â€„*l*â€²</sub>(1+*b*<sub>*l*,â€†*l*â€²</sub>(*e*<sup>2*Î²**J*</sup>âˆ’1))
bonding variable
*b*<sub>*l*,â€†*l*â€²</sub>â€„=â€„*I*(*u*<sub>*l*,â€†*l*â€²</sub>\>1),
â„™(*b*<sub>*l*,â€†*l*â€²</sub>=1)â€„=â€„*e*<sup>âˆ’2*Î²**J*</sup>

partial resample on fiber ğ’³<sub>*Î±*</sub>,
ğ’³â€„=â€„âˆª<sub>*Î±*â€„âˆˆâ€„*A*</sub>ğ’³<sub>*Î±*</sub>,
ğ’³<sub>*Î±*</sub>â€…âˆ©â€…ğ’³<sub>*Î²*</sub>â€„=â€„âˆ…,
*Ï€*(**x**)â€„=â€„âˆ«*Î½*<sub>*Î±*</sub>(**x**)*d**Ï*(*Î±*)

-   Gibbs (axis) move *Î±*â€„âˆˆâ€„â„,
    ğ’³<sub>*Î±*</sub>â€„=â€„{**x**â€„:â€„*x*<sub>1</sub>â€„=â€„*Î±*},
    *Î½*<sub>*Î±*</sub>â€„=â€„*Ï€*(*x*<sub>2</sub>\|*x*<sub>1</sub>)*I*(*x*<sub>1</sub>=*Î±*)

-   *Î±*â€„âˆˆâ€„â„,
    ğ’³<sub>*Î±*</sub>â€„=â€„{**x**â€„:â€„*x*<sub>2</sub>â€„=â€„*x*<sub>1</sub>â€…+â€…*Î±*},
    *Î½*<sub>*Î±*</sub>â€„=â€„*Ï€*(*x*<sub>1</sub>,*x*<sub>2</sub>)*I*(*x*<sub>2</sub>=*x*<sub>1</sub>+*Î±*)

-   *Î±*â€„âˆˆâ€„â„â€…âˆ–â€…{0},
    ğ’³<sub>*Î±*</sub>â€„=â€„{**x**â€„:â€„*x*<sub>1</sub>â€„=â€„*Î±**x*<sub>1</sub>},
    *Î½*<sub>*Î±*</sub>â€„=â€„\|*x*<sub>1</sub>\|*Ï€*(*x*<sub>1</sub>,*Î±**x*<sub>1</sub>)*I*(*x*<sub>2</sub>=*Î±**x*<sub>1</sub>)

Gausian random field model
$\pi(\mathbf{x}) \propto \exp\left\\-\frac{1}{2}\sum\_{s\sim s'}\beta\_{ss'}(x_s-x\_{s'})^2-\frac{1}{2}\sum\_{s\in\Lambda}\gamma\_{s}(x_s-\mu\_{s})^2 \right\\$,
*s*,â€†*s*â€²â€„âˆˆâ€„*Î›*â€„âŠ‚â€„*N*â€…Ã—â€…*N* Markov random field (MRF)

-   Gibbs
    $\pi(x_s\|\mathbf{x}\_{\[-s\]}) \propto \exp\left\\-\frac{1}{2}\left(\gamma_s+\sum\_{s'\sim s}\beta\_{ss'}\right)\left(x_s-\frac{\gamma_s\mu_s+\sum\_{s'\sim s}\beta\_{ss'}x\_{s'}}{\gamma_s+\sum\_{s\sim s'}\beta\_{ss'}}\right)^2\right\\$

-   coarsing move
    **x**â€„â†’â€„(**x**<sub>*S*</sub>+*Î´*,**x**<sub>\[âˆ’*S*\]</sub>),
    ğ’³<sub>*Î±*</sub>â€„=â€„{**x**â€„:â€„*x*<sub>\[âˆ’*S*\]</sub>â€„=â€„*Î±*}, draw
    *p*(*Î´*)â€„âˆâ€„*Ï€*(**x**<sub>*S*</sub>+*Î´*,**x**<sub>\[âˆ’*S*\]</sub>),
    *Î´*â€„âˆ¼â€„*N*(*Î¼*<sub>\*</sub>,*Ïƒ*<sub>\*</sub><sup>2</sup>),
    $\mu\_\*= \frac{\sum\_{s'\sim s\in\partial S}\beta\_{ss'}(x_s-x\_{s'}) + \sum\_{s\in S}\gamma_s\mu_s}{\sum\_{s'\sim s\in\partial S}\beta\_{ss'} + \sum\_{s\in S}\gamma_s}$,
    *Ïƒ*<sub>\*</sub><sup>2</sup>â€„=â€„\[âˆ‘<sub>*s*â€²â€„âˆ¼â€„*s*â€„âˆˆâ€„âˆ‚*S*</sub>*Î²*<sub>*s**s*â€²</sub>+âˆ‘<sub>*s*â€„âˆˆâ€„*S*</sub>*Î³*<sub>*s*</sub>\]<sup>âˆ’1</sup>

generalized Gibbs draw *Î³*, set **x**â€²â€„=â€„*Î³*(**x**)

locally compact group *Î“*â€„=â€„{*Î³*}: locally compact space, group
operation
*Î³*<sub>1</sub>*Î³*<sub>2</sub>(**x**)â€„=â€„*Î³*<sub>1</sub>(*Î³*<sub>2</sub>(**x**)),
continuous
(*Î³*<sub>1</sub>,*Î³*<sub>2</sub>)â€„â†’â€„*Î³*<sub>1</sub>*Î³*<sub>2</sub> and
*Î³*â€„â†’â€„*Î³*<sup>âˆ’1</sup>.

left Haar measure
*L*(*B*)â€„=â€„âˆ«<sub>*B*</sub>*L*(*d**Î³*)â€„=â€„âˆ«<sub>*Î³*<sub>0</sub>*B*</sub>*L*(*d**Î³*)â€„=â€„*L*(*Î³*<sub>0</sub>*B*),
*Î³*<sub>0</sub>â€„âˆˆâ€„*Î“*, *B*â€„âŠ‚â€„*Î“*

**x**â€„âˆ¼â€„*Ï€*(**x**),
*Î³*â€„âˆ¼â€„*p*<sub>**x**</sub>(*Î³*)â€„âˆâ€„*Ï€*(*Î³*(**x**))\|*J*<sub>*Î³*</sub>(**x**)\|*L*(*d**Î³*)â€„â‡’â€„**x**â€²â€„=â€„*Î³*(**x**)â€„âˆ¼â€„*Ï€*

partial resampling ğ’³<sub>*Î±*</sub>â€„=â€„{**x**â€„âˆˆâ€„ğ’³â€„:â€„**x**â€„=â€„*Î³*(*Î±*)},
*Î½*<sub>*Î±*</sub>(**x**)â€„âˆâ€„*Ï€*(*Î³*(*Î±*))\|*J*<sub>*Î±*</sub>(*Î³*)\|*L*(*d**Î³*)*I*(**x**=*Î³*(*Î±*))

Metrpolis *A*<sub>**x**</sub>(*Î³*,*Î³*â€²)*L*(*d**Î³*) such that
*p*<sub>**x**</sub>(*Î³*)*d**Î³*â€„âˆâ€„*Ï€*(*Î³*(**x**))\|*J*<sub>*Î³*</sub>(**x**)\|*L*(*d**Î³*)
invariant and
*A*<sub>**x**</sub>(*Î³*,*Î³*â€²)â€„=â€„*A*<sub>*Î³*<sub>0</sub><sup>âˆ’1</sup>**x**</sub>(*Î³**Î³*<sub>0</sub>,*Î³*â€²*Î³*<sub>0</sub>).
If **x**â€„âˆ¼â€„*Ï€* and *Î³*â€„âˆ¼â€„*A*<sub>**x**</sub>(*Î³*<sub>id</sub>,*Î³*), then
*w*â€„=â€„*Î³*(**x**)â€„âˆ¼â€„*Ï€*

parameter expanded data augmentation (PX-DA) draw
**y**<sub>mis</sub>â€„âˆ¼â€„*f*(**y**<sub>mis</sub>\|*Î¸*,**y**<sub>obs</sub>),
draw
*Î±*â€„âˆ¼â€„*p*(*Î±*\|**y**<sub>obs</sub>,**y**<sub>mis</sub>)âˆ*f*(**y**<sub>obs</sub>,*Î³*<sub>*Î±*</sub>(**y**<sub>mis</sub>))\|*J*<sub>*Î±*</sub>(**y**<sub>mis</sub>)\|*d**H*(*Î±*),
compute **y**â€²<sub>mis</sub>â€„=â€„*Î³*<sub>*Î±*</sub>(**y**<sub>mis</sub>),
draw *Î¸*â€„âˆ¼â€„*f*(*Î¸*\|**y**<sub>obs</sub>,**y**<sub>mis</sub>)

### hybrid Monte Carlo (HMC) with *L* (40-70) steps of deterministic Hamiltonian moves

molecular dynamics - Monte Carlo

ergodicity theorem
$\lim\limits\_{t\to\infty}\frac{1}{t}\int_0^th(\mathbf{x}\_s)ds = Z^{-1}\int h(\mathbf{x})\exp(-U(\mathbf{x})/\beta T)d\mathbf{x}$

Newtonian mechanism *d*-dimensional position **x**(*t*), *d*â€„=â€„3*N*,
$\mathbf{v}(t)=\dot{\mathbf{x}}(t)$, Newtonâ€™s law of motion
**F**â€„=â€„**m****vÌ‡**(*t*), momentum **p**â€„=â€„**m****v**, kinetic energy
$k(\mathbf{p})=\frac{1}{2}\left\\\frac{\mathbf{p}}{\sqrt{\mathbf{m}}}\right\\^2$,
total energy *H*(**x**,**p**)â€„=â€„*U*(**x**)â€…+â€…*k*(**p**), Hamilton
equation
$\begin{cases} \dot{\mathbf{x}}(t)=\frac{\partial H(\mathbf{x},\mathbf{p})}{\partial \mathbf{p}}\\\dot{\mathbf{p}}(t)=-\frac{\partial H(\mathbf{x},\mathbf{p})}{\partial \mathbf{x}} \end{cases}$

Verlet algorithm
$\mathbf{x}(t+\Delta t) = 2\mathbf{x}(t) - \mathbf{x}(t-\Delta t) - \frac{1}{\mathbf{m}}\frac{\partial H}{\partial \mathbf{x}}\Big\|\_t(\Delta t)^2$,
$\mathbf{p}(t+\Delta t) = \mathbf{m}\frac{\mathbf{x}(t+\Delta t) -\mathbf{x}(t-\Delta t)}{2 \Delta t}$

leap-frog method (Hockney 1970)
$\mathbf{x}(t+\Delta t) = \mathbf{x}(t)  + \Delta t \frac{\mathbf{p}(t+\frac{1}{2}\Delta t)}{\mathbf{m}}$,
$\mathbf{p}(t+\frac{1}{2}\Delta t) = \mathbf{p}(t-\frac{1}{2}\Delta t) + \frac{\partial H}{\partial \mathbf{x}}\Big\|\_t \Delta t$

volume preservation
\|*V*(*t*)\|â€„=â€„âˆ«<sub>*V*(*t*)</sub>*d***x***d***p**â€„=â€„âˆ«<sub>*V*(0)</sub>*d***x***d***p**â€„=â€„\|*V*(0)\|,
*V*(*t*)â€„=â€„{(**x**(*t*),**p**(*t*))â€„:â€„(**x**(0),**p**(0))â€„âˆˆâ€„*V*(0)}

HMC draw **p**â€„âˆ¼â€„*Ï•*(**p**)â€„âˆâ€„expâ€†{â€…âˆ’â€…*k*(**p**)}, leap-frog *L* steps
(**x**,**p**)â€„â†’â€„(**x**â€²,**p**â€²), acceptance ratio
minâ€†{1,â€†expâ€†(âˆ’*H*(**x**â€²,**p**â€²)+*H*(**x**,**p**))}

Langevin-Euler move
$d\mathbf{x}\_t = -\frac{1}{2}\frac{\partial U(\mathbf{x}\_t)}{\partial \mathbf{x}}dt + dW_t \Longrightarrow X_t\sim\pi$,
discretization
$\mathbf{x}\_{t+1}=\mathbf{x}\_{t} -\frac{1}{2}\frac{\partial U(\mathbf{x}\_t)}{\partial \mathbf{x}}h +\sqrt{h}Z_t$

generalized HMC *Ï•*â€„=â€„(**x**,**p**),
*Ï€*<sup>\*</sup>(*Ï•*)*T*(*Ï•*,*Ï•*â€²)â€„=â€„*Ï€*<sup>\*</sup>(*Ï•*â€²)*T*(*Ï•*â€²,*Ï•*),
acceptance ratio
$\min\left\\1,\frac{\pi(\phi^{(1)})/\pi^\*(\phi^{(k)})}{\pi(\phi^{(0)})/\pi^\*(\phi^{(0)})}\right\\$

surrogate transimition method, a reversible Markov transition
*S*(**x**,**y**) leaving
*Ï€*<sup>\*</sup>(**x**)â€„âˆâ€„expâ€†{â€…âˆ’â€…*h*<sup>\*</sup>(**x**)} invariant,
*Ï€*<sup>\*</sup>(**x**)*S*(**x**,**y**)â€„=â€„*Ï€*<sup>\*</sup>(**y**)*S*(**y**,**x**),
draw **y**<sub>*i*</sub>â€„âˆ¼â€„*S*(**y**<sub>*i*â€…âˆ’â€…1</sub>,â‹…) with
acceptance ratio
$\min\left\\1,\frac{\pi(\mathbf{y}\_k)/\pi^\*(\mathbf{y}\_k)}{\pi(\mathbf{x}^{(t)})/\pi^\*(\mathbf{x}^{(t)})}\right\\$,
actual transition
$A(\mathbf{x},\mathbf{y}) = S^{(k)}(\mathbf{x},\mathbf{y})\min\left\\1,\frac{\pi(\mathbf{y})/\pi^\*(\mathbf{y})}{\pi(\mathbf{x})/\pi^\*(\mathbf{x})}\right\\$

Neal (1994) window method: choose *W*â€„\<â€„*L*, draw *K* from
{0,â€†â€¦,â€†*W*â€…âˆ’â€…1}, obtain trjectory
{*Ï•*(*l*)â€„:â€„*l*â€„=â€„â€…âˆ’â€…*K*,â€†â€¦,â€†*L*â€…âˆ’â€…*K*} from
*Ï•*(0)â€„=â€„(**x**<sup>(*t*)</sup>,**p**<sup>(*t*)</sup>), compute free
energy *F*(ğ’²)â€„=â€„â€…âˆ’â€…logâ€†{âˆ‘<sub>*Ï•*(*j*)â€„âˆˆâ€„ğ’²</sub>exp(âˆ’*H*(*Ï•*(*j*)))} for
acceptance window
ğ’œâ€„=â€„{*Ï•*(*l*)â€„:â€„*l*â€„=â€„*L*â€…âˆ’â€…*K*â€…âˆ’â€…*W*â€…+â€…1,â€†â€¦,â€†*L*â€…âˆ’â€…*K*} and rejection
window â„›â€„=â€„{*Ï•*(*l*)â€„:â€„*l*â€„=â€„â€…âˆ’â€…*K*,â€†â€¦,â€†â€…âˆ’â€…*K*â€…+â€…*W*â€…âˆ’â€…1}, go to
acceptance window with probability minâ€†{1,â€†expâ€†(*F*(ğ’œ)âˆ’*F*(â„›))} and
rejection window otherwise, select state *Ï•* within chosen window with
probability expâ€†(âˆ’*H*(*Ï•*(*j*))+*F*(ğ’²))}

multipoint method: obtain trjectory {*Ï•*(*l*)â€„:â€„*l*â€„=â€„1,â€†â€¦,â€†*L*} from
*Ï•*(0)â€„=â€„(**x**<sup>(*t*)</sup>,**p**<sup>(*t*)</sup>), select
*Ï•*â€²â€„=â€„*Ï•*(*L*âˆ’*K*) from {*Ï•*(*l*)â€„:â€„*l*â€„=â€„*L*â€…âˆ’â€…*M*â€…+â€…1,â€†â€¦,â€†*L*} with
Boltzmann probabilities
â„™(*Ï•*â€²=*Ï•*(*L*âˆ’*M*+*k*))â€„âˆ¼â€„*w*<sub>*k*</sub>expâ€†(âˆ’*H*(*Ï•*(*L*âˆ’*M*+*k*))),
obtain trajectory {*Ï•*(*l*)â€„:â€„*l*â€„=â€„â€…âˆ’â€…1,â€†â€¦,â€†â€…âˆ’â€…*K*}, accept *Ï•*â€² with
probability
$p=\min\left\\1,\frac{\sum\_{j=1}^Mw_j\exp(-H(\phi(L-M+j)))}{\sum\_{j=1}^Mw_j\exp(-H(\phi(M-K-j)))}\right\\$
and *Ï•*(0) otherwise, renew **p**<sup>(*t*+1)</sup>â€„âˆ¼â€„*N*(0,*Î£*) with
*Î£*â€„=â€„diag(*m*<sub>1</sub><sup>âˆ’1</sup>,â€¦,*m*<sub>*d*</sub><sup>âˆ’1</sup>)

umbrella sampling (Torrie and Valleau 1977)

-   estimand
    $A=\frac{\int q_1(\mathbf{x})d\mathbf{x}}{\int q_0(\mathbf{x})d\mathbf{x}} = \frac{Z_1}{Z_0}=\mathbb{E}\_0\left\\\frac{q_1(\mathbf{x})}{q_0(\mathbf{x})}\right\\ = \frac{\mathbb{E}\_u\\q_1(\mathbf{x})/q_u(\mathbf{x})\\}{\mathbb{E}\_u\\q_0(\mathbf{x})/q_u(\mathbf{x})\\}$
-   umbrella distribution
    *Ï€*<sub>*u*</sub>(**x**)â€„âˆâ€„*w*(*Î”**h*(**x**))*Ï€*<sub>0</sub>(**x**),
    *Ï€*<sub>*i*</sub>(**x**)â€„âˆâ€„expâ€†{â€…âˆ’â€…*h*<sub>*i*</sub>(**x**)/*k**T*<sub>*i*</sub>},
    *Î”**h*(**x**)â€„=â€„*h*<sub>1</sub>(**x**)/*k**T*<sub>1</sub>â€…âˆ’â€…*h*<sub>0</sub>(**x**)/*k**T*<sub>0</sub>
-   when *h*<sub>0</sub>(**x**)â€„=â€„*h*<sub>1</sub>(**x**) but
    *T*<sub>0</sub>â€„â‰ â€„*T*<sub>1</sub>,
    *Ï€*<sub>*Î±*<sub>*i*</sub></sub>(**x**)â€„âˆâ€„expâ€†{â€…âˆ’â€…*h*<sub>0</sub>(**x**)/*T*<sub>*Î±*<sub>*i*</sub></sub>},
    *T*<sub>0</sub>â€„\>â€„*T*<sub>*Î±*<sub>1</sub></sub>â€„\>â€„â€¦â€„\>â€„*T*<sub>1</sub>,
    0â€„\<â€„*Î±*<sub>1</sub>â€„\<â€„â€¦â€„\<â€„*Î±*<sub>*k*â€…âˆ’â€…1</sub>â€„=â€„1,
    $\hat{A}=\frac{\hat{c}\_{\alpha_1}}{\hat{c}\_{0}}\times\dots\times\frac{\hat{c}\_{1}}{\hat{c}\_{\alpha\_{k-1}}}$
-   bridge sampling
    $A=\frac{c_1}{c_0}=\frac{\mathbb{E}\_0\\q_1(\mathbf{x})\alpha(\mathbf{x})\\}{\mathbb{E}\_1\\q_0(\mathbf{x})\alpha(\mathbf{x})\\}$,
    $\hat{A}\_{BS}=\frac{\sum\_{l=1}^{m_0}q_1(\mathbf{x}\_0^{(l)})\alpha(\mathbf{x}\_0^{(l)})/n_0}{\sum\_{l=1}^{m_1}q_1(\mathbf{x}\_1^{(l)})\alpha(\mathbf{x}\_1^{(l)})/n_1}$,
    *Î±*(**x**)â€„âˆâ€„{*m*<sub>0</sub>*Ï€*<sub>0</sub>(**x**)â€…+â€…*m*<sub>1</sub>*Ï€*<sub>1</sub>(**x**)}<sup>âˆ’1</sup>

simulated annealing (SA) (Kirkpatrick et al.Â 1983)

-   minimum of *h*(**x**)
-   initialize **x**<sup>(0)</sup> and *T*<sub>1</sub>,
    *N*<sub>*k*</sub> MCMC iterations *Ï€*<sub>*k*</sub>(**x**) and pass
    final configuration to next interation
-   global minimum of *h*(**x**) with probability 1 if
    *T*<sub>*k*</sub>â€„=â€„*O*(log(*L*<sub>*k*</sub><sup>âˆ’1</sup>)) with
    *L*<sub>*k*</sub>â€„=â€„*N*<sub>1</sub>â€…+â€…â€¦â€…+â€…*N*<sub>*k*</sub>

simulated tempering (ST) (Parisi 1992, Geyer and Thompson 1995)

-   *Î *â€„=â€„{*Ï€*<sub>*i*</sub>(**x**)â€„âˆâ€„expâ€†(âˆ’*h*(**x**)/*T*<sub>*i*</sub>),â€†*i*â€„âˆˆâ€„*I*},
    target distribution when temperature is lowest
-   *Ï€*<sub>*s**t*</sub>(**x**,*i*)â€„âˆâ€„*c*<sub>*i*</sub>expâ€†(âˆ’*h*(**x**)/*T*<sub>*i*</sub>),
    *c*<sub>*i*</sub>â€„âˆâ€„*Z*<sub>*i*</sub><sup>âˆ’1</sup>
-   (**x**<sup>(*t*)</sup>,*i*<sup>(*t*)</sup>)â€„=â€„(**x**,*i*), draw
    *u*â€„âˆˆâ€„*U*(0,1), if *u*â€„â‰¤â€„*Î±*<sub>0</sub>,
    *i*<sup>(*t*+1)</sup>â€„=â€„*i* and draw
    **x**<sup>(*t*+1)</sup>â€„âˆ¼â€„*T*<sub>*i*</sub>(**x**,**x**<sup>(*t*+1)</sup>)
    leaving *Ï€*<sub>*i*</sub> invariant; if *u*â€„\>â€„*Î±*<sub>0</sub>,
    **x**<sup>(*t*+1)</sup>â€„=â€„**x** and draw *i*â€²â€„âˆ¼â€„*Î±*(*i*,*i*â€²) and
    accept *i*<sup>(*t*+1)</sup>â€„=â€„*i*â€² with
    $\min\left\\1,\frac{c\_{i'}\pi\_{i'}(\mathbf{x})\alpha(i',i)}{c\_{i}\pi\_{i}(\mathbf{x})\alpha(i,i')}\right\\$

parallel tempering (PT) (Greyer 1991)

-   *Ï€*<sub>*p**t*</sub>(**x**<sub>1</sub>,â€¦,**x**<sub>*I*</sub>)â€„=â€„âˆ<sub>*i*â€„âˆˆâ€„*I*</sub>*Ï€*<sub>*i*</sub>(**x**<sub>*i*</sub>)
-   (**x**<sub>1</sub><sup>(*t*)</sup>,â€¦,**x**<sub>*I*</sub><sup>(*t*)</sup>),
    draw *u*â€„âˆ¼â€„*U*(0,1), if *u*â€„â‰¤â€„*Î±*<sub>0</sub>, parallel step, draw
    each **x**<sup>(*t*+1)</sup> via MCMC; if *u*â€„\>â€„*Î±*<sub>0</sub>,
    swapping step, swap neighhoring pair
    **x**<sub>*i*</sub><sup>(*t*)</sup> and
    **x**<sub>*i*â€…+â€…1</sub><sup>(*t*)</sup> with
    $\min\left\\1,\frac{\pi\_{i}(\mathbf{x}\_{i+1}^{(t)})\pi\_{i+1}(\mathbf{x}\_i^{(t)})}{\pi\_{i}(\mathbf{x}\_{i}^{(t)})\pi\_{i+1}(\mathbf{x}\_{i+1}^{(t)})}\right\\$

canonical ensemble simulation - Boltzmann
*Ï€*(**x**)â€„=â€„*Z*<sup>âˆ’1</sup>expâ€†(âˆ’*Î²**H*(**x**)), *Î²*â€„=â€„1/*k**T*,
canonical ensemble assumption constant-Number Volume Temperature

-   multicanonical sampling (Beg and Neuhaus 1991)
    *U*â€„=â€„*H*(**x**)â€„âˆ¼â€„*Z*<sup>âˆ’1</sup>*Î©*(*u*)expâ€†(âˆ’*Î²**u*), density of
    states (spectral density) *Î©*(*u*). draw
    **x**â€²â€„âˆ¼â€„expâ€†(âˆ’*S*(*h*(**x**))), *S*(*u*)â€„=â€„logâ€†*Î©*(*u*), then
    *Ï€*â€²(*u*)â€„âˆâ€„*c*
-   1/*k*-ensemble sampling (Hesselbo and Stinchcombe 1995)
    *Ï€*<sup>\*</sup>(**x**) such that entropy
    *S*â€„=â€„*S*(*H*(**x**))â€„=â€„logâ€†(*Î©*(*H*(**x**)))â€„âˆâ€„*c*, then
    $\pi_S^\*(u)\propto \frac{d \log\Omega(u)}{du}$.
    *Ï€*<sup>\*</sup>(**x**)â€„âˆâ€„1/*k*(*H*(**x**)),
    *k*(*H*)â€„=â€„âˆ«<sub>âˆ’âˆ</sub><sup>*H*</sup>*Î©*(*H*â€²)*d**H*â€² number of
    configurations with smaller or equal energy,
    $P\_{1/k}(u)\propto \frac{\Omega(u)}{k(u)}=\frac{d\log k(u)}{du}$

adaptive direction sampling (ADS) (Gilks et al.Â 1994)

-   snooker algorithm
    ğ’®<sup>(*t*)</sup>â€„=â€„{**x**<sup>(*t*,1)</sup>,â€†â€¦,â€†**x**<sup>(*t*,*m*)</sup>},
    draw a stream **x**<sup>(*t*,*c*)</sup> and anchor
    **x**<sup>(*t*,*a*)</sup>,
    **e**â€„=â€„(**x**<sup>(*t*,*c*)</sup>âˆ’**x**<sup>(*t*,*c*)</sup>)/âˆ¥**x**<sup>(*t*,*c*)</sup>â€…âˆ’â€…**x**<sup>(*t*,*a*)</sup>âˆ¥,
    draw *r*â€„âˆ¼â€„*f*(*r*), update
    **x**<sup>(*t*+1,*c*)</sup>â€„=â€„**x**<sup>(*t*,*a*)</sup>â€…+â€…*r***e**

conjugate gradient Monte Carlo (CGMC) draw
**x**<sup>(*t*,*a*)</sup>â€„âˆˆâ€„ğ’®<sup>(*t*)</sup>, find local mode or high
density value point **y** of *Ï€* by gradient or conjugate gradient,
choose
**x**<sup>(*t*,*c*)</sup>â€„âˆˆâ€„ğ’®<sup>(*t*)</sup>â€…âˆ–â€…{**x**<sup>(*t*,*a*)</sup>},
**e**â€„=â€„(**y**âˆ’**x**<sup>(*t*,*c*)</sup>)/âˆ¥**y**â€…âˆ’â€…**x**<sup>(*t*,*c*)</sup>âˆ¥,
sample along **x**<sup>(*t*+1,*c*)</sup>â€„=â€„**y**â€…+â€…*r***e** by MTM with
*f*(*r*)â€„âˆâ€„\|*r*\|<sup>*d*â€…âˆ’â€…1</sup>*Ï€*(**y**+*r***e**), update other
member of ğ’®<sup>(*t*)</sup>

evolutionary Monte Carlo (EMC)

-   target *Ï€*(**x**)â€„âˆâ€„expâ€†(âˆ’*H*(**x**))
-   Gibbs distribution
    *Ï€*<sub>*i*</sub>(**x**<sub>*i*</sub>)â€„=â€„*Z*<sub>*i*</sub>(*t*<sub>*i*</sub>)<sup>âˆ’1</sup>expâ€†(âˆ’*H*(**x**<sub>*i*</sub>)/*t*<sub>*i*</sub>)
-   target of augmented system
    $\pi(\mathbf{x})=Z(t)^{-1}\exp\left(-\sum\_{l=1}^mH(\mathbf{x}\_i)/t_i\right)$
-   mutation. select **x**<sub>*k*</sub> and flip some random position
    to **y**<sub>*k*</sub> with MH ratio
    *r*<sub>*m*</sub>â€„=â€„expâ€†(âˆ’(*H*(**y**<sub>*k*</sub>)âˆ’*H*(**x**<sub>*k*</sub>))/*t*<sub>*k*</sub>)
-   crossover (**x**<sub>*i*</sub>,**x**<sub>*j*</sub>) with MH ratio
    $r_c=\exp\left(-\frac{H(\mathbf{y}\_i)-H(\mathbf{x}\_i)}{t_i}-\frac{H(\mathbf{y}\_j)-H(\mathbf{x}\_j)}{t_j}\right)\frac{T(\mathbf{Y},\mathbf{X})}{T(\mathbf{X},\mathbf{Y})}$
    -   snooker crossover
        *f*(*r*)â€„âˆâ€„\|*r*\|<sup>*d*â€…âˆ’â€…1</sup>*Ï€*(**x**<sub>*j*</sub>+*r***e**)
-   exchange (**x**<sub>*i*</sub>,**x**<sub>*j*</sub>) with MH ratio

variation distance (*L*<sup>1</sup>-distance)
$\\P-Q\\\_{\mbox{var}}=\sup\limits\_{S\in\mathcal{X}}\|P(S)-Q(S)\|=\frac{1}{2}\sum\limits\_{\mathbf{x}\in\mathcal{X}}\|P(\mathbf{x})-Q(\mathbf{x})\|=\frac{1}{2}\\P-Q\\\_{L^1}=\frac{1}{2}\int\|p(\mathbf{x})-q(\mathbf{x})\|d\mathbf{x}$

*Ï‡*<sup>2</sup>-distance
$\\P-Q\\^2\_{\chi^2}=\mathrm{Var}\_{P}\\Q(\mathbf{x})/P(\mathbf{x})\\=\sum\limits\_{\mathbf{x}\in\mathcal{X}}\|Q(\mathbf{x})-P(\mathbf{x})\|^2/P(\mathbf{x})$

*Q*(**x**,**y**)â€„=â€„*Ï€*(**x**)*A*(**x**,**y**)â€„=â€„*Ï€*(**y**)*A*(**y**,**x**),
1â€„=â€„*Î²*<sub>1</sub>â€„\>â€„*Î²*<sub>2</sub>â€„â‰¥â€„â€¦â€„â‰¥â€„*Î²*<sub>*m*â€…âˆ’â€…1</sub>â€„â‰¥â€„â€…âˆ’â€…1,
*m*â€„=â€„\|ğ’³\|.

Laplacian *L*â€„=â€„*I*â€…âˆ’â€…*A* with
*Î»*<sub>*i*</sub>â€„=â€„1â€…âˆ’â€…*Î²*<sub>*i*</sub>,
$\lambda_1=\inf\limits\_{\mathrm{Var}(\phi)\>0}\frac{\mathcal{E}(\phi,\phi)}{\mathrm{Var}(\phi)}$,
quadratic form of Laplacian
$\mathcal{E}(\phi,\phi)=\frac{1}{2}\sum\limits\_{\mathbf{x},\mathbf{y}}\[\phi(\mathbf{x})-\phi(\mathbf{y})\]^2Q(\mathbf{x},\mathbf{y})$.

length of path *Î³*<sub>*x**y*</sub>
$\\\gamma\_{xy}\\=\sum\limits\_{e\in\gamma\_{xy}}Q(e)^{-1}$,
$\kappa=\kappa(\Gamma)=\max\limits_e\sum\limits\_{\gamma\_{xy}\ni e}\\\gamma\_{xy}\\\pi(\mathbf{x})\pi(\mathbf{y})$.
$K=\max\limits_eQ(e)^{-1}\sum\limits\_{\gamma\_{xy}\ni e}\|\gamma\_{xy}\|\pi(\mathbf{x})\pi(\mathbf{y})$.
Poincar'e inequality *Î²*<sub>1</sub>â€„â‰¤â€„1â€…âˆ’â€…*Îº*<sup>âˆ’1</sup> and
*Î²*<sub>1</sub>â€„â‰¤â€„1â€…âˆ’â€…*K*<sup>âˆ’1</sup>

*S*â€„âˆˆâ€„ğ’³,
$Q(S,S^c)=\sum\limits\_{\mathbf{x}\in S}\sum\limits\_{\mathbf{y}\in S^c}Q(\mathbf{x},\mathbf{y})$.
the conductance of the chain
$h=\min\limits\_{S:\pi(S)\le 1/2} \frac{Q(S,S^c)}{\pi(S)}$. Cheegerâ€™s
inequality $1-2h\le\beta_1\le 1-\frac{h^2}{2}$

$\eta=\max\limits_eQ(e)^{-1}\sum\limits\_{\gamma\_{xy}\ni e}\pi(\mathbf{x})\pi(\mathbf{y})$.
*Î²*<sub>1</sub>â€„â‰¤â€„1â€…âˆ’â€…1/8*Î·*<sup>2</sup>

*h*(**x**)â€„âˆˆâ€„*L*<sub>0</sub><sup>2</sup>(*Ï€*), forward operator
*F**h*(**x**)â€„=â€„âˆ«*h*(**y**)*A*(**x**,**y**)*d***y**â€„=â€„ğ”¼{*h*(**x**<sup>(1)</sup>)\|**x**<sup>(0)</sup>â€„=â€„**x**},
backward operator
$Bh(\mathbf{y})=\int h(\mathbf{x})\frac{A(\mathbf{x},\mathbf{y})\pi(\mathbf{x})}{\pi(\mathbf{y})}d\mathbf{x}=\mathbb{E}\\h(\mathbf{x}^{(0)})\|\mathbf{x}^{(1)}=\mathbf{y}\\$.

$\\F\\=\max\limits\_{\\h\\=1}\\Fh\\$, spectral radius
$r_F=\lim\limits\_{n\to\infty}\\F^n\\^{1/n}$.
âŸ¨*F**h*,â€†*g*âŸ©â€„=â€„âŸ¨*h*,â€†*B**g*âŸ©.

**x**<sup>(0)</sup>â€„âˆ¼â€„*Ï€*. *h*,â€†*g*â€„âˆˆâ€„*L*<sub>0</sub><sup>2</sup>(*Ï€*).
Cov(*h*(**x**<sup>(*n*)</sup>),*h*(**x**<sup>(0)</sup>))â€„=â€„Cov<sub>*Ï€*</sub>(*F*<sup>*k*</sup>*h*(**x**),*B*<sup>*n*â€…âˆ’â€…*k*</sup>*h*(**x**)),
0â€„â‰¤â€„*k*â€„â‰¤â€„*n*.

reversible Markov chain, *F*â€„=â€„*B*. **x**<sup>(0)</sup>â€„âˆ¼â€„*Ï€*.
*h*â€„âˆˆâ€„*L*<sub>0</sub><sup>2</sup>(*Ï€*).
Cov(*h*(**x**<sup>(0)</sup>),*h*(**x**<sup>(2*m*)</sup>))â€„=â€„ğ”¼<sub>*Ï€*</sub>{\[*F*<sup>*m*</sup>*h*(**x**)\]<sup>2</sup>}â€„=â€„ğ”¼<sub>*Ï€*</sub>{\[*B*<sup>*m*</sup>*h*(**x**)\]<sup>2</sup>}â€„=â€„Var(ğ”¼{â€¦ğ”¼{ğ”¼{*h*(**x**<sup>(0)</sup>)\|**x**<sup>(1)</sup>}\|**x**<sup>(2)</sup>\|â€¦\|**x**<sup>(*m*)</sup>}).
\|Cov(*h*(**x**<sup>(0)</sup>),*h*(**x**<sup>(2*m*+1)</sup>))\|â€„â‰¤â€„Cov(*h*(**x**<sup>(0)</sup>),*h*(**x**<sup>(2*m*)</sup>))

maximal correlation
$\gamma=\sup\limits\_{f\in L_x^2(\pi),g\in L_y^2(\pi)}\mbox{Corr}(f(\mathbf{x}),g(\mathbf{y}))=\sup\limits\_{\mathbb{E}\\h\\=0,\mathrm{Var}(h)=1}\mathrm{Var}(\mathbb{E}\\h(\mathbf{x})\|\mathbf{y}\\)$.
$\\F^n\\=\\B^n\\=\gamma_n=\sup\limits\_{f,g\in L^2(\pi)}\mbox{Corr}(f(\mathbf{x}^{(0)}),g(\mathbf{x}^{(n)}))$

reversible Markov chain
*Ï*<sub>*n*</sub>(*h*)â€„=â€„Corr(*F*<sup>*n*</sup>*h*(**x**),*h*(**x**)),
$\lim\limits\_{n\to\infty}\|\rho_n(h)\|^{1/n}=\|\lambda_1\|$

irreducible aperiodic finite-state Markov chain
*hÌ„*<sub>*m*</sub>â€„â†’â€„ğ”¼<sub>*Ï€*</sub>{*h*} a.s.
$\sqrt{m} \[\bar{h}\_m-\mathbb{E}\_{\pi}\\h\\\]\to N(0,\sigma^2(h))$,
$\sigma(h)^2=\sigma^2\left\[1+2\sum\limits\_{j=1}^{\infty}\rho_j\right\]=2\tau\_{\mbox{int}}(h)\sigma^2$,
*Ï*<sub>*j*</sub>â€„=â€„Corr(*h*(**x**<sup>(1)</sup>),*h*(**x**<sup>(*j*+1)</sup>))
